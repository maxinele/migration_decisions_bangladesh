{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e83b84",
   "metadata": {},
   "source": [
    "# Assessing the Interplay Between Natural Hazards and Political Instability on Migration Decisions in Bangladesh\n",
    "\n",
    "\n",
    "##### -  Main Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad319ad",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5841dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxle647/miniconda3/envs/ccm_bangladesh/lib/python3.9/site-packages/xgboost/compat.py:105: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Type hinting and utility imports\n",
    "from typing import Any, Dict, List, Union, Optional, Tuple\n",
    "import pickle # For saving and loading serialized objects\n",
    "import glob # For file path pattern matching\n",
    "import os # For operating system interactions\n",
    "\n",
    "# Data management and manipulation tools\n",
    "import pandas as pd # Data manipulation with DataFrames\n",
    "import numpy as np # Numerical computations\n",
    "import geopandas as gpd # Geographic data processing\n",
    "import pyreadstat # Read and write SPSS, Stata, and SAS files\n",
    "import random # Random number generation\n",
    "from collections import Counter # Counting hashable objects\n",
    "from itertools import product # Cartesian product of iterables\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.experimental import enable_iterative_imputer # Enable experimental imputer\n",
    "from sklearn.impute import IterativeImputer # Multivariate imputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # Random Forest models\n",
    "from xgboost import XGBClassifier # XGBoost classifier\n",
    "from sklearn import linear_model # Linear models\n",
    "\n",
    "# Statsmodels for regressions\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Model evaluation metrics and calibration\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV # For calibrating classifier probabilities\n",
    "from random import randrange # Random number within a range\n",
    "import shap # SHAP values for model interpretability\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib as mp # Core matplotlib library\n",
    "import matplotlib.pyplot as plt # Matplotlib plotting functions\n",
    "import seaborn as sns # Seaborn data visualization\n",
    "\n",
    "# Custom functions \n",
    "import functions_transforms as fun      \n",
    "import evaluate_train_combined as fun_analysis\n",
    "import plotting_functions as fun_plots \n",
    "\n",
    "# Utility imports for function operations\n",
    "import operator  # For functional operations like itemgetter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd58a18",
   "metadata": {},
   "source": [
    "## Define universal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484c11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "random.seed(130822)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e314a",
   "metadata": {},
   "source": [
    "#### Instructions for replication:\n",
    "\n",
    "Merge_data, do_regr, do_tuning and train are set to 'False' in oder to start directly with the analysis of the results. \n",
    "\n",
    "If you wish to do all the steps, change these parameters to 'True'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should data be merged? \n",
    "# Note, this is an initial step and does not need to be done once the dataset is prepared. Set to False for replication. \n",
    "merge_data = True\n",
    "# Define if we want to do descriptives for DV. \n",
    "do_descr = True \n",
    "# Esimate regression models.\n",
    "do_regr = True\n",
    "# Tune models. \n",
    "# Note, set to False for replication.\n",
    "do_tuning = False\n",
    "# Should models be trained? \n",
    "# Note, this block needs to be run three times. Once for the \"performance evaluation\" and once for the \"shap values\" and finally for the \"shap interaction\".\n",
    "# Note, to replicate the figures in the paper without retraining the models, set this to False.\n",
    "train = True\n",
    "train_robustness = True # Train a model excluding migration history as robustness check\n",
    "# Evaluation.\n",
    "do_evaluation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c6ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder structure to store results\n",
    "base_folder_path = os.path.abspath(os.path.join(os.getcwd(), \".\")) # You can also enter the path to your folder manually\n",
    "#print('Check path:',base_folder_path) # Shows the path you are in\n",
    "\n",
    "# Define input paths \n",
    "input_data_path = os.path.join(base_folder_path,'data')\n",
    "#print('Data will be loaded from:',input_data_path) # Shows the path your data is read from\n",
    "\n",
    "# Define results path\n",
    "results_path = os.path.join(base_folder_path,'results')\n",
    "results_path_ext = os.path.join(results_path, '{sub}')\n",
    "#print('Results will be stored in:',results_path_ext) # Shows the path your results will be stored in\n",
    "\n",
    "output_paths = {\n",
    "    'descriptives': results_path_ext.format(sub=f'descriptives'),\n",
    "    'models': results_path_ext.format(sub=f'models'),\n",
    "    'predictions': results_path_ext.format(sub=f'predictions'),\n",
    "    'features': results_path_ext.format(sub=f'features'),\n",
    "    'evalscores': results_path_ext.format(sub=f'evalscores'),\n",
    "    'regression': results_path_ext.format(sub=f'regression'),\n",
    "    # Sub folders \n",
    "    'shapvals': os.path.join(results_path_ext.format(sub=f\"features\"), \"shapvals\"),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132182a",
   "metadata": {},
   "source": [
    "## Load, merge and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdeb0c1",
   "metadata": {},
   "source": [
    "Instructions: Note that this step is skipped, when merge_data is set to 'False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8325af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hh_n  month    year  mig_month_id  internal_mig district_mig  \\\n",
      "0  1_1    3.0  2011.0           NaN           0.0           NA   \n",
      "1  1_1   11.0  2011.0           NaN           0.0           NA   \n",
      "2  1_1    2.0  2011.0           NaN           0.0           NA   \n",
      "3  1_1    7.0  2011.0           NaN           0.0           NA   \n",
      "4  1_1    9.0  2011.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ... head_ocup  \\\n",
      "0                0.0          NA              0.0          0.0  ...      68.0   \n",
      "1                0.0          NA              0.0          0.0  ...      68.0   \n",
      "2                0.0          NA              0.0          0.0  ...      68.0   \n",
      "3                0.0          NA              0.0          0.0  ...      68.0   \n",
      "4                0.0          NA              0.0          0.0  ...      68.0   \n",
      "\n",
      "  head_ocup_agg Bihari Tribal_ethn mobile radio   tv loss_flood  pol_unrest  \\\n",
      "0           0.0    0.0         0.0    1.0   0.0  1.0        0.0         0.0   \n",
      "1           0.0    0.0         0.0    1.0   0.0  1.0        0.0         0.0   \n",
      "2           0.0    0.0         0.0    1.0   0.0  1.0        0.0         0.0   \n",
      "3           0.0    0.0         0.0    1.0   0.0  1.0        0.0         0.0   \n",
      "4           0.0    0.0         0.0    1.0   0.0  1.0        0.0         0.0   \n",
      "\n",
      "   incr_food_prices  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "\n",
      "[5 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    df_hh, meta = pyreadstat.read_dta(os.path.join(input_data_path,'waves_v6.dta'))\n",
    "    print(df_hh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a96e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5503\n",
      "Index(['hh_n', 'month', 'year', 'mig_month_id', 'internal_mig', 'district_mig',\n",
      "       'international_mig', 'country_mig', 'remittances_mig', 'mig_history',\n",
      "       'Division_Code', 'Division_Name', 'District_Code', 'District_Name',\n",
      "       'Upazila', 'Upazila_Name', 'Union', 'Village', 'community_id',\n",
      "       'religion', 'Muslim', 'Hindu', 'Christian', 'language', 'Bengali',\n",
      "       'Urdu', 'Hindi', 'Tribal', 'Other', 'ethnicity', 'Bangali',\n",
      "       'Other_ethn', 'head_sex', 'Male', 'Female', 'head_age', 'age_17_24',\n",
      "       'age_25_34', 'age_35_48', 'age_49_70', 'age_71', 'head_literacy',\n",
      "       'head_educ', 'prim_educ', 'sec_educ', 'ter_educ', 'no_educ',\n",
      "       'mob_asset_value_own', 'machinery_value_own', 'savings', 'loans',\n",
      "       'land_type', 'land_size', 'produc_harvested', 'ag_subsidy',\n",
      "       'livestock_value', 'sl_benefited', 'income_rent', 'income_property',\n",
      "       'income_insurance', 'income_other', 'income', 'n_houses',\n",
      "       'health_fac_avail', 'health_fac_num', 'educ_fac_avail', 'educ_fac_num',\n",
      "       'safety_prog_benef', 'remittances_in', 'remittances_in_value',\n",
      "       'active_member', 'active_leader', 'satisfied_relat', 'satisfied_leave',\n",
      "       'satisfied_life', 'hhweight', 'popweight', 'pc_expm', 'pc_foodxm',\n",
      "       'pc_nonfxm', 'head_ocup', 'head_ocup_agg', 'Bihari', 'Tribal_ethn',\n",
      "       'mobile', 'radio', 'tv', 'loss_flood', 'pol_unrest',\n",
      "       'incr_food_prices'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Number of households\n",
    "    print(len(df_hh.hh_n.unique()))\n",
    "    print(df_hh.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd1f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hh_n  month    year  mig_month_id  internal_mig district_mig  \\\n",
      "0  1_1    3.0  2011.0           NaN           0.0           NA   \n",
      "1  1_1   11.0  2011.0           NaN           0.0           NA   \n",
      "2  1_1    2.0  2011.0           NaN           0.0           NA   \n",
      "3  1_1    7.0  2011.0           NaN           0.0           NA   \n",
      "4  1_1    9.0  2011.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "0                0.0          NA              0.0          0.0  ...   \n",
      "1                0.0          NA              0.0          0.0  ...   \n",
      "2                0.0          NA              0.0          0.0  ...   \n",
      "3                0.0          NA              0.0          0.0  ...   \n",
      "4                0.0          NA              0.0          0.0  ...   \n",
      "\n",
      "  head_ocup_agg Bihari Tribal_ethn mobile radio   tv loss_flood pol_unrest  \\\n",
      "0           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "1           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "2           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "3           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "4           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "\n",
      "   incr_food_prices     yrmo  \n",
      "0               0.0  2011-03  \n",
      "1               0.0  2011-11  \n",
      "2               0.0  2011-02  \n",
      "3               0.0  2011-07  \n",
      "4               0.0  2011-09  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Get year month id\n",
    "    df_hh['yrmo'] = pd.to_datetime(df_hh[['year', 'month']].assign(DAY=1)).dt.strftime('%Y-%m')\n",
    "    print(df_hh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f6e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5503\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Transform HH\n",
    "    df_hh[\"hh_n\"] = df_hh['hh_n'].map(lambda x: str(x)[:-2])\n",
    "    df_hh[\"hh_n\"] = df_hh[\"hh_n\"].astype(int)\n",
    "    print(len(df_hh.hh_n.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c734f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hh_n  month    year  mig_month_id  internal_mig district_mig  \\\n",
      "6      1    1.0  2011.0           NaN           0.0           NA   \n",
      "2      1    2.0  2011.0           NaN           0.0           NA   \n",
      "0      1    3.0  2011.0           NaN           0.0           NA   \n",
      "11     1    4.0  2011.0           NaN           0.0           NA   \n",
      "5      1    5.0  2011.0           NaN           0.0           NA   \n",
      "\n",
      "    international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "6                 0.0          NA              0.0          0.0  ...   \n",
      "2                 0.0          NA              0.0          0.0  ...   \n",
      "0                 0.0          NA              0.0          0.0  ...   \n",
      "11                0.0          NA              0.0          0.0  ...   \n",
      "5                 0.0          NA              0.0          0.0  ...   \n",
      "\n",
      "   head_ocup_agg Bihari Tribal_ethn mobile radio   tv loss_flood pol_unrest  \\\n",
      "6            0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "2            0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "0            0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "11           0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "5            0.0    0.0         0.0    1.0   0.0  1.0        0.0        0.0   \n",
      "\n",
      "    incr_food_prices     yrmo  \n",
      "6                0.0  2011-01  \n",
      "2                0.0  2011-02  \n",
      "0                0.0  2011-03  \n",
      "11               0.0  2011-04  \n",
      "5                0.0  2011-05  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Sort df\n",
    "    df_hh = df_hh.sort_values(by=['hh_n','year','month'])\n",
    "    print(df_hh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b94cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5503\n",
      "5503\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Check NAs for time-invariant vars; drop households with NAs for waves\n",
    "    print(len(df_hh[df_hh.head_age.isna()].hh_n.astype(int).unique()))\n",
    "    print(len(df_hh[df_hh.head_sex.isna()].hh_n.astype(int).unique()))\n",
    "    print(len(df_hh[df_hh.head_literacy.isna()].hh_n.astype(int).unique()))\n",
    "    \n",
    "    hhnas = list(np.array(df_hh[df_hh.head_age.isna()].hh_n.astype(int).unique())) + \\\n",
    "    list(np.array(df_hh[df_hh.head_sex.isna()].hh_n.astype(int).unique())) + \\\n",
    "    list(np.array(df_hh[df_hh.head_literacy.isna()].hh_n.astype(int).unique()))\n",
    "    print(len(hhnas))\n",
    "    \n",
    "    hhnasunique = list(set(hhnas))\n",
    "    print(len(hhnasunique))\n",
    "    \n",
    "    print(len(df_hh.hh_n.unique()) - len(hhnasunique))\n",
    "    print(len(df_hh.set_index('hh_n').drop(hhnasunique,axis=0).index.unique())) \n",
    "    \n",
    "    # Dropping households here\n",
    "    df_hh = df_hh.set_index('hh_n').drop(hhnasunique,axis=0).reset_index()\n",
    "    df_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5583bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Check if for any of the values we have only missingnes\n",
    "    ffillcols = [\n",
    "            'mob_asset_value_own', 'machinery_value_own', 'savings', 'loans',\n",
    "               'land_type', 'land_size', 'produc_harvested', 'ag_subsidy',\n",
    "               'livestock_value', 'sl_benefited', 'income_rent', 'income_property',\n",
    "               'income_insurance', 'income_other', 'income', 'n_houses', 'pc_expm',\n",
    "               'head_ocup_agg',\n",
    "               'health_fac_avail', 'health_fac_num', 'educ_fac_avail', 'educ_fac_num',\n",
    "               'safety_prog_benef','remittances_in', 'remittances_in_value',\n",
    "               'active_member', 'active_leader', 'satisfied_relat', 'satisfied_leave',\n",
    "               'satisfied_life',\n",
    "        ]\n",
    "    \n",
    "    for f in ffillcols:\n",
    "        if df_hh[f].isnull().all():\n",
    "            print('information for all rows in column is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cbf5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5503\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Drop columns for the moment due to missingness and some of them are on the district level\n",
    "    dropcols = [\n",
    "        'n_houses',\n",
    "        'health_fac_avail', \n",
    "        'health_fac_num', \n",
    "        'educ_fac_avail', \n",
    "        'educ_fac_num',\n",
    "        'safety_prog_benef',\n",
    "    ]\n",
    "    \n",
    "    df_hh = df_hh.drop(dropcols,axis=1)\n",
    "    print(len(df_hh.hh_n.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e888ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code outcome\n",
    "if merge_data:\n",
    "        df_hh['both_out'] = df_hh.international_mig + df_hh.internal_mig # As they are either or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1a7179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hh_n  month    year  mig_month_id  internal_mig district_mig  \\\n",
      "0     1    1.0  2011.0           NaN           0.0           NA   \n",
      "1     1    2.0  2011.0           NaN           0.0           NA   \n",
      "2     1    3.0  2011.0           NaN           0.0           NA   \n",
      "3     1    4.0  2011.0           NaN           0.0           NA   \n",
      "4     1    5.0  2011.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ... Bihari  \\\n",
      "0                0.0          NA              0.0          0.0  ...    0.0   \n",
      "1                0.0          NA              0.0          0.0  ...    0.0   \n",
      "2                0.0          NA              0.0          0.0  ...    0.0   \n",
      "3                0.0          NA              0.0          0.0  ...    0.0   \n",
      "4                0.0          NA              0.0          0.0  ...    0.0   \n",
      "\n",
      "  Tribal_ethn mobile radio   tv loss_flood pol_unrest incr_food_prices  \\\n",
      "0         0.0    1.0   0.0  1.0        0.0        0.0              0.0   \n",
      "1         0.0    1.0   0.0  1.0        0.0        0.0              0.0   \n",
      "2         0.0    1.0   0.0  1.0        0.0        0.0              0.0   \n",
      "3         0.0    1.0   0.0  1.0        0.0        0.0              0.0   \n",
      "4         0.0    1.0   0.0  1.0        0.0        0.0              0.0   \n",
      "\n",
      "      yrmo  both_out  \n",
      "0  2011-01       0.0  \n",
      "1  2011-02       0.0  \n",
      "2  2011-03       0.0  \n",
      "3  2011-04       0.0  \n",
      "4  2011-05       0.0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Check data\n",
    "    df = df_hh.copy()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc6e63-fe65-4f5e-b94c-4f9197965e99",
   "metadata": {},
   "source": [
    "#### Merge with external data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaba27c-4575-4306-9cef-5f660ca35e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Nightlight data on upazila level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c63dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    df_ngt = pd.read_csv(os.path.join(input_data_path,'newupazilawlight.csv'), delimiter=',')\n",
    "    df_ngt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890cb081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  ID_0  ISO      NAME_0  ID_1   NAME_1  ID_2   NAME_2  ID_3  \\\n",
      "0           0    19  BGD  Bangladesh     1  Barisal     1  Barisal     1   \n",
      "1           1    19  BGD  Bangladesh     1  Barisal     1  Barisal     1   \n",
      "2           2    19  BGD  Bangladesh     1  Barisal     1  Barisal     1   \n",
      "3           3    19  BGD  Bangladesh     1  Barisal     1  Barisal     1   \n",
      "4           4    19  BGD  Bangladesh     1  Barisal     1  Barisal     1   \n",
      "\n",
      "    NAME_3  ...     ENGTYPE_4      2011      2012      2013      2014  \\\n",
      "0  Barisal  ...  Sub-district  0.612245  0.567010  0.304124  4.989691   \n",
      "1  Barisal  ...  Sub-district  2.478873  2.869767  2.162791  4.669767   \n",
      "2  Barisal  ...  Sub-district  0.305804  0.143177  0.129754  2.975391   \n",
      "3  Barisal  ...  Sub-district  0.839506  1.245283  0.534591  4.100629   \n",
      "4  Barisal  ...  Sub-district  7.668874  8.750000  8.694079  9.072368   \n",
      "\n",
      "       2015      2016       2017       2018  \\\n",
      "0  5.608247  4.427835   7.025773   7.036082   \n",
      "1  6.246512  4.855814   7.404651   7.767442   \n",
      "2  3.736018  2.382550   6.702461   7.203579   \n",
      "3  5.031447  3.899371   6.691824   7.106918   \n",
      "4  9.595395  8.601974  11.450658  12.394737   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((90.13227844238287 23.02925109863287,...  \n",
      "1  POLYGON ((90.33027648925787 22.94618034362793,...  \n",
      "2  MULTIPOLYGON (((90.43000030517584 22.501388549...  \n",
      "3  POLYGON ((90.05272674560553 22.808050155639762...  \n",
      "4  MULTIPOLYGON (((90.50460815429693 22.693744659...  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Rename columns\n",
    "    for i in range(11,19):\n",
    "        df_ngt = df_ngt.rename(columns={f'lights{i}':2000+i})\n",
    "    print(df_ngt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b57eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       NAME_4  year  nlight_mean\n",
      "0  Agailjhara  2011     0.612245\n",
      "1  Agailjhara  2012     0.567010\n",
      "2  Agailjhara  2013     0.304124\n",
      "3  Agailjhara  2014     4.989691\n",
      "4  Agailjhara  2015     5.608247\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Reshape df\n",
    "    cols_ngt = ['NAME_4',2011,2012,2013,2014,2015,2016,2017,2018]\n",
    "    df_ngt = df_ngt[cols_ngt].set_index('NAME_4').stack()\n",
    "    # Rename\n",
    "    df_ngt = df_ngt.rename('nlight_mean')\n",
    "    df_ngt = df_ngt.reset_index().rename(columns={'level_1':'year'})\n",
    "    print(df_ngt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe1c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Fix names\n",
    "    df_hh.Upazila_Name = df_hh.Upazila_Name.str.replace('Baghai Chhari','Baghaichhari')\n",
    "    df_hh.Upazila_Name = df_hh.Upazila_Name.str.replace('Sadar','S.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2721849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Shib Char', 'Shajahanpur', 'Palong', 'Chakoria', 'Banchharampur', 'Baghaichhari', 'Maulvibazar Sada', 'Fulchhari', 'Mohanganj Thana', 'Char Rajibpur', 'Sarankhola', 'Char Fasson', 'Kalukhali', 'Jiban Nagar', 'Austagram', 'Noakhali S. (Sudh', \"Cox'S Bazar S.\", 'Maheshpur Upazil', 'Sulla', 'Brahmanbaria Sad', 'Haim Char', 'Palash Paurashava', 'Daulat Khan', 'Beani Bazar', 'Fulbaria', 'Barisal S. (Kotwa', 'Golapganj', 'Comilla S. (Kotwa', 'Roypura', 'Kaliganj Upazila', 'Mahadebpur', 'Kuliar Char', 'Harinakundu Upaz', 'Dhanbari', 'Mitha Pukur', 'Bakshiganj', 'Roypur', 'Kaligang', 'Raumari', 'Ashuganj', 'S.pur', 'Kendua Thana', 'Maulvi Bazar S.', 'Royganj', 'Comilla Adarsha', 'Kanthalia', 'Titas', \"Cox's Bazar Sada\"}\n",
      "{'Dashina', 'Bochaganj', 'Paba', 'Anwara', 'Bagha', 'Chandanaish', 'Baksiganj', 'Kuliarchar', 'Tejgaon', 'Chhagalnaiya', 'Bagatipara', 'Daulatkhan', 'Gaurnadi', 'Bholahat', 'Sapahar', 'Jagannathpur', 'Gaibandha S.', 'Cox Bazar S', 'Madan', 'Durgapur', 'Kalai', 'Panchagarh S.', 'Goalandaghat', 'Atpara', 'Agailjhara', 'Charfasson', 'Sandwip', 'Dighalia', 'Dewanganj', 'Tentulia', 'Sonatala', 'Jaldhaka', 'Panchbibi', 'Rangamati S.', 'Burhanuddin', 'Kotchandpur', 'Kawkhali', 'Sharsha', 'Khansama', 'Faridpur', 'Ullahpara', 'Keraniganj', 'Koyra', 'Nakla', 'Sirajganj S.', 'Gomastapur', 'Kalapara', 'Gurudaspur', 'Sughatta', 'Phulbari', 'Atgharia', 'Bandar', 'Comilla S.', 'Kutubdia', 'Brahmanpara', 'Phulchhari', 'Dimla', 'Dohar', 'Bagherpara', 'Kurigram S.', 'Boalia', 'Betagi', 'Bajitpur', 'Hakimpur', 'Badalgachhi', 'Basail', 'Boda', 'Jhenaidaha S.', 'Mehendiganj', 'Puthia', 'Mongla', 'Khagrachhari', 'Nawabganj S.', 'Dighinala', 'Tazumuddin', 'Zakiganj', 'Kishoreganj S.', 'Astagram', 'Raiganj', 'Mohammadpur', 'Ramgarh', 'Bagmara', 'Shivalaya', 'Nandigram', 'Patuakhali S.', 'Baliakandi', 'Jhikargachha', 'Khoksa', 'Nazirpur', 'Ruma', 'Chandhina', 'Dacope', 'Dinajpur S.', 'Alfadanga', 'Rajasthali', 'Bhangura', 'Madaripur S.', 'Ghior', 'Nilphahari S.', 'Nalchity', 'Dhobaura', 'Mollahat', 'Sarail', 'Lakhai', 'Hossainpur', 'Chitalmari', 'Zanjira', 'Raipur', 'Kathalia', 'Narayanganj S.', 'Abhaynagar', 'Chilmari', 'Lalhonirhat S.', 'Atwari', 'Nangalkot', 'Jibannagar', 'Taraganj', 'Beanibazar', 'Delduar', 'Juraichhari', 'Raomari', 'Bagaichhari', 'Kasba', 'Mohanpur', 'Rajbari S.', 'Jhalakati S.', 'Meherpur S.', 'Parbatipur', 'Debhata', 'Aditmari', 'Joypurhat S.', 'Thakurgaon S.', 'Parshuram', 'Gazipur S.', 'Bishwamvarpur', 'Birganj', 'Gangachhara', 'Dupchanchia', 'Barkal', 'Rowangghhari', 'Patiya', 'Sadarpur', 'Bheramara', 'Sirajdikhan', 'Manikchhari', 'Akhaura', 'Gabtali', 'Belaichhari', 'Char Bhadrasan', 'Haripur', 'Mirpur', 'Hatibandha', 'Satkhira S.', 'Alamdanga', 'Mahalchhari', 'Haimchar', 'Tangail S.', 'Rupsa', 'Nannerchar', 'Lakshmichhari', 'Kaptai', 'Pakundia', 'Kazipur', 'Shailkupa', 'Mahadevpur', 'Thanchi', 'Bandarban S.', 'Banaripara', 'Bamna', 'Rajarhat', 'Tala', 'Bancharampur', 'Naikhongchhari', 'Lalmohan', 'Lohajang', 'Kamarkhanda', 'Manikganj S.', 'Charrajibpur', 'Savar', 'Maulvibazar S.', 'Naogaon S.', 'Bhurungamari', 'Madhukhali', 'Rampal', 'Santhia', 'Ramu', 'Ukhia', 'Shibchar', 'Companiganj', 'Belkuchi', 'Lalpur', 'Sullah', 'Ghoraghat', 'Terokhada', 'Madarganj', 'Gopalganj S.', 'Damudya', 'Patnitala', 'Mithamain', 'Assasuni', 'Nesarabad', 'Tungipara', 'Chatmohar', 'Sherpur', 'Lama', 'Haziganj', 'Paikgachha', 'Porsha', 'Dhamoirhat', 'Langadu', 'Panchhari', 'Rangpur S.', 'Hathazari', 'Damurhula', 'Sitakunda'}\n",
      "445\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Rename upazilas based on df_hh\n",
    "    print(set(df_hh.Upazila_Name.unique()) - set(df_ngt.NAME_4.unique()))\n",
    "    print(set(df_ngt.NAME_4.unique()) - set(df_hh.Upazila_Name.unique()))\n",
    "    print(len(df_ngt.NAME_4.unique()))\n",
    "    print(len(df_hh.Upazila_Name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "490bfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Rename all apart from Nasirabad, Parbattya Chattagram\n",
    "    df_hh.Upazila_Name = df_hh.Upazila_Name.replace({\n",
    "        'Maheshpur Upazil':'Maheshpur', \n",
    "        'Bakshiganj':'Baksiganj',\n",
    "        'Noakhali S. (Sudh':'Noakhali S.', \n",
    "        'Palash Paurashava':'Palash',\n",
    "        'Roypura':'Shibpur',\n",
    "        'Raumari':'Raomari', \n",
    "        'Barisal S. (Kotwa':'Barisal S.', \n",
    "        'Baghaichhari':'Bagaichhari', \n",
    "        'Char Rajibpur':'Charrajibpur', \n",
    "        'Kaligang':'Kaliganj', \n",
    "        'Sarankhola':'Morrelganj', \n",
    "        'Haim Char':'Haimchar',\n",
    "        'Sulla':'Sullah', \n",
    "        'Daulat Khan':'Daulatkhan',\n",
    "        'Comilla S. (Kotwa':'Comilla S.', \n",
    "        'Dhanbari':'Madhupur',\n",
    "        'Fulchhari':'Gaibandha S.', \n",
    "        'Kalukhali':'Kawkhali', \n",
    "        'Banchharampur':'Bancharampur', \n",
    "        'Comilla Adarsha':'Comilla S.', \n",
    "        'Kendua Thana':'Kendua', \n",
    "        'Kuliar Char':'Kuliarchar', \n",
    "        'Maulvibazar Sada':'Maulvibazar S.', \n",
    "        'Banari Para':'Banaripara', \n",
    "        \"Cox's Bazar Sada\":'Cox Bazar S', \n",
    "        'Mohanganj Thana':'Mohanganj', \n",
    "        'Austagram':'Astagram', \n",
    "        'Titas':'Faridpur', \n",
    "        'S.pur':'Sadarpur', \n",
    "        'Kanthalia':'Kathalia', \n",
    "        'Shib Char':'Shibchar', \n",
    "        'Royganj':'Raiganj', \n",
    "        'Harinakundu Upaz':'Harinakunda', \n",
    "        'Shajahanpur':'Shahjadpur', \n",
    "        'Golapganj':'Gopalganj S.', \n",
    "        'Beani Bazar':'Beanibazar', \n",
    "        'Maulvi Bazar S.':'Maulvibazar S.', \n",
    "        'Palong':'Shariatpur S.', \n",
    "        'Fulbaria':'Trishal', \n",
    "        'Naria':'Sakhipur', \n",
    "        'Roypur':'Raipur', \n",
    "        'Bagher Para':'Bagherpara', \n",
    "        'Kaliganj Upazila':'Kaliganj', \n",
    "        'Mitha Pukur':'Mithapukur', \n",
    "        'Mahadebpur':'Mahadevpur', \n",
    "        'Char Fasson':'Charfasson', \n",
    "        'Ashuganj':'Amtali', \n",
    "        'Brahmanbaria Sad':'Brahmanbaria S.', \n",
    "        \"Cox'S Bazar S.\":'Cox Bazar S', \n",
    "        'Jiban Nagar':'Jibannagar', \n",
    "        'Chakoria':'Chakaria',    \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5d3f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    print(set(df_hh.Upazila_Name.unique()) - set(df_ngt.NAME_4.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d252335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Duplicates: False\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    df_ngt = df_ngt.set_index(['NAME_4','year'])\n",
    "    df_ngt = df_ngt[~df_ngt.index.duplicated(keep='first')]\n",
    "    print('Has Duplicates:',df_ngt.index.has_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0a59f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    df_ngt = df_ngt.reset_index(['NAME_4','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c5234c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of rows after join equals number of rows before join: True\n",
      "   hh_n  month    year  mig_month_id  internal_mig district_mig  \\\n",
      "0     1    1.0  2011.0           NaN           0.0           NA   \n",
      "1     1    2.0  2011.0           NaN           0.0           NA   \n",
      "2     1    3.0  2011.0           NaN           0.0           NA   \n",
      "3     1    4.0  2011.0           NaN           0.0           NA   \n",
      "4     1    5.0  2011.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ... mobile  \\\n",
      "0                0.0          NA              0.0          0.0  ...    1.0   \n",
      "1                0.0          NA              0.0          0.0  ...    1.0   \n",
      "2                0.0          NA              0.0          0.0  ...    1.0   \n",
      "3                0.0          NA              0.0          0.0  ...    1.0   \n",
      "4                0.0          NA              0.0          0.0  ...    1.0   \n",
      "\n",
      "  radio   tv loss_flood pol_unrest incr_food_prices     yrmo both_out  \\\n",
      "0   0.0  1.0        0.0        0.0              0.0  2011-01      0.0   \n",
      "1   0.0  1.0        0.0        0.0              0.0  2011-02      0.0   \n",
      "2   0.0  1.0        0.0        0.0              0.0  2011-03      0.0   \n",
      "3   0.0  1.0        0.0        0.0              0.0  2011-04      0.0   \n",
      "4   0.0  1.0        0.0        0.0              0.0  2011-05      0.0   \n",
      "\n",
      "        NAME_4  nlight_mean  \n",
      "0  Bagerhat S.     2.508929  \n",
      "1  Bagerhat S.     2.508929  \n",
      "2  Bagerhat S.     2.508929  \n",
      "3  Bagerhat S.     2.508929  \n",
      "4  Bagerhat S.     2.508929  \n",
      "\n",
      "[5 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Merge\n",
    "    df_m = df_hh.merge(df_ngt,how='left',left_on=['Upazila_Name', 'year'], right_on=['NAME_4', 'year'])\n",
    "    print('Numbers of rows after join equals number of rows before join:', len(df_hh) == len(df_m))\n",
    "    print(df_m.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b065873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [hh_n, yrmo, month, year, mig_month_id, internal_mig, district_mig, international_mig, country_mig, remittances_mig, mig_history, Division_Code, Division_Name, District_Code, District_Name, Upazila, Upazila_Name, Union, Village, community_id, religion, Muslim, Hindu, Christian, language, Bengali, Urdu, Hindi, Tribal, Other, ethnicity, Bangali, Other_ethn, head_sex, Male, Female, head_age, age_17_24, age_25_34, age_35_48, age_49_70, age_71, head_literacy, head_educ, prim_educ, sec_educ, ter_educ, no_educ, mob_asset_value_own, machinery_value_own, savings, loans, land_type, land_size, produc_harvested, ag_subsidy, livestock_value, sl_benefited, income_rent, income_property, income_insurance, income_other, income, remittances_in, remittances_in_value, active_member, active_leader, satisfied_relat, satisfied_leave, satisfied_life, hhweight, popweight, pc_expm, pc_foodxm, pc_nonfxm, head_ocup, head_ocup_agg, Bihari, Tribal_ethn, mobile, radio, tv, loss_flood, pol_unrest, incr_food_prices, both_out, NAME_4, nlight_mean]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Extrapolate\n",
    "    df_m = df_m.set_index(['hh_n','yrmo'])\n",
    "    df_m['nlight_mean'] = fun.extrapolate(df=df_m['nlight_mean'],grouplevel=0)\n",
    "    df_m = df_m.reset_index()\n",
    "    print(df_m[df_m['nlight_mean'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bb4ff-2911-46de-9cf4-b7b57454c6e6",
   "metadata": {},
   "source": [
    "##### Other external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de4b3b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        NAME_2     yrmo  year  ID_2  PG_GCP_PPP_LI  \\\n",
      "0      Barisal  1989-01  1989     1       0.000000   \n",
      "1      Barisal  1989-02  1989     1       0.000000   \n",
      "2      Barisal  1989-03  1989     1       0.000000   \n",
      "3      Barisal  1989-04  1989     1       0.000000   \n",
      "4      Barisal  1989-05  1989     1       0.000000   \n",
      "...        ...      ...   ...   ...            ...   \n",
      "23803   Sylhet  2019-08  2019    64       1.808422   \n",
      "23804   Sylhet  2019-09  2019    64       1.808422   \n",
      "23805   Sylhet  2019-10  2019    64       1.808422   \n",
      "23806   Sylhet  2019-11  2019    64       1.808422   \n",
      "23807   Sylhet  2019-12  2019    64       1.808422   \n",
      "\n",
      "                                                geometry  gdis_n_disasters  \\\n",
      "0      MULTIPOLYGON (((90.43000031 22.49504471, 90.43...               0.0   \n",
      "1      MULTIPOLYGON (((90.43000031 22.49504471, 90.43...               0.0   \n",
      "2      MULTIPOLYGON (((90.43000031 22.49504471, 90.43...               0.0   \n",
      "3      MULTIPOLYGON (((90.43000031 22.49504471, 90.43...               0.0   \n",
      "4      MULTIPOLYGON (((90.43000031 22.49504471, 90.43...               0.0   \n",
      "...                                                  ...               ...   \n",
      "23803  POLYGON ((92.38098145 24.86759377, 92.37892151...               0.0   \n",
      "23804  POLYGON ((92.38098145 24.86759377, 92.37892151...               0.0   \n",
      "23805  POLYGON ((92.38098145 24.86759377, 92.37892151...               0.0   \n",
      "23806  POLYGON ((92.38098145 24.86759377, 92.37892151...               0.0   \n",
      "23807  POLYGON ((92.38098145 24.86759377, 92.37892151...               0.0   \n",
      "\n",
      "       gdis_disastertype_extreme_temp  gdis_disastertype_flood  \\\n",
      "0                                 0.0                      0.0   \n",
      "1                                 0.0                      0.0   \n",
      "2                                 0.0                      0.0   \n",
      "3                                 0.0                      0.0   \n",
      "4                                 0.0                      0.0   \n",
      "...                               ...                      ...   \n",
      "23803                             0.0                      0.0   \n",
      "23804                             0.0                      0.0   \n",
      "23805                             0.0                      0.0   \n",
      "23806                             0.0                      0.0   \n",
      "23807                             0.0                      0.0   \n",
      "\n",
      "       gdis_disastertype_landslide  ...  spei_3_tlag6  spei_3_tlag12  \\\n",
      "0                              0.0  ...      0.000000       0.000000   \n",
      "1                              0.0  ...      0.000000       0.000000   \n",
      "2                              0.0  ...      0.000000       0.000000   \n",
      "3                              0.0  ...      0.000000       0.000000   \n",
      "4                              0.0  ...      0.000000       0.000000   \n",
      "...                            ...  ...           ...            ...   \n",
      "23803                          0.0  ...      1.571526      -0.829604   \n",
      "23804                          0.0  ...      0.341584      -1.826815   \n",
      "23805                          0.0  ...      0.097748      -1.916294   \n",
      "23806                          0.0  ...     -0.065973      -1.669830   \n",
      "23807                          0.0  ...     -0.417949      -1.002374   \n",
      "\n",
      "       spei_6_tlag1  spei_6_tlag3  spei_6_tlag6  spei_6_tlag12  spei_12_tlag1  \\\n",
      "0          0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "1          0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "2          0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "3          0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "4          0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "...             ...           ...           ...            ...            ...   \n",
      "23803     -0.040966     -0.023275     -1.533650       0.052970      -0.925650   \n",
      "23804     -0.392136     -0.374668     -0.750651      -0.210879      -0.945959   \n",
      "23805     -0.214937     -0.040966      0.069444      -1.073636      -0.515529   \n",
      "23806     -0.038221     -0.392136     -0.023275      -1.748772       0.002309   \n",
      "23807     -0.081597     -0.214937     -0.374668      -1.817348       0.033580   \n",
      "\n",
      "       spei_12_tlag3  spei_12_tlag6  spei_12_tlag12  \n",
      "0           0.000000       0.000000        0.000000  \n",
      "1           0.000000       0.000000        0.000000  \n",
      "2           0.000000       0.000000        0.000000  \n",
      "3           0.000000       0.000000        0.000000  \n",
      "4           0.000000       0.000000        0.000000  \n",
      "...              ...            ...             ...  \n",
      "23803      -1.199009      -0.438598        0.511177  \n",
      "23804      -1.116892      -0.439109        0.135412  \n",
      "23805      -0.925650      -0.962179       -0.543647  \n",
      "23806      -0.945959      -1.199009       -0.567441  \n",
      "23807      -0.515529      -1.116892       -0.571681  \n",
      "\n",
      "[23808 rows x 975 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Load external dfs\n",
    "    df_ext = pd.read_csv(os.path.join(input_data_path,'external_district_Nov29_df.csv.zip'), compression='zip', header=0)\n",
    "    print(df_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a022b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Check for NAs\n",
    "    for col in df_ext.columns:\n",
    "        if df_ext[f'{col}'].isna().any() == True:\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417c634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Cox's Bazar\", 'Brahmanbaria', 'Chapai Nawabganj', 'Jhenaidah Zila T', 'Kishorgonj'}\n",
      "{'Brahamanbaria'}\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    print(set(df_m.District_Name.unique()) - set(df_ext.NAME_2.unique()))\n",
    "    print(set(df_ext.NAME_2.unique()) - set(df_m.District_Name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99231fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Cox's Bazar\", 'Brahmanbaria', 'Chapai Nawabganj', 'Jhenaidah Zila T', 'Kishorgonj'}\n",
      "No differences: True\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Rename admin 2 based on x_sub\n",
    "    print(set(df_m.District_Name.unique()) - set(df_ext.NAME_2.unique()))\n",
    "    \n",
    "    df_m['District_Name'] = df_m['District_Name'].replace({\n",
    "        'Chapai Nawabganj':'Nawabganj',\n",
    "        'Brahmanbaria':'Brahamanbaria',\n",
    "        \"Cox's Bazar\":\"Cox'S Bazar\",\n",
    "        'Jhenaidah Zila T':'Jhenaidah',\n",
    "        'Kishorgonj':'Kishoreganj',\n",
    "        \n",
    "    })\n",
    "    \n",
    "    print('No differences:', set(df_m.District_Name.unique()) == set(df_ext.NAME_2.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d7f8b49-75dd-4450-bc00-23afb6672144",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Transform spei to more intuitive scale\n",
    "    df_ext['spei_3_tlag12'] = df_ext['spei_3_tlag12']*(-1)\n",
    "    df_ext['spei_3'] = df_ext['spei_3']*(-1)\n",
    "    df_ext['spei_6_tlag12'] = df_ext['spei_6_tlag12']*(-1)\n",
    "    df_ext['spei_6'] = df_ext['spei_6']*(-1)\n",
    "    df_ext['spei_12_tlag12'] = df_ext['spei_12_tlag12']*(-1)\n",
    "    df_ext['spei_12'] = df_ext['spei_12']*(-1)\n",
    "\n",
    "    # Add dummies\n",
    "    df_ext[\"spei_3_tlag12_severe\"]  = (df_ext[\"spei_3_tlag12\"] > 1.5).astype(int)\n",
    "    df_ext[\"spei_3_tlag12_extreme\"] = (df_ext[\"spei_3_tlag12\"] > 2.0).astype(int)\n",
    "    df_ext[\"spei_3_severe\"]  = (df_ext[\"spei_3\"] > 1.5).astype(int)\n",
    "    df_ext[\"spei_3_extreme\"] = (df_ext[\"spei_3\"] > 2.0).astype(int)\n",
    "\n",
    "    # Add additional transformations for spei varialbes\n",
    "    df_ext = df_ext.sort_values([\"NAME_2\", \"yrmo\"])\n",
    "    \n",
    "    for col in [\"spei_3_severe\", \"spei_3_extreme\"]:\n",
    "        df_ext[f\"{col}_ts_decay_6\"] = (\n",
    "            df_ext.groupby(\"NAME_2\", sort=False)[col]\n",
    "              .transform(lambda s: fun.decay(fun.time_since_last_event(s), halflife=6))\n",
    "              .astype(float)\n",
    "        )\n",
    "        # 12-month lag\n",
    "        df_ext[f\"{col}_ts_decay_6_tlag12\"] = (\n",
    "            df_ext.groupby(\"NAME_2\", sort=False)[f'{col}_ts_decay_6'].shift(12)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8594e4a5-328a-4d16-ad77-a406f4bf6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Filter years\n",
    "    df_ext = df_ext[df_ext.year>2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ec1d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of rows after join equals number of rows before join: True\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Merge\n",
    "    df = df_m.merge(df_ext,how='left',left_on=['District_Name', 'yrmo'], right_on=['NAME_2', 'yrmo'])\n",
    "    \n",
    "    print('Numbers of rows after join equals number of rows before join:', len(df_hh) == len(df_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3176152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [hh_n, yrmo, month, year_x, mig_month_id, internal_mig, district_mig, international_mig, country_mig, remittances_mig, mig_history, Division_Code, Division_Name, District_Code, District_Name, Upazila, Upazila_Name, Union, Village, community_id, religion, Muslim, Hindu, Christian, language, Bengali, Urdu, Hindi, Tribal, Other, ethnicity, Bangali, Other_ethn, head_sex, Male, Female, head_age, age_17_24, age_25_34, age_35_48, age_49_70, age_71, head_literacy, head_educ, prim_educ, sec_educ, ter_educ, no_educ, mob_asset_value_own, machinery_value_own, savings, loans, land_type, land_size, produc_harvested, ag_subsidy, livestock_value, sl_benefited, income_rent, income_property, income_insurance, income_other, income, remittances_in, remittances_in_value, active_member, active_leader, satisfied_relat, satisfied_leave, satisfied_life, hhweight, popweight, pc_expm, pc_foodxm, pc_nonfxm, head_ocup, head_ocup_agg, Bihari, Tribal_ethn, mobile, radio, tv, loss_flood, pol_unrest, incr_food_prices, both_out, NAME_4, nlight_mean, NAME_2, year_y, ID_2, PG_GCP_PPP_LI, geometry, gdis_n_disasters, gdis_disastertype_extreme_temp, gdis_disastertype_flood, gdis_disastertype_landslide, gdis_disastertype_storm, gdis_disastertype_earthquake, spei_3, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 1070 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Check if merge successful\n",
    "    print(df[df.year_y != df.year_x])\n",
    "    df = df.drop(columns=['year_x'])\n",
    "    df = df.rename(columns={'year_y':'year'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f30998-8e09-437e-94e7-4ed54eb47ab9",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc78b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594324, 1069)\n",
      "   hh_n     yrmo  month  mig_month_id  internal_mig district_mig  \\\n",
      "0     1  2011-01    1.0           NaN           0.0           NA   \n",
      "1     1  2011-02    2.0           NaN           0.0           NA   \n",
      "2     1  2011-03    3.0           NaN           0.0           NA   \n",
      "3     1  2011-04    4.0           NaN           0.0           NA   \n",
      "4     1  2011-05    5.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "0                0.0          NA              0.0          0.0  ...   \n",
      "1                0.0          NA              0.0          0.0  ...   \n",
      "2                0.0          NA              0.0          0.0  ...   \n",
      "3                0.0          NA              0.0          0.0  ...   \n",
      "4                0.0          NA              0.0          0.0  ...   \n",
      "\n",
      "  spei_12_tlag6 spei_12_tlag12 spei_3_tlag12_severe spei_3_tlag12_extreme  \\\n",
      "0      0.188981       0.066370                    0                     0   \n",
      "1     -0.135336       0.038016                    0                     0   \n",
      "2     -0.153748       0.231013                    0                     0   \n",
      "3      0.309312       0.234005                    0                     0   \n",
      "4     -0.299233       0.434518                    0                     0   \n",
      "\n",
      "  spei_3_severe spei_3_extreme spei_3_severe_ts_decay_6  \\\n",
      "0             0              0                      0.0   \n",
      "1             0              0                      0.0   \n",
      "2             0              0                      0.0   \n",
      "3             0              0                      0.0   \n",
      "4             0              0                      0.0   \n",
      "\n",
      "  spei_3_severe_ts_decay_6_tlag12  spei_3_extreme_ts_decay_6  \\\n",
      "0                             0.0                        0.0   \n",
      "1                             0.0                        0.0   \n",
      "2                             0.0                        0.0   \n",
      "3                             0.0                        0.0   \n",
      "4                             0.0                        0.0   \n",
      "\n",
      "   spei_3_extreme_ts_decay_6_tlag12  \n",
      "0                               0.0  \n",
      "1                               0.0  \n",
      "2                               0.0  \n",
      "3                               0.0  \n",
      "4                               0.0  \n",
      "\n",
      "[5 rows x 1069 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Summarize shape\n",
    "    print(df.shape)\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b470e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   yrmo  hh_n  month  mig_month_id  internal_mig district_mig  \\\n",
      "0     1     1    1.0           NaN           0.0           NA   \n",
      "1     1     2    1.0           NaN           0.0           NA   \n",
      "2     1     3    1.0           NaN           0.0           NA   \n",
      "3     1     4    1.0           NaN           0.0           NA   \n",
      "4     1     5    1.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "0                0.0          NA              0.0          0.0  ...   \n",
      "1                0.0          NA              0.0          0.0  ...   \n",
      "2                0.0          NA              0.0          0.0  ...   \n",
      "3                0.0          NA              0.0          0.0  ...   \n",
      "4                0.0          NA              0.0          0.0  ...   \n",
      "\n",
      "  spei_12_tlag6 spei_12_tlag12 spei_3_tlag12_severe spei_3_tlag12_extreme  \\\n",
      "0      0.188981        0.06637                    0                     0   \n",
      "1      0.188981        0.06637                    0                     0   \n",
      "2      0.188981        0.06637                    0                     0   \n",
      "3      0.188981        0.06637                    0                     0   \n",
      "4      0.188981        0.06637                    0                     0   \n",
      "\n",
      "  spei_3_severe spei_3_extreme spei_3_severe_ts_decay_6  \\\n",
      "0             0              0                      0.0   \n",
      "1             0              0                      0.0   \n",
      "2             0              0                      0.0   \n",
      "3             0              0                      0.0   \n",
      "4             0              0                      0.0   \n",
      "\n",
      "  spei_3_severe_ts_decay_6_tlag12  spei_3_extreme_ts_decay_6  \\\n",
      "0                             0.0                        0.0   \n",
      "1                             0.0                        0.0   \n",
      "2                             0.0                        0.0   \n",
      "3                             0.0                        0.0   \n",
      "4                             0.0                        0.0   \n",
      "\n",
      "   spei_3_extreme_ts_decay_6_tlag12  \n",
      "0                               0.0  \n",
      "1                               0.0  \n",
      "2                               0.0  \n",
      "3                               0.0  \n",
      "4                               0.0  \n",
      "\n",
      "[5 rows x 1069 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Make unique monthid and set index\n",
    "    df = df.set_index(['yrmo','hh_n'])\n",
    "    month_id = list(range(1,len(df.index.get_level_values(0).unique())+1))\n",
    "    df.index = df.index.set_levels(month_id, level='yrmo')\n",
    "    \n",
    "    df = df.sort_index(level=0).reset_index()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff539008",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Add string for plotting\n",
    "    df[\"date_str\"] = df.year.astype(int).astype(str) + \"-\" + df.month.astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bdac47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      internal_mig  international_mig  year\n",
      "yrmo                                       \n",
      "89            30.0                4.0  2018\n",
      "90            27.0                4.0  2018\n",
      "91            28.0               10.0  2018\n",
      "92            39.0               11.0  2018\n",
      "93            45.0                6.0  2018\n",
      "94            29.0               10.0  2018\n",
      "95            27.0                7.0  2018\n",
      "96            24.0                9.0  2018\n",
      "97            31.0                7.0  2019\n",
      "98            22.0                2.0  2019\n",
      "99             1.0                1.0  2019\n",
      "100            1.0                0.0  2019\n",
      "101            0.0                0.0  2019\n",
      "102            0.0                0.0  2019\n",
      "103            0.0                0.0  2019\n",
      "104            0.0                0.0  2019\n",
      "105            0.0                0.0  2019\n",
      "106            0.0                0.0  2019\n",
      "107            0.0                0.0  2019\n",
      "108            0.0                0.0  2019\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Check where to cut off end months\n",
    "    print(df.groupby('yrmo').aggregate({'internal_mig':'sum','international_mig':'sum','year':'first'}).loc[89:])\n",
    "    df.groupby('yrmo').aggregate({'both_out':'sum'}).loc[87:].plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8810ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   yrmo  hh_n  month  mig_month_id  internal_mig district_mig  \\\n",
      "0     1     1    1.0           NaN           0.0           NA   \n",
      "1     1     2    1.0           NaN           0.0           NA   \n",
      "2     1     3    1.0           NaN           0.0           NA   \n",
      "3     1     4    1.0           NaN           0.0           NA   \n",
      "4     1     5    1.0           NaN           0.0           NA   \n",
      "\n",
      "   international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "0                0.0          NA              0.0          0.0  ...   \n",
      "1                0.0          NA              0.0          0.0  ...   \n",
      "2                0.0          NA              0.0          0.0  ...   \n",
      "3                0.0          NA              0.0          0.0  ...   \n",
      "4                0.0          NA              0.0          0.0  ...   \n",
      "\n",
      "  spei_12_tlag12 spei_3_tlag12_severe spei_3_tlag12_extreme spei_3_severe  \\\n",
      "0        0.06637                    0                     0             0   \n",
      "1        0.06637                    0                     0             0   \n",
      "2        0.06637                    0                     0             0   \n",
      "3        0.06637                    0                     0             0   \n",
      "4        0.06637                    0                     0             0   \n",
      "\n",
      "  spei_3_extreme spei_3_severe_ts_decay_6 spei_3_severe_ts_decay_6_tlag12  \\\n",
      "0              0                      0.0                             0.0   \n",
      "1              0                      0.0                             0.0   \n",
      "2              0                      0.0                             0.0   \n",
      "3              0                      0.0                             0.0   \n",
      "4              0                      0.0                             0.0   \n",
      "\n",
      "  spei_3_extreme_ts_decay_6  spei_3_extreme_ts_decay_6_tlag12  date_str  \n",
      "0                       0.0                               0.0    2011-1  \n",
      "1                       0.0                               0.0    2011-1  \n",
      "2                       0.0                               0.0    2011-1  \n",
      "3                       0.0                               0.0    2011-1  \n",
      "4                       0.0                               0.0    2011-1  \n",
      "\n",
      "[5 rows x 1070 columns]\n",
      "        yrmo  hh_n  month  mig_month_id  internal_mig district_mig  \\\n",
      "528283    96  5499   12.0           NaN           0.0           NA   \n",
      "528284    96  5500   12.0           NaN           0.0           NA   \n",
      "528285    96  6501   12.0           NaN           0.0           NA   \n",
      "528286    96  6502   12.0           NaN           0.0           NA   \n",
      "528287    96  6503   12.0           NaN           0.0           NA   \n",
      "\n",
      "        international_mig country_mig  remittances_mig  mig_history  ...  \\\n",
      "528283                0.0          NA              0.0          0.0  ...   \n",
      "528284                0.0          NA              0.0          0.0  ...   \n",
      "528285                0.0          NA              0.0          0.0  ...   \n",
      "528286                0.0          NA              0.0          0.0  ...   \n",
      "528287                0.0          NA              0.0          1.0  ...   \n",
      "\n",
      "       spei_12_tlag12 spei_3_tlag12_severe spei_3_tlag12_extreme  \\\n",
      "528283      -1.029053                    0                     0   \n",
      "528284      -1.029053                    0                     0   \n",
      "528285      -1.465199                    0                     0   \n",
      "528286      -1.465199                    0                     0   \n",
      "528287      -1.465199                    0                     0   \n",
      "\n",
      "       spei_3_severe spei_3_extreme spei_3_severe_ts_decay_6  \\\n",
      "528283             0              0                 0.890899   \n",
      "528284             0              0                 0.890899   \n",
      "528285             0              0                 0.890899   \n",
      "528286             0              0                 0.890899   \n",
      "528287             0              0                 0.890899   \n",
      "\n",
      "       spei_3_severe_ts_decay_6_tlag12 spei_3_extreme_ts_decay_6  \\\n",
      "528283                        0.314980                  0.890899   \n",
      "528284                        0.314980                  0.890899   \n",
      "528285                        0.008769                  0.000000   \n",
      "528286                        0.008769                  0.000000   \n",
      "528287                        0.008769                  0.000000   \n",
      "\n",
      "        spei_3_extreme_ts_decay_6_tlag12  date_str  \n",
      "528283                               0.0   2018-12  \n",
      "528284                               0.0   2018-12  \n",
      "528285                               0.0   2018-12  \n",
      "528286                               0.0   2018-12  \n",
      "528287                               0.0   2018-12  \n",
      "\n",
      "[5 rows x 1070 columns]\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Filter years (December 2018)\n",
    "    df = df.set_index(['yrmo','hh_n']).loc[:96,:].reset_index()\n",
    "    print(df.head())\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782bc7f-efcf-45b3-a3c9-2503747c5f91",
   "metadata": {},
   "source": [
    "#### Apply additional transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe5ade41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # LN income variables\n",
    "    colshh = [\n",
    "        'mob_asset_value_own',\n",
    "        'machinery_value_own',\n",
    "        'savings',\n",
    "        'loans',\n",
    "        'land_size',\n",
    "        'pc_expm',\n",
    "        'produc_harvested',\n",
    "        'livestock_value',\n",
    "        'income',\n",
    "        'remittances_in_value',\n",
    "    ]\n",
    "    \n",
    "    for col in colshh:\n",
    "        df[f'ln_{col}'] = fun.ln(df[f'{col}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65e25a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lagging ln_mob_asset_value_own\n",
      "lagging ln_machinery_value_own\n",
      "lagging ln_savings\n",
      "lagging ln_loans\n",
      "lagging ln_land_size\n",
      "lagging ln_pc_expm\n",
      "lagging ag_subsidy\n",
      "lagging ln_livestock_value\n",
      "lagging sl_benefited\n",
      "lagging ln_income\n",
      "lagging ln_remittances_in_value\n",
      "lagging mobile\n",
      "lagging radio\n",
      "lagging tv\n"
     ]
    }
   ],
   "source": [
    "if merge_data:\n",
    "    # Lag income variables\n",
    "    df = df.set_index(['hh_n', 'yrmo'])\n",
    "    \n",
    "    for var in [\n",
    "        'ln_mob_asset_value_own',\n",
    "        'ln_machinery_value_own',\n",
    "        'ln_savings',\n",
    "        'ln_loans',\n",
    "        'ln_land_size',\n",
    "        'ln_pc_expm',\n",
    "        #'ln_produc_harvested', - land size correlation\n",
    "        'ag_subsidy',\n",
    "        'ln_livestock_value',\n",
    "        'sl_benefited',\n",
    "        'ln_income',\n",
    "        'ln_remittances_in_value',\n",
    "        'mobile', 'radio', 'tv'\n",
    "    ]:\n",
    "        print('lagging', var)\n",
    "        df[f'{var}_tlag12'] = df.groupby(level=0)[f'{var}'].shift(12)\n",
    "        \n",
    "    df = df.reset_index()\n",
    "    df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d88c371b-cc29-4e3a-bd3d-6a8dde2b1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_data:\n",
    "    # Only save columns/variables that are used\n",
    "    save_cols = [\n",
    "        'NAME_2',\n",
    "        'District_Name',\n",
    "        'Division_Name',\n",
    "        'year',\n",
    "        'hh_n',\n",
    "        'yrmo',\n",
    "        'internal_mig',\n",
    "        'international_mig',\n",
    "        'both_out',\n",
    "        'date_str',\n",
    "        'head_age',\n",
    "        'head_sex',\n",
    "        'prim_educ',\n",
    "        'sec_educ',\n",
    "        'ter_educ',\n",
    "        'no_educ',\n",
    "        'religion',\n",
    "        'ethnicity',\n",
    "        'mig_history',\n",
    "        'mobile',\n",
    "        'radio',\n",
    "        'tv',\n",
    "        'head_age',\n",
    "        'head_sex',\n",
    "        # education of hh\n",
    "        'prim_educ',\n",
    "        'sec_educ',\n",
    "        'ter_educ',\n",
    "        'no_educ',\n",
    "        'religion',\n",
    "        'ethnicity',\n",
    "        'head_ocup_agg',\n",
    "        # Migration history\n",
    "        'mig_history',\n",
    "        # Forms of income\n",
    "        'ln_mob_asset_value_own',\n",
    "        'ln_savings',\n",
    "        'ln_loans',\n",
    "        'ln_land_size',\n",
    "        'ln_pc_expm',\n",
    "        'ag_subsidy',\n",
    "        'ln_livestock_value',\n",
    "        'sl_benefited',\n",
    "        'ln_income',\n",
    "        'ln_remittances_in_value',\n",
    "        'mob_asset_value_own', # Non-logged versions\n",
    "        'savings',\n",
    "        'loans',\n",
    "        'land_size',\n",
    "        'pc_expm',\n",
    "        'livestock_value',\n",
    "        'income',\n",
    "        'remittances_in_value',\n",
    "        # Additional\n",
    "        'active_member',\n",
    "        'active_leader',\n",
    "        'satisfied_leave',\n",
    "        'satisfied_life',\n",
    "        'mobile',\n",
    "        'radio',\n",
    "        'tv',\n",
    "        'nlight_mean',\n",
    "        'PG_GCP_PPP_LI',\n",
    "        'ged_best_ns_tlag12',\n",
    "        'ged_best_sb_tlag12',\n",
    "        'ged_best_osv_tlag12',\n",
    "        'ged_best_ns_splag1_tlag12',\n",
    "        'ged_best_sb_splag1_tlag12',\n",
    "        'ged_best_osv_splag1_tlag12',\n",
    "        'acled_prrio_count_tlag12',\n",
    "        'acled_prrio_count_splag1_tlag12',\n",
    "        'acled_prex_count_tlag12',\n",
    "        'acled_prex_count_splag1_tlag12',\n",
    "        'ged_best_ns_ts_decay_6_tlag12',\n",
    "        'ged_best_osv_ts_decay_6_tlag12',\n",
    "        'ged_best_sb_ts_decay_6_tlag12',\n",
    "        'acled_prrio_count_ts_decay_6_tlag12',\n",
    "        'acled_prrio_count_splag1_ts_decay_6_tlag12',\n",
    "        'acled_prex_count_ts_decay_6_tlag12',\n",
    "        'acled_prex_count_splag1_ts_decay_6_tlag12',\n",
    "        'deco_best_ns_tlag12',\n",
    "        'deco_best_sb_tlag12',\n",
    "        'deco_best_osv_tlag12',\n",
    "        'deco_best_ns_splag1_tlag12',\n",
    "        'deco_best_sb_splag1_tlag12',\n",
    "        'deco_best_osv_splag1_tlag12',\n",
    "        'deco_best_ns_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "        'deco_best_sb_ts_decay_6_tlag12',\n",
    "        'gdis_n_disasters_tlag12',\n",
    "        'flood_dummy_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_earthquake_ts_decay_6_tlag12',\n",
    "        'spei_3_tlag12',\n",
    "        'spei_3_severe_ts_decay_6_tlag12',\n",
    "        'spei_3_extreme_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_landslide_tlag12',\n",
    "        'gdis_disastertype_storm_tlag12',\n",
    "        'gdis_disastertype_earthquake_tlag12',\n",
    "        'gdis_disastertype_flood_tlag12',\n",
    "        'acled_prex_count_msum_12_tlag12', 'acled_prrio_count_msum_12_tlag12', 'acled_prrio_count_splag1_msum_12_tlag12', 'deco_best_ns_msum_12_tlag12', 'deco_best_ns_splag1_msum_12_tlag12', 'deco_best_ns_splag1_ts_decay_6_tlag12', 'deco_best_osv_msum_12_tlag12', 'deco_best_osv_splag1_msum_12_tlag12', 'deco_best_osv_splag1_ts_decay_6_tlag12', 'deco_best_sb_msum_12_tlag12', 'deco_best_sb_splag1_msum_12_tlag12', 'deco_best_sb_splag1_ts_decay_6_tlag12', 'gdis_disastertype_earthquake_msum_12_tlag12', 'gdis_disastertype_flood_msum_12_tlag12', 'gdis_disastertype_landslide_msum_12_tlag12', 'gdis_disastertype_storm_msum_12_tlag12', 'ged_best_ns_msum_12_tlag12', 'ged_best_ns_splag1_msum_12_tlag12', 'ged_best_ns_splag1_ts_decay_6_tlag12', 'ged_best_osv_msum_12_tlag12', 'ged_best_osv_splag1_msum_12_tlag12', 'ged_best_osv_splag1_ts_decay_6_tlag12', 'ged_best_sb_msum_12_tlag12', 'ged_best_sb_splag1_msum_12_tlag12', 'ged_best_sb_splag1_ts_decay_6_tlag12', 'language'\n",
    "    ]\n",
    "\n",
    "    # Make sure there are no duplicates\n",
    "    save_cols_unique = list(set(save_cols))    \n",
    "\n",
    "    df[save_cols_unique].to_csv(os.path.join(input_data_path,'merged_df_ncc.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a78237",
   "metadata": {},
   "source": [
    "## Descriptives: DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ff87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if do_descr:\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(input_data_path,'merged_df_ncc.csv'))\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8b1fa-b175-4ef2-98dd-4f64f10ba17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    \n",
    "    print(df.year.min(),df.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484043b-111f-497b-94c8-f77d0fef2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    cols = [\"international_mig\", \"internal_mig\"]\n",
    "    \n",
    "    # Collapse to household level \n",
    "    g = df.groupby(\"hh_n\")[cols].sum().fillna(0)\n",
    "    intl   = g[\"international_mig\"] > 0\n",
    "    inter  = g[\"internal_mig\"] > 0\n",
    "    \n",
    "    # Shares (can overlap)\n",
    "    share_intl  = intl.mean() # share of HH with any international migration\n",
    "    share_inter = inter.mean() # share of HH with any internal migration\n",
    "    \n",
    "    # Exclusive breakdown \n",
    "    share_intl_only  = (intl & ~inter).mean()\n",
    "    share_inter_only = (~intl & inter).mean()\n",
    "    share_both       = (intl & inter).mean()\n",
    "    share_none       = (~(intl | inter)).mean()\n",
    "    \n",
    "    # Any migration\n",
    "    share_any = (intl | inter).mean()\n",
    "    \n",
    "    # Counts\n",
    "    n_hh   = len(g)\n",
    "    counts = {\n",
    "        \"intl\": int(intl.sum()),\n",
    "        \"inter\": int(inter.sum()),\n",
    "        \"intl_only\": int((intl & ~inter).sum()),\n",
    "        \"inter_only\": int((~intl & inter).sum()),\n",
    "        \"both\": int((intl & inter).sum()),\n",
    "        \"none\": int((~(intl | inter)).sum()),\n",
    "        \"any\": int((intl | inter).sum()),\n",
    "        \"total_households\": n_hh,\n",
    "    }\n",
    "    \n",
    "    print(f\"Share intl:  {share_intl:.1%}\")\n",
    "    print(f\"Share internal: {share_inter:.1%}\")\n",
    "    print(f\"Intl only:  {share_intl_only:.1%}\")\n",
    "    print(f\"Internal only: {share_inter_only:.1%}\")\n",
    "    print(f\"Both: {share_both:.1%}\")\n",
    "    print(f\"None: {share_none:.1%}\")\n",
    "    print(f\"Any migration: {share_any:.1%}\")\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98885ae6-d42d-4510-9957-40147778f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    cols = [\"international_mig\", \"internal_mig\"]\n",
    "    \n",
    "    g = (df.groupby(\"hh_n\")[cols]\n",
    "           .sum()\n",
    "           .fillna(0)\n",
    "           .astype(int))\n",
    "    \n",
    "    total_events = g.sum(axis=1)\n",
    "    \n",
    "    # How many HHs had >1 migration events (of any type)?\n",
    "    n_gt1_any   = int((total_events > 1).sum())\n",
    "    share_gt1   = (total_events > 1).mean()\n",
    "    n_hh        = len(g)\n",
    "    \n",
    "    print(f\">1 events (any type): {n_gt1_any} / {n_hh} ({share_gt1:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a824774-f2ff-43d6-8e5f-8ad51f3da8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE S3 Appendix:\n",
    "if do_descr:\n",
    "    def make_migration_category(row): # Make category for table S3 in Appendix\n",
    "        if row['international_mig'] == 1:\n",
    "            return \"international movements\"\n",
    "        elif row['internal_mig'] == 1:\n",
    "            return \"internal movements\"\n",
    "        else:\n",
    "            return \"immobility\"\n",
    "    \n",
    "    df['migration_all'] = df.apply(make_migration_category, axis=1)\n",
    "    \n",
    "    counts = df['migration_all'].value_counts()\n",
    "    percentages = df['migration_all'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    df_descr = pd.DataFrame({\n",
    "        \"Categories\": counts.index,\n",
    "        \"Counts\": counts.values,\n",
    "        \"Percentage\": percentages.round(2).values\n",
    "    })\n",
    "    \n",
    "    # Add total row\n",
    "    df_descr.loc[len(df_descr)] = [\"total\", counts.sum(), 100.00]\n",
    "\n",
    "    # Save csv\n",
    "    df_descr.to_csv(os.path.join(output_paths['descriptives'],\n",
    "                                 f\"SI_tabs3_summary_stats_both_out_fulldf.csv\"),\n",
    "                    index=False)\n",
    "\n",
    "    # Save tex\n",
    "    tex = df_descr.to_latex(index=False)\n",
    "    with open(os.path.join(output_paths['descriptives'],\n",
    "                           f\"SI_tabs3_summary_stats_both_out_fulldf.tex\"), \"w\") as f:\n",
    "        f.write(tex)\n",
    "\n",
    "    print(f\"Wrote scores table to {output_paths['descriptives']}\")\n",
    "    print(df_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4360cbd-b1ce-450c-b884-3cf3f1004bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE S4 Appendix:\n",
    "if do_descr:\n",
    "    events_per_year = (\n",
    "        df.groupby('year')[['international_mig', 'internal_mig']]\n",
    "          .sum()\n",
    "          .assign(any=lambda d: d['international_mig'] + d['internal_mig'])\n",
    "    ).reset_index()\n",
    "\n",
    "    # \n",
    "    events_per_year['Household Share'] = (events_per_year['any'].div(5503))*100\n",
    "    events_per_year[['international_mig','internal_mig','any']]= events_per_year[['international_mig','internal_mig','any']].astype(int)\n",
    "\n",
    "    # Rename\n",
    "    events_per_year = events_per_year.rename(columns={\n",
    "        \"year\": \"Year\",\n",
    "        \"international_mig\": \"Households (HH) w. international migration\",\n",
    "        \"internal_mig\": \"HH w. internal migration\",\n",
    "        \"any\": \"HH w. any migration\",\n",
    "        \"Household Share\": \"Share of HH w. any migration\"\n",
    "    }).round(2)\n",
    "\n",
    "    # Save tex\n",
    "    tex = events_per_year.to_latex(index=False)\n",
    "    with open(os.path.join(output_paths['descriptives'],\n",
    "                           f\"SI_tabs4_summary_stats_yearly.tex\"), \"w\") as f:\n",
    "        f.write(tex)\n",
    "\n",
    "    print(f\"Wrote scores table to {output_paths['descriptives']}\")\n",
    "    print(events_per_year)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de362a6-ecb8-455f-bb41-bb00f704a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE S5 Appendix:\n",
    "if do_descr:\n",
    "    \n",
    "    # Count households per district\n",
    "    district_counts = df.groupby('District_Name')['both_out'].sum().reset_index()\n",
    "    district_counts = district_counts.rename(columns={'both_out': 'Households'})\n",
    "\n",
    "    # Add percentage\n",
    "    total = district_counts['Households'].sum()\n",
    "    district_counts['Share (%)'] = (district_counts['Households'] / total * 100).round(2)\n",
    "\n",
    "    # Sort for readability (optional)\n",
    "    district_counts = district_counts.sort_values('Households', ascending=False)\n",
    "\n",
    "    # Export LaTeX table\n",
    "    tex = district_counts.to_latex(\n",
    "        index=False,\n",
    "        caption=\"Number and percentage of surveyed households by district.\",\n",
    "        label=\"tab:district_distribution\",\n",
    "        longtable=False\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(output_paths['descriptives'], \"SI_tabs5_district_distribution.tex\"), \"w\") as f:\n",
    "        f.write(tex)\n",
    "\n",
    "    print(district_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6634a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "\n",
    "    # Households per month that migrated\n",
    "    fig_scale=1\n",
    "    save_fig = False\n",
    "    name = 'both_outcomes'\n",
    "\n",
    "    # Set up figure space.\n",
    "    fig = plt.figure(figsize=(20 * fig_scale, 7.5 * fig_scale))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Set up y-axis lim.\n",
    "    #plt.ylim([ymin, ymax])\n",
    "    \n",
    "\n",
    "    # Format grid lines.\n",
    "    ax.grid(\n",
    "        which=\"major\",\n",
    "        axis=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        dashes=(2, 3),\n",
    "        lw=1,\n",
    "        color=\"black\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    # Remove plot framelines.\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "    labels = [item for item in ax.get_xticklabels()]\n",
    "\n",
    "    ax.set_xticks(df.reset_index().groupby('yrmo').aggregate({'both_out':'sum','date_str':'first'}).index[::4])\n",
    "    ax.set_xticklabels(df.reset_index().groupby('yrmo').aggregate({'both_out':'sum','date_str':'first'}).date_str[::4])\n",
    "\n",
    "    df.reset_index().groupby('yrmo').aggregate({'both_out':'sum','date_str':'first'}).both_out.plot(color='black')\n",
    "    #df.reset_index().groupby('yrmo').aggregate({'both_out':'sum'}).both_out.plot(label='International')\n",
    "\n",
    "    ax.set_xlabel('Year-Month')\n",
    "    ax.set_ylabel('Number of migrant-sending households')\n",
    "    \n",
    "    plt.grid(axis = 'x', linewidth=0.5)\n",
    "\n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(output_paths['descriptives'],f'lineplot_{name}.png'), bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "        print(f\"Wrote to path.\")\n",
    "        plt.show(fig)\n",
    "    else:\n",
    "        plt.show(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88704f06",
   "metadata": {},
   "source": [
    "## Define outcome, features and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outcome\n",
    "Y_outcome = 'both_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "# Baseline model, features for aspiration + ability\n",
    "\n",
    "feat_baseline = [\n",
    "    'head_age',\n",
    "    'head_sex',\n",
    "    # education of hh\n",
    "    'prim_educ',\n",
    "    'sec_educ',\n",
    "    'ter_educ',\n",
    "    'no_educ',\n",
    "    'religion',\n",
    "    'ethnicity',\n",
    "    'head_ocup_agg',\n",
    "    # Migration history\n",
    "    'mig_history',\n",
    "    # Forms of income\n",
    "    'ln_mob_asset_value_own',\n",
    "    'ln_savings',\n",
    "    'ln_loans',\n",
    "    'ln_land_size',\n",
    "    'ln_pc_expm',\n",
    "    'ag_subsidy',\n",
    "    'ln_livestock_value',\n",
    "    'sl_benefited',\n",
    "    'ln_income',\n",
    "    'ln_remittances_in_value',\n",
    "    # Additional\n",
    "    'active_member',\n",
    "    'active_leader',\n",
    "    'satisfied_leave',\n",
    "    'satisfied_life',\n",
    "    'mobile',\n",
    "    'radio',\n",
    "    'tv'\n",
    "  ]\n",
    "\n",
    "feat_baseline_district = [\n",
    "    'nlight_mean',\n",
    "    'PG_GCP_PPP_LI',\n",
    "] + feat_baseline\n",
    "\n",
    "feat_violence_agg = [\n",
    "    'ged_best_ns_tlag12',\n",
    "    'ged_best_sb_tlag12',\n",
    "    'ged_best_osv_tlag12',\n",
    "    'ged_best_ns_splag1_tlag12',\n",
    "    'ged_best_sb_splag1_tlag12',\n",
    "    'ged_best_osv_splag1_tlag12',\n",
    "    'acled_prrio_count_tlag12',\n",
    "    'acled_prrio_count_splag1_tlag12',\n",
    "    'acled_prex_count_tlag12',\n",
    "    'acled_prex_count_splag1_tlag12',\n",
    "    'ged_best_ns_ts_decay_6_tlag12',\n",
    "    'ged_best_osv_ts_decay_6_tlag12',\n",
    "    'ged_best_sb_ts_decay_6_tlag12',\n",
    "    'acled_prrio_count_ts_decay_6_tlag12',\n",
    "    'acled_prrio_count_splag1_ts_decay_6_tlag12',\n",
    "    'acled_prex_count_ts_decay_6_tlag12',\n",
    "    'acled_prex_count_splag1_ts_decay_6_tlag12',\n",
    "]\n",
    "\n",
    "feat_elect_vio_agg = [\n",
    "    'deco_best_ns_tlag12',\n",
    "    'deco_best_sb_tlag12',\n",
    "    'deco_best_osv_tlag12',\n",
    "    'deco_best_ns_splag1_tlag12',\n",
    "    'deco_best_sb_splag1_tlag12',\n",
    "    'deco_best_osv_splag1_tlag12',\n",
    "    'deco_best_ns_ts_decay_6_tlag12',\n",
    "    'deco_best_osv_ts_decay_6_tlag12',\n",
    "    'deco_best_sb_ts_decay_6_tlag12',\n",
    "]\n",
    "\n",
    "feat_disaster_agg = [\n",
    "    'gdis_n_disasters_tlag12',\n",
    "    'flood_dummy_ts_decay_6_tlag12',\n",
    "    'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "    'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "    'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "    'spei_3_severe_ts_decay_6_tlag12'\n",
    "]\n",
    "\n",
    "# Main models\n",
    "feat_district_violence = feat_baseline_district + feat_violence_agg + feat_elect_vio_agg # Includes also deco.\n",
    "feat_district_disaster = feat_baseline_district + feat_disaster_agg\n",
    "feat_district_violence_disaster = feat_district_violence + feat_disaster_agg\n",
    "\n",
    "# Add all the defined feature lists here to check for duplicates.\n",
    "featurelists = [ \n",
    "    feat_baseline_district,\n",
    "    feat_district_violence,\n",
    "    feat_district_disaster,\n",
    "    feat_district_violence_disaster,\n",
    "]\n",
    "# Check for duplicates\n",
    "for ls in featurelists:\n",
    "    print('has duplicates:',fun.checkIfDuplicates_1(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models.\n",
    "\n",
    "baseline_district  = {\n",
    "  'model_name': 'baseline_district',\n",
    "  'cols_features': feat_baseline_district,\n",
    "  'col_outcome': Y_outcome,\n",
    "}\n",
    "\n",
    "district_violence = {\n",
    "  'model_name': 'district_violence',\n",
    "  'cols_features': feat_district_violence,\n",
    "  'col_outcome': Y_outcome,\n",
    "}\n",
    "\n",
    "\n",
    "district_disaster = {\n",
    "  'model_name': 'district_disaster',\n",
    "  'cols_features': feat_district_disaster,\n",
    "  'col_outcome': Y_outcome,\n",
    "}\n",
    "\n",
    "district_violence_disaster = {\n",
    "  'model_name': 'district_violence_disaster',\n",
    "  'cols_features': feat_district_violence_disaster,\n",
    "  'col_outcome': Y_outcome,\n",
    "}\n",
    "\n",
    "# Include all models in list\n",
    "allmodels = [\n",
    "    baseline_district,\n",
    "    district_violence,\n",
    "    district_disaster,\n",
    "    district_violence_disaster,\n",
    "]\n",
    "\n",
    "for i in allmodels:\n",
    "    print('Model name:', i['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95c7f1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['hh_n','yrmo']).sort_index(level=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00682301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.reset_index().groupby(['hh_n','year']).agg(\n",
    "        {\n",
    "            # Target\n",
    "            'both_out':'max',\n",
    "\n",
    "            # For random/fixed effects - Admin name. \n",
    "            'NAME_2':'first',\n",
    "            'District_Name':'first',\n",
    "            'Division_Name':'first',\n",
    "\n",
    "            # Baseline\n",
    "            'nlight_mean':'mean',\n",
    "            'PG_GCP_PPP_LI':'mean',\n",
    "            'head_age':'first',\n",
    "            'head_sex':'first',\n",
    "            'prim_educ':'first',\n",
    "            'sec_educ':'first',\n",
    "            'ter_educ':'first',\n",
    "            'no_educ':'first',\n",
    "            'religion':'first',\n",
    "            'language':'first',\n",
    "            'ethnicity':'first',\n",
    "            'head_ocup_agg':'first',\n",
    "            'mig_history':'first',\n",
    "            'ln_mob_asset_value_own':'first',\n",
    "            'mob_asset_value_own':'first',\n",
    "            'ln_savings':'first',\n",
    "            'savings':'first',\n",
    "            'ln_loans':'first',\n",
    "            'loans':'first',\n",
    "            'ln_land_size':'first',\n",
    "            'land_size':'first',\n",
    "            'ln_pc_expm':'first',\n",
    "            'pc_expm':'first',\n",
    "            'ag_subsidy':'first',\n",
    "            'ln_livestock_value':'first',\n",
    "            'livestock_value':'first',\n",
    "            'sl_benefited':'first',\n",
    "            'ln_income':'first',\n",
    "            'income':'first',\n",
    "            'ln_remittances_in_value':'first',\n",
    "            'remittances_in_value':'first',\n",
    "            'active_member':'first',\n",
    "            'active_leader':'first',\n",
    "            'satisfied_leave':'first',\n",
    "            'satisfied_life':'first',\n",
    "            'mobile':'first',\n",
    "            'tv':'first',\n",
    "            'radio':'first',\n",
    "\n",
    "            # Violence\n",
    "            'ged_best_ns_tlag12':'sum',\n",
    "            'ged_best_sb_tlag12':'sum',\n",
    "            'ged_best_osv_tlag12':'sum',\n",
    "            'ged_best_ns_splag1_tlag12':'sum',\n",
    "            'ged_best_sb_splag1_tlag12':'sum',\n",
    "            'ged_best_osv_splag1_tlag12':'sum',\n",
    "            'acled_prrio_count_tlag12':'sum',\n",
    "            'acled_prrio_count_splag1_tlag12':'sum',\n",
    "            'acled_prex_count_tlag12':'sum',\n",
    "            'acled_prex_count_splag1_tlag12':'sum',\n",
    "            'ged_best_ns_msum_12_tlag12':'last',\n",
    "            'ged_best_osv_msum_12_tlag12':'last',\n",
    "            'ged_best_sb_msum_12_tlag12':'last',\n",
    "            'ged_best_ns_splag1_msum_12_tlag12':'last',\n",
    "            'ged_best_osv_splag1_msum_12_tlag12':'last',\n",
    "            'ged_best_sb_splag1_msum_12_tlag12':'last',\n",
    "            'acled_prrio_count_msum_12_tlag12':'last',\n",
    "            'acled_prrio_count_splag1_msum_12_tlag12':'last',\n",
    "            'acled_prex_count_msum_12_tlag12':'last',\n",
    "            'ged_best_ns_ts_decay_6_tlag12':'max',\n",
    "            'ged_best_osv_ts_decay_6_tlag12':'max',\n",
    "            'ged_best_sb_ts_decay_6_tlag12':'max',\n",
    "            'ged_best_ns_splag1_ts_decay_6_tlag12':'max',\n",
    "            'ged_best_osv_splag1_ts_decay_6_tlag12':'max',\n",
    "            'ged_best_sb_splag1_ts_decay_6_tlag12':'max',\n",
    "            'acled_prrio_count_ts_decay_6_tlag12':'max',\n",
    "            'acled_prrio_count_splag1_ts_decay_6_tlag12':'max',\n",
    "            'acled_prex_count_ts_decay_6_tlag12':'max',\n",
    "            'acled_prex_count_splag1_ts_decay_6_tlag12':'max',\n",
    "\n",
    "            # Disasters.\n",
    "            'gdis_n_disasters_tlag12':'sum',\n",
    "            'gdis_disastertype_landslide_msum_12_tlag12':'sum',\n",
    "            'gdis_disastertype_storm_msum_12_tlag12':'sum',\n",
    "            'gdis_disastertype_earthquake_msum_12_tlag12':'sum',\n",
    "            'gdis_disastertype_flood_msum_12_tlag12':'sum',\n",
    "            'flood_dummy_ts_decay_6_tlag12':'max',\n",
    "            'gdis_disastertype_flood_ts_decay_6_tlag12':'max',\n",
    "            'gdis_disastertype_landslide_ts_decay_6_tlag12':'max',\n",
    "            'gdis_disastertype_storm_ts_decay_6_tlag12':'max',\n",
    "            'spei_3_severe_ts_decay_6_tlag12':'max',\n",
    "            'gdis_disastertype_landslide_tlag12':'sum',\n",
    "            'gdis_disastertype_storm_tlag12':'sum',\n",
    "            'gdis_disastertype_earthquake_tlag12':'sum',\n",
    "            'gdis_disastertype_flood_tlag12':'sum',\n",
    "\n",
    "            # Elect violence.\n",
    "            'deco_best_ns_tlag12':'sum',\n",
    "            'deco_best_sb_tlag12':'sum',\n",
    "            'deco_best_osv_tlag12':'sum',\n",
    "            'deco_best_ns_splag1_tlag12':'sum',\n",
    "            'deco_best_sb_splag1_tlag12':'sum',\n",
    "            'deco_best_osv_splag1_tlag12':'sum',\n",
    "            'deco_best_ns_msum_12_tlag12':'last',\n",
    "            'deco_best_osv_msum_12_tlag12':'last',\n",
    "            'deco_best_sb_msum_12_tlag12':'last',\n",
    "            'deco_best_ns_splag1_msum_12_tlag12':'last',\n",
    "            'deco_best_osv_splag1_msum_12_tlag12':'last',\n",
    "            'deco_best_sb_splag1_msum_12_tlag12':'last',\n",
    "            'deco_best_ns_ts_decay_6_tlag12':'max',\n",
    "            'deco_best_osv_ts_decay_6_tlag12':'max',\n",
    "            'deco_best_sb_ts_decay_6_tlag12':'max',\n",
    "            'deco_best_ns_splag1_ts_decay_6_tlag12':'max',\n",
    "            'deco_best_osv_splag1_ts_decay_6_tlag12':'max',\n",
    "            'deco_best_sb_splag1_ts_decay_6_tlag12':'max',\n",
    "            })\n",
    "\n",
    "df_analysis = df_agg # Define aggregate dataframe as dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check years: limit the years (2011 - 2018)\n",
    "print(df_analysis.index.get_level_values(1).min(),df_analysis.index.get_level_values(1).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e12406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLES 6-8 Appendix\n",
    "if do_descr:\n",
    "    corrlist = [ \n",
    "                feat_baseline_district, #Baseline \n",
    "                feat_violence_agg+feat_elect_vio_agg, # Violence\n",
    "                feat_disaster_agg, # Disaster \n",
    "            ]\n",
    "\n",
    "    for feats,fname in zip(corrlist,['baselinedistrict','violence','disasters']):\n",
    "\n",
    "        descr_vars = feats\n",
    "\n",
    "        df_descr = df_analysis[descr_vars].describe().round(3).loc[['count','mean','std','min','max']].T\n",
    "        print(df_descr)\n",
    "        df_descr.to_csv(os.path.join(output_paths['descriptives'],f'SI_tabs6-s8_summary_stats_{fname}_yragg_preimp.csv'))\n",
    "\n",
    "        tex = df_descr.reset_index().to_latex(index=False)\n",
    "        # Get meta infromation\n",
    "        with open(os.path.join(output_paths['descriptives'],f'SI_tabs6-s8_summary_stats_{fname}_yragg_preimp.tex'), \"w\") as f:\n",
    "            f.write(tex)\n",
    "        print(f\"Wrote scores table to {output_paths['descriptives']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ebc6d-6156-4c8f-857e-f9dcde61c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional information for table SI 6\n",
    "non_logged_hh = [\n",
    "    'mob_asset_value_own',\n",
    "    'savings',\n",
    "    'loans',\n",
    "    'land_size',\n",
    "    'pc_expm',\n",
    "    'livestock_value',\n",
    "    'income',\n",
    "    'remittances_in_value',\n",
    "]\n",
    "\n",
    "if do_descr:\n",
    "    corrlist = [ \n",
    "                non_logged_hh, # Non-logged baseline features \n",
    "            ]\n",
    "\n",
    "    for feats,fname in zip(corrlist,['baseline_non_logged']):\n",
    "\n",
    "        descr_vars = feats\n",
    "\n",
    "        df_descr = df_analysis[descr_vars].describe().round(3).loc[['count','mean','std','min','max']].T\n",
    "        print(df_descr)\n",
    "        df_descr.to_csv(os.path.join(output_paths['descriptives'],f'SI_tabs6_1_summary_stats_{fname}_yragg_preimp.csv'))\n",
    "\n",
    "        tex = df_descr.reset_index().to_latex(index=False)\n",
    "        # Get meta infromation\n",
    "        with open(os.path.join(output_paths['descriptives'],f'SI_tabs6_1_summary_stats_{fname}_yragg_preimp.tex'), \"w\") as f:\n",
    "            f.write(tex)\n",
    "        print(f\"Wrote scores table to {output_paths['descriptives']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3cf42-b9db-49e1-b636-33064e8113a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURES S2-4 Appendix \n",
    "if do_descr:\n",
    "\n",
    "    corrlist = [ \n",
    "                feat_baseline_district, #Baseline \n",
    "                feat_violence_agg+feat_elect_vio_agg, # Violence\n",
    "                feat_disaster_agg, # Disaster \n",
    "            ]\n",
    "    for feats, fname in zip(corrlist, ['Baseline model', 'Violence model', 'Disasters, Hazards model']):\n",
    "\n",
    "        # Compute and plot correlation matrix\n",
    "        corr_matrix = df_analysis[feats].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=False, fmt='.2f', cmap='coolwarm', linewidths=0.5,center=0)\n",
    "        plt.title(f'Correlation Matrix - {fname}')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(output_paths['descriptives'], f'SI_correlation_matrix_{fname}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"Saved correlation matrix plot for {fname}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b6025-57fd-4536-af03-fd26f2c6db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    \n",
    "    # Create a copy with relevant columns\n",
    "    df_violence_exp = df_analysis[[\n",
    "        'ged_best_ns_tlag12',\n",
    "        'ged_best_sb_tlag12',\n",
    "        'ged_best_osv_tlag12',\n",
    "        'deco_best_ns_tlag12',\n",
    "        'deco_best_sb_tlag12',\n",
    "        'deco_best_osv_tlag12',\n",
    "    ]].copy()\n",
    "    \n",
    "    # Create binary columns for each type of violence exposure\n",
    "    df_violence_exp['violence_ged_ns'] = df_violence_exp['ged_best_ns_tlag12'] > 0\n",
    "    df_violence_exp['violence_ged_sb'] = df_violence_exp['ged_best_sb_tlag12'] > 0\n",
    "    df_violence_exp['violence_ged_osv'] = df_violence_exp['ged_best_osv_tlag12'] > 0\n",
    "    df_violence_exp['violence_deco_ns'] = df_violence_exp['deco_best_ns_tlag12'] > 0\n",
    "    df_violence_exp['violence_deco_sb'] = df_violence_exp['deco_best_sb_tlag12'] > 0\n",
    "    df_violence_exp['violence_deco_osv'] = df_violence_exp['deco_best_osv_tlag12'] > 0\n",
    "    \n",
    "    # Create a binary column for any violence exposure\n",
    "    df_violence_exp['any_violence'] = (\n",
    "        df_violence_exp[['violence_ged_ns', 'violence_ged_sb', 'violence_ged_osv',\n",
    "                         'violence_deco_ns', 'violence_deco_sb', 'violence_deco_osv']]\n",
    "        .any(axis=1)\n",
    "    )\n",
    "    \n",
    "    # Reset index\n",
    "    df_violence_exp = df_violence_exp.reset_index()\n",
    "    \n",
    "    # Get all unique years in the dataset\n",
    "    all_years = pd.DataFrame({'year': df_violence_exp['year'].unique()})\n",
    "    \n",
    "    # Group by year and calculate affected households for each type of violence\n",
    "    violence_counts = (\n",
    "        df_violence_exp\n",
    "        .groupby('year')\n",
    "        .agg(\n",
    "            violence_affected_households=('any_violence', lambda x: x.sum()),\n",
    "            violence_ged_ns=('violence_ged_ns', lambda x: x.sum()),\n",
    "            violence_ged_sb=('violence_ged_sb', lambda x: x.sum()),\n",
    "            violence_ged_osv=('violence_ged_osv', lambda x: x.sum()),\n",
    "            violence_deco_ns=('violence_deco_ns', lambda x: x.sum()),\n",
    "            violence_deco_sb=('violence_deco_sb', lambda x: x.sum()),\n",
    "            violence_deco_osv=('violence_deco_osv', lambda x: x.sum())\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Merge with all years to include zeros for missing years\n",
    "    affected_hh = pd.merge(all_years, violence_counts, on='year', how='left').fillna(0)\n",
    "    \n",
    "    # Convert affected households to integers\n",
    "    affected_hh.iloc[:, 1:] = affected_hh.iloc[:, 1:].astype(int)\n",
    "    \n",
    "    # Write to CSV and LaTeX\n",
    "    output_path = output_paths['descriptives']\n",
    "    \n",
    "    # Save to CSV\n",
    "    affected_hh.to_csv(os.path.join(output_path, 'SI_tabs9_violence_affected_households.csv'), index=False)\n",
    "    \n",
    "    # Save to LaTeX\n",
    "    tex = affected_hh.to_latex(index=False)\n",
    "    with open(os.path.join(output_path, 'SI_tabs9_violence_affected_households.tex'), \"w\") as f:\n",
    "        f.write(tex)\n",
    "    \n",
    "    print(f\"Wrote affected households table to {output_path}\")\n",
    "    \n",
    "    print(affected_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14390200-bd2e-417b-b211-a5286ae36e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    df_dis_exp = df_analysis[[\n",
    "    'gdis_disastertype_landslide_tlag12',\n",
    "    'gdis_disastertype_storm_tlag12',\n",
    "    'gdis_disastertype_earthquake_tlag12',\n",
    "    'gdis_disastertype_flood_tlag12',\n",
    "    ]].copy()\n",
    "    \n",
    "    # Create a binary column for any disaster exposure\n",
    "    df_dis_exp['any_dis'] = (\n",
    "        df_dis_exp[['gdis_disastertype_landslide_tlag12',\n",
    "        'gdis_disastertype_storm_tlag12',\n",
    "        'gdis_disastertype_earthquake_tlag12',\n",
    "        'gdis_disastertype_flood_tlag12',]]\n",
    "        .sum(axis=1) > 0\n",
    "    )\n",
    "    df_dis_exp = df_dis_exp.reset_index()\n",
    "    \n",
    "    # Get all unique years in the dataset\n",
    "    all_years = pd.DataFrame({'year': df_dis_exp['year'].unique()})\n",
    "    \n",
    "    # Group by year for any disaster exposure\n",
    "    affected_hh = (\n",
    "        df_dis_exp[df_dis_exp['any_dis']]\n",
    "        .groupby('year')['hh_n']\n",
    "        .nunique()\n",
    "        .reset_index(name='disaster_affected_households')\n",
    "    )\n",
    "    \n",
    "    # Merge with all years to include years with zero affected households\n",
    "    affected_hh = pd.merge(all_years, affected_hh, on='year', how='left').fillna(0)\n",
    "    \n",
    "    # Convert affected households to integers (if needed)\n",
    "    affected_hh['disaster_affected_households'] = affected_hh['disaster_affected_households'].astype(int)\n",
    "    \n",
    "    affected_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce444a58-8907-4603-a3ff-0f72807f24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descr:\n",
    "    df_dis_exp = df_analysis[[\n",
    "        'gdis_disastertype_landslide_tlag12',\n",
    "        'gdis_disastertype_storm_tlag12',\n",
    "        'gdis_disastertype_flood_tlag12',\n",
    "    ]].copy()\n",
    "    \n",
    "    # Create binary columns for each type of disaster exposure\n",
    "    df_dis_exp['dis_landslide'] = df_dis_exp['gdis_disastertype_landslide_tlag12'] > 0\n",
    "    df_dis_exp['dis_storm'] = df_dis_exp['gdis_disastertype_storm_tlag12'] > 0\n",
    "    df_dis_exp['dis_flood'] = df_dis_exp['gdis_disastertype_flood_tlag12'] > 0\n",
    "    \n",
    "    # Create a binary column for any disaster exposure\n",
    "    df_dis_exp['any_dis'] = (\n",
    "        df_dis_exp[['dis_landslide', 'dis_storm', 'dis_flood']].any(axis=1)\n",
    "    )\n",
    "    \n",
    "    # Reset index\n",
    "    df_dis_exp = df_dis_exp.reset_index()\n",
    "    \n",
    "    # Get all unique years in the dataset\n",
    "    all_years = pd.DataFrame({'year': df_dis_exp['year'].unique()})\n",
    "    \n",
    "    # Group by year and calculate affected households for each type of disaster\n",
    "    disaster_counts = (\n",
    "        df_dis_exp\n",
    "        .groupby('year')\n",
    "        .agg(\n",
    "            disaster_affected_households=('any_dis', lambda x: x.sum()),\n",
    "            disaster_landslide=('dis_landslide', lambda x: x.sum()),\n",
    "            disaster_storm=('dis_storm', lambda x: x.sum()),\n",
    "            disaster_flood=('dis_flood', lambda x: x.sum())\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Merge with all years to include zeros for missing years\n",
    "    affected_hh = pd.merge(all_years, disaster_counts, on='year', how='left').fillna(0)\n",
    "    \n",
    "    # Convert affected households to integers\n",
    "    affected_hh.iloc[:, 1:] = affected_hh.iloc[:, 1:].astype(int)\n",
    "    \n",
    "    # Write to CSV and LaTeX\n",
    "    output_path = output_paths['descriptives']\n",
    "    \n",
    "    # Save to CSV\n",
    "    affected_hh.to_csv(os.path.join(output_path, 'SI_tabs10_disaster_affected_households.csv'), index=False)\n",
    "    \n",
    "    # Save to LaTeX\n",
    "    tex = affected_hh.to_latex(index=False)\n",
    "    with open(os.path.join(output_path, 'SI_tabs_disaster10_affected_households.tex'), \"w\") as f:\n",
    "        f.write(tex)\n",
    "    \n",
    "    print(f\"Wrote disaster affected households table to {output_path}\")\n",
    "    print(affected_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebc1e",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eedc72",
   "metadata": {},
   "source": [
    "Note that the regression results are included in the SI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83988da",
   "metadata": {},
   "source": [
    "### Multiple imputation in preparation for estimating regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45597416-bf68-43f2-8811-f1d4855af85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_regr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bed113",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    # Numerical features for multiple imputation \n",
    "    # Step 1\n",
    "    imp_numerical_missing = [\n",
    "        'ln_mob_asset_value_own',\n",
    "        'ln_savings',\n",
    "        'ln_loans',\n",
    "        'ln_land_size',\n",
    "        'ln_pc_expm',\n",
    "        'ln_livestock_value',\n",
    "        'ln_income',\n",
    "        'ln_remittances_in_value',\n",
    "    ]\n",
    "\n",
    "    imp_numerical_predictors = [\n",
    "        'nlight_mean', \n",
    "        'PG_GCP_PPP_LI',\n",
    "        'head_age',\n",
    "        'head_sex',\n",
    "        'head_ocup_agg',\n",
    "        'mig_history'\n",
    "    ]\n",
    "\n",
    "    # Step 2\n",
    "    # Categorical features \n",
    "    imp_cat_missing = [\n",
    "        'prim_educ',\n",
    "        'sec_educ',\n",
    "        'ter_educ',\n",
    "        'no_educ',\n",
    "        'ag_subsidy',\n",
    "        'sl_benefited',\n",
    "        'active_member',\n",
    "        'active_leader',\n",
    "        #'satisfied_leave',\n",
    "        #'satisfied_life',\n",
    "        'mobile',\n",
    "        'radio',\n",
    "        'tv',\n",
    "    ] \n",
    "\n",
    "    imp_cat_predictors = [\n",
    "        'religion', \n",
    "        'ethnicity', \n",
    "        'head_ocup_agg', \n",
    "        'head_sex',\n",
    "        'head_ocup_agg',\n",
    "        'mig_history',\n",
    "        #'nlight_mean', \n",
    "        #'PG_GCP_PPP_LI',\n",
    "        #'head_age',\n",
    "    ]\n",
    "\n",
    "    # Step 3: ordinal variables\n",
    "    imp_ord_missing = [\n",
    "        'satisfied_leave',\n",
    "        'satisfied_life',\n",
    "    ]\n",
    "\n",
    "    imp_ord_predictors = imp_numerical_missing + imp_numerical_predictors + imp_cat_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    # Make function for imputation\n",
    "    def apply_imputation(\n",
    "        imp_estimator, \n",
    "        df, \n",
    "        miss_vars,\n",
    "        pred_vars,\n",
    "        max_value,\n",
    "        randomstate,\n",
    "\n",
    "    ):\n",
    "        # Initialise the imputer\n",
    "        imputer_f = IterativeImputer(\n",
    "            estimator=imp_estimator,\n",
    "            min_value=0,\n",
    "            max_value=max_value,\n",
    "            skip_complete=True,\n",
    "            max_iter=1500,\n",
    "            random_state=randomstate,\n",
    "            verbose=2)\n",
    "\n",
    "        # \n",
    "        df_out = pd.DataFrame(\n",
    "            imputer_f.fit_transform(df[miss_vars+pred_vars]),\n",
    "            columns = miss_vars+pred_vars,\n",
    "            index = df[miss_vars+pred_vars].index\n",
    "            )\n",
    "        return df_out[miss_vars]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbe8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    # Apply to numerical variables\n",
    "    df_analysis_imp = df_analysis.copy()\n",
    "\n",
    "    print('Do numerical')    \n",
    "    imp_num_df = apply_imputation(\n",
    "        imp_estimator=linear_model.BayesianRidge(), \n",
    "        df=df_analysis, \n",
    "        miss_vars=imp_numerical_missing,\n",
    "        pred_vars=imp_numerical_predictors,\n",
    "        randomstate=1308,\n",
    "        max_value=None,\n",
    "    )\n",
    "\n",
    "    df_analysis_imp[imp_numerical_missing] = imp_num_df\n",
    "\n",
    "    print('Do categorical')\n",
    "    # Apply to categorical variables \n",
    "    imp_cat_df = apply_imputation(\n",
    "        imp_estimator=linear_model.LogisticRegression(max_iter=200),#RandomForestClassifier(random_state=0,n_jobs=-1), \n",
    "        df=df_analysis_imp, \n",
    "        miss_vars=imp_cat_missing,\n",
    "        pred_vars=imp_cat_predictors,\n",
    "        randomstate=1308,\n",
    "        max_value=1,\n",
    "    )\n",
    "\n",
    "    df_analysis_imp[imp_cat_missing] = imp_cat_df\n",
    "\n",
    "    print('Do ordinal')\n",
    "    # Apply to ordinal variable\n",
    "    imp_ord_df = apply_imputation(\n",
    "        imp_estimator=linear_model.BayesianRidge(), \n",
    "        df=df_analysis_imp, \n",
    "        miss_vars=imp_ord_missing,\n",
    "        pred_vars=imp_ord_predictors,\n",
    "        randomstate=1308,\n",
    "        max_value=10,\n",
    "    )\n",
    "\n",
    "    df_analysis_imp[imp_ord_missing] = imp_ord_df.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    # Hot encode some of the categorical variables \n",
    "    # Recode the variable for sex\n",
    "    df_analysis_imp['head_sex'] = df_analysis_imp['head_sex'].replace({1: 0, 2: 1})\n",
    "\n",
    "    make_dummies = [\n",
    "        'religion',\n",
    "        'ethnicity',\n",
    "    ]\n",
    "\n",
    "    dummies_rel = pd.get_dummies(df_analysis_imp['religion'], prefix='religion')\n",
    "    dummies_ethn = pd.get_dummies(df_analysis_imp['ethnicity'], prefix='ethnicity')\n",
    "\n",
    "    dummies_df = dummies_rel.join(dummies_ethn,how='left')\n",
    "    df_analysis_imp = df_analysis_imp.join(dummies_df,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66385362",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    # Adjust features due to multicolinearity\n",
    "    dict_baseline_district_reg = {\n",
    "        'nlight_mean': 'Nightlight',\n",
    "        'PG_GCP_PPP_LI': 'GCP',\n",
    "        'head_age': 'Head age of hh',\n",
    "        'head_sex': 'Head sex of hh',\n",
    "        'prim_educ': 'Primary education',\n",
    "        'sec_educ': 'Secondary education',\n",
    "        'ter_educ': 'Tertiary education',\n",
    "        'religion_2.0': 'Hindu', \n",
    "        'religion_3.0': 'Christian',\n",
    "        'ethnicity_2.0': 'Bihari',\n",
    "        'ethnicity_3.0': 'Tribal',\n",
    "        'head_ocup_agg': 'Head occupation of hh',\n",
    "        'mig_history': 'Migration history',\n",
    "        'ln_savings': 'Ln (savings)',\n",
    "        'ln_loans': 'Ln (loans)',\n",
    "        'ln_mob_asset_value_own':'Ln (mobile asset value)',\n",
    "        'ag_subsidy':'Agricultural subsidies',\n",
    "        'sl_benefited': 'Safety net program',\n",
    "        'ln_income': 'Ln (income)',\n",
    "        'ln_remittances_in_value': 'Ln (received remittances)',\n",
    "        'active_member': 'Active member',\n",
    "        'active_leader': 'Active leader',\n",
    "        'satisfied_leave':'Satisfied with alt places',\n",
    "        'satisfied_life': 'Satisfied with life',\n",
    "        'mobile':'Mobile',\n",
    "        'radio':'Radio',\n",
    "        'tv':'TV',\n",
    "    }\n",
    "\n",
    "    # Add violence to baseline\n",
    "    dict_bl_vioged_reg = {\n",
    "        'ged_best_ns_tlag12':'Count NS violence (t-12)',\n",
    "        'ged_best_sb_tlag12':'Count SB violence (t-12)',\n",
    "        'ged_best_osv_tlag12':'Count OSV violence (t-12)',\n",
    "    }\n",
    "    dict_bl_vioged_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "    dict_bl_vioged_decay_reg = {\n",
    "        'ged_best_ns_ts_decay_6_tlag12':'NS decay (t-12)',\n",
    "        'ged_best_osv_ts_decay_6_tlag12':'OSV decay (t-12)',\n",
    "        'ged_best_sb_ts_decay_6_tlag12':'SB decay (t-12)',\n",
    "    }\n",
    "    dict_bl_vioged_decay_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "    dict_bl_viodeco_reg = {\n",
    "        'deco_best_ns_tlag12':'NS electoral violence (t-12)',\n",
    "        'deco_best_sb_tlag12':'SB electoral violence (t-12)',\n",
    "        'deco_best_osv_tlag12':'OSV electoral violence (t-12)',\n",
    "    }\n",
    "    dict_bl_viodeco_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "    dict_bl_viodeco_decay_reg = {\n",
    "        'deco_best_ns_ts_decay_6_tlag12':'NS electoral violence decay (t-12)',\n",
    "        'deco_best_osv_ts_decay_6_tlag12':'OSV electoral violence decay (t-12)',\n",
    "        'deco_best_sb_ts_decay_6_tlag12':'SB electoral violence decay (t-12)',\n",
    "    }\n",
    "    dict_bl_viodeco_decay_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "    dict_bl_spei_reg = {\n",
    "        'spei_3_severe_ts_decay_6_tlag12':'SPEI over 3 months decay (t-12)'\n",
    "    }\n",
    "    dict_bl_spei_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "    dict_all_gdis_reg = {\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12':'Flood decay (t-12)',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12':'Landslide decay (t-12)',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12':'Storm decay (t-12)',\n",
    "        'spei_3_severe_ts_decay_6_tlag12':'SPEI over 3 months  decay (t-12)',\n",
    "    }\n",
    "    dict_all_gdis_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "\n",
    "    dict_all_gdis_and_vio_reg = {\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12':'Flood decay (t-12)',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12':'Landslide decay (t-12)',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12':'Storm decay (t-12)',\n",
    "        'spei_3_severe_ts_decay_6_tlag12':'SPEI over 3 months  decay (t-12)',\n",
    "        'deco_best_ns_ts_decay_6_tlag12':'NS electoral violence decay (t-12)',\n",
    "        'deco_best_osv_ts_decay_6_tlag12':'OSV electoral violence decay (t-12)',\n",
    "        'deco_best_sb_ts_decay_6_tlag12':'SB electoral violence decay (t-12)',\n",
    "    }\n",
    "    dict_all_gdis_and_vio_reg.update(dict_baseline_district_reg)\n",
    "\n",
    "\n",
    "    \n",
    "    regmodels = [\n",
    "        dict_bl_vioged_reg,\n",
    "        dict_bl_vioged_decay_reg,\n",
    "        dict_bl_viodeco_reg,\n",
    "        dict_bl_viodeco_decay_reg,\n",
    "        dict_bl_spei_reg,\n",
    "        dict_all_gdis_reg,\n",
    "        dict_all_gdis_and_vio_reg,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_regr:\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "    \n",
    "    for m,name in zip(regmodels,[\n",
    "        'dict_bl_vioged_reg',\n",
    "        'dict_bl_vioged_decay_reg',\n",
    "        'dict_bl_viodeco_reg',\n",
    "        'dict_bl_viodeco_decay_reg',\n",
    "        'dict_bl_spei_reg',\n",
    "        'dict_all_gdis_reg',\n",
    "        'dict_all_gdis_and_vio_reg',\n",
    "    ]):\n",
    "        iv_list = list(m.keys())\n",
    "        X_reg = df_analysis_imp[iv_list].astype(float) \n",
    "        X_reg = sm.add_constant(X_reg, prepend=False) \n",
    "\n",
    "        # Mixed effects model\n",
    "        m1_mfe_re = sm.MixedLM(endog=df_analysis_imp[Y_outcome], exog=X_reg, groups=df_analysis_imp['NAME_2'])\n",
    "        m1_mfe_re.family = sm.families.Binomial()\n",
    "        res_m1_mfe_re = m1_mfe_re.fit()\n",
    "\n",
    "        # Print a summary of the logistic regression results\n",
    "        print(res_m1_mfe_re.summary())\n",
    "\n",
    "        # Get the coefficients and their standard errors from the results\n",
    "        coefficients = res_m1_mfe_re.params\n",
    "        coefficients = coefficients.drop(['const','Group Var'])\n",
    "        std_errors = res_m1_mfe_re.bse\n",
    "        std_errors = std_errors.drop(['const','Group Var'])\n",
    "        coefficients = coefficients.rename(index=m)\n",
    "\n",
    "        # Create a scatter plot of coefficients\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(coefficients, coefficients.index, c='black', marker='o', alpha=0.7)\n",
    "\n",
    "        # Add vertical lines to represent standard errors\n",
    "        for var, coef, se in zip(coefficients.index, coefficients, std_errors):\n",
    "            plt.plot([coef - se, coef + se], [var, var], color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title('Logistic Regression Coefficients (with Standard Errors)')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.ylabel('Variables')\n",
    "        plt.xticks(fontsize=12)  \n",
    "        plt.yticks(fontsize=12)  \n",
    "\n",
    "        # Add a vertical line at 0\n",
    "        plt.axvline(0, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_paths['regression'],f'SI_figs14-s15_regmod_{name}.png'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # --- NEW: Predictions & metrics ---\n",
    "        y_prob = res_m1_mfe_re.predict(X_reg)  # predicted probabilities\n",
    "        y_true = df_analysis_imp[Y_outcome]\n",
    "\n",
    "        # Thresholded predictions\n",
    "        y_pred_05 = (y_prob >= 0.5).astype(int)\n",
    "        y_pred_01 = (y_prob >= 0.1).astype(int)\n",
    "\n",
    "        # Precision, recall, accuracy at 0.5\n",
    "        precision_05 = precision_score(y_true, y_pred_05, zero_division=0)\n",
    "        recall_05 = recall_score(y_true, y_pred_05, zero_division=0)\n",
    "        accuracy_05 = accuracy_score(y_true, y_pred_05)\n",
    "\n",
    "        # F1 scores\n",
    "        f1_macro_05 = f1_score(y_true, y_pred_05, average=\"macro\", zero_division=0)\n",
    "        f1_macro_01 = f1_score(y_true, y_pred_01, average=\"macro\", zero_division=0)\n",
    "        f1_weighted_05 = f1_score(y_true, y_pred_05, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # Threshold-free metrics\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "        print(f\"Logit {name} performance:\")\n",
    "        print(f\"  Macro F1 (0.5):        {f1_macro_05:.3f}\")\n",
    "        print(f\"  Macro F1 (0.1):        {f1_macro_01:.3f}\")\n",
    "        print(f\"  AUROC:                 {auc:.3f}\")\n",
    "        print(f\"  Average Precision (AP): {ap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59875b2f-e8d7-4064-8709-add7b5927f86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning of ML models\n",
    "\n",
    "Implemented to avoid over-fitting by finding optimal numbers of trees.\n",
    "\n",
    "See: https://mljar.com/blog/xgboost-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in allmodels:\n",
    "    print(m['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a555be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_tuning:\n",
    "    # Note each model needs to be tuned seperately, therefore change the model name.\n",
    "    model_early_stopping = district_violence_disaster \n",
    "    \n",
    "    # Define parameters.\n",
    "    random_state= 1308\n",
    "\n",
    "    ############### Imputation\n",
    "    new_imp = True\n",
    "    model_to_impute = baseline_district\n",
    "    imp_max_iter = 50\n",
    "\n",
    "    ############### Sampling # Set to false if not implemented\n",
    "    oversampling = False \n",
    "    o_sampling_strategy='minority'\n",
    "\n",
    "    ###############  Tuning\n",
    "\n",
    "    n_rep = 5\n",
    "    early_stopping_rounds = 25  # stop if no improvement after 200 rounds\n",
    "    rounds_boosting = 250\n",
    "    \n",
    "    ############## Hyper parameters \n",
    "    \n",
    "    parm_1,parm_2 = 'max_depth','learning_rate'\n",
    "\n",
    "    # Range of hyperparameters to tune. \n",
    "    max_depths = list(range(2,7,1)) # change to 8 for other models\n",
    "    learning_rates = list(np.logspace(-3, -1, 5))\n",
    "    \n",
    "    full_dict = {\n",
    "        parm_1:max_depths,\n",
    "        parm_2:learning_rates,\n",
    "    }\n",
    "    \n",
    "    keys, values = zip(*full_dict.items())\n",
    "    full_search_dict = [dict(zip(keys, p)) for p in product(*values)]\n",
    "    \n",
    "    print(full_search_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_tuning:\n",
    "    \n",
    "    master_cv = fun_analysis.cross_validation_split(df_analysis[model_early_stopping['col_outcome']],n_rep=n_rep,set_seed=1308)\n",
    "    \n",
    "    # Cv and get best params\n",
    "    current_params, results_df = fun_analysis.cv_over_param_dict(\n",
    "        df=df_analysis, \n",
    "        param_dict=full_search_dict, \n",
    "        parm_1=parm_1,\n",
    "        parm_2=parm_2,\n",
    "        model_early_stopping=model_early_stopping, \n",
    "        cv_folds=master_cv, \n",
    "        rounds_boosting=rounds_boosting, \n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        evalmetricstr='aucpr',\n",
    "        evalmetric = average_precision_score,\n",
    "        random_state=random_state,\n",
    "        new_imp=new_imp,\n",
    "        model_to_impute = model_to_impute,\n",
    "        imp_max_iter = imp_max_iter,\n",
    "        oversampling = oversampling,\n",
    "        o_sampling_strategy = o_sampling_strategy,\n",
    "        output_paths=output_paths,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_tuning:\n",
    "    # Print results\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129256bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train models and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a1df0-7430-4f05-ae83-a785a28fb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for m in allmodels:\n",
    "        print(m['model_name'],len(m['cols_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45217059-da85-4bb5-896c-41206facd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_robustness: \n",
    "    district_violence_disaster_rob = district_violence_disaster.copy()\n",
    "    \n",
    "    # remove one or several variables\n",
    "    to_remove = ['mig_history']\n",
    "    district_violence_disaster_rob['cols_features'] = [c for c in district_violence_disaster_rob['cols_features'] if c not in to_remove]\n",
    "    district_violence_disaster_rob['model_name'] = \"district_violence_disaster_rob\"\n",
    "    len(district_violence_disaster_rob['cols_features'])\n",
    "\n",
    "    rob_models = [district_violence_disaster_rob]\n",
    "    do_robustness = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd745db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Define whether to run all models for performance evaluation with more repetitions or only the full model with a smaller number of repetition due to computiational limitations.\n",
    "    # Note that the models need to be trained three times. \n",
    "    do_shapvals = True \n",
    "    do_shapinter = False\n",
    "    \n",
    "    if do_shapvals:\n",
    "        n_reps = 100\n",
    "        run_models = allmodels\n",
    "        if do_robustness:\n",
    "            n_reps = 100\n",
    "        run_models = rob_models  \n",
    "    elif do_shapinter: \n",
    "        n_reps = 100\n",
    "        run_models = [district_violence_disaster]   \n",
    "    else:\n",
    "        n_reps = 250\n",
    "        run_models = allmodels\n",
    "    \n",
    "    print('Running, number of repetitions =', n_reps)\n",
    "    print('Running, number of models =', len(run_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e75654",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Add tuned parameters to dictionary\n",
    "    baseline_district['estimator'] = XGBClassifier(\n",
    "        n_estimators=250, \n",
    "        max_depth=2, \n",
    "        learning_rate=0.03162,\n",
    "        n_jobs=14,\n",
    "    )\n",
    "    district_violence['estimator'] = XGBClassifier(\n",
    "        n_estimators=250, \n",
    "        max_depth=2, \n",
    "        learning_rate=0.03162,\n",
    "        n_jobs=14,\n",
    "    )\n",
    "    district_disaster['estimator'] = XGBClassifier(\n",
    "        n_estimators=250, \n",
    "        max_depth=2,\n",
    "        learning_rate=0.03162,\n",
    "        n_jobs=14,\n",
    "    )\n",
    "    district_violence_disaster['estimator'] = XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.03162,\n",
    "        n_jobs=14,\n",
    "    )\n",
    "    district_violence_disaster_rob['estimator'] = XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.03162,\n",
    "        n_jobs=14,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5920385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which models should be trained.\n",
    "if train:\n",
    "    print('Models to run:')\n",
    "    for mod in run_models:\n",
    "        print(mod['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Do not change these parameters\n",
    "esttype ='xgb' \n",
    "model_to_impute = baseline_district\n",
    "imp_iter = 50\n",
    "\n",
    "# Sampling\n",
    "sampling = 'none' # Choices: 'none','under','over'\n",
    "share_neg = 0.1 # Only applies if choice is not 'none'\n",
    "osampling_strategy = 'minority' # Only applies if choice is not 'none'\n",
    "\n",
    "# Imputation\n",
    "impute_data = True\n",
    "do_perm_imp = False # Permuted feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5438ff-c801-4e29-80f1-4a76027a0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Numerical features for multiple imputation \n",
    "    # Step 1\n",
    "    imp_numerical_missing = [\n",
    "        'ln_mob_asset_value_own',\n",
    "        'ln_savings',\n",
    "        'ln_loans',\n",
    "        'ln_land_size',\n",
    "        'ln_pc_expm',\n",
    "        'ln_livestock_value',\n",
    "        'ln_income',\n",
    "        'ln_remittances_in_value',\n",
    "    ]\n",
    "\n",
    "    imp_numerical_predictors = [\n",
    "        'nlight_mean', \n",
    "        'PG_GCP_PPP_LI',\n",
    "        'head_age',\n",
    "        'head_sex',\n",
    "        'head_ocup_agg',\n",
    "        'mig_history'\n",
    "    ]\n",
    "\n",
    "    # Step 2\n",
    "    # Categorical features \n",
    "    imp_cat_missing = [\n",
    "        'prim_educ',\n",
    "        'sec_educ',\n",
    "        'ter_educ',\n",
    "        'no_educ',\n",
    "        'ag_subsidy',\n",
    "        'sl_benefited',\n",
    "        'active_member',\n",
    "        'active_leader',\n",
    "        #'satisfied_leave',\n",
    "        #'satisfied_life',\n",
    "        'mobile',\n",
    "        'radio',\n",
    "        'tv',\n",
    "    ] \n",
    "\n",
    "    imp_cat_predictors = [\n",
    "        'religion', \n",
    "        'ethnicity', \n",
    "        'head_ocup_agg', \n",
    "        'head_sex',\n",
    "        'head_ocup_agg',\n",
    "        'mig_history',\n",
    "        #'nlight_mean', \n",
    "        #'PG_GCP_PPP_LI',\n",
    "        #'head_age',\n",
    "    ]\n",
    "\n",
    "    # Step 3: ordinal variables\n",
    "    imp_ord_missing = [\n",
    "        'satisfied_leave',\n",
    "        'satisfied_life',\n",
    "    ]\n",
    "\n",
    "    imp_ord_predictors = imp_numerical_missing + imp_numerical_predictors + imp_cat_missing   \n",
    "\n",
    "    if train_robustness:\n",
    "        imp_numerical_predictors = [\n",
    "            'nlight_mean', \n",
    "            'PG_GCP_PPP_LI',\n",
    "            'head_age',\n",
    "            'head_sex',\n",
    "            'head_ocup_agg',\n",
    "            #'mig_history'\n",
    "        ]\n",
    "        imp_cat_predictors = [\n",
    "            'religion', \n",
    "            'ethnicity', \n",
    "            'head_ocup_agg', \n",
    "            'head_sex',\n",
    "            'head_ocup_agg',\n",
    "            #'mig_history',\n",
    "            #'nlight_mean', \n",
    "            #'PG_GCP_PPP_LI',\n",
    "            #'head_age',\n",
    "        ]\n",
    "        imp_ord_predictors = imp_numerical_missing + imp_numerical_predictors + imp_cat_missing\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deae786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters \n",
    "if train:\n",
    "    \n",
    "    # Define empty dictonaries and lists to store results\n",
    "    \n",
    "    # Eval scores\n",
    "    scores_dict = {}\n",
    "    \n",
    "    modelobjdict = {}\n",
    "    shapvalsdict = {}\n",
    "    shapinterdict = {}\n",
    "    train_outdict = {}\n",
    "    permimpsdict = {}\n",
    "    auc_pr_cruve_dict={}\n",
    "    preds_dict={}\n",
    "\n",
    "    for mod in run_models:\n",
    "\n",
    "        scores_dict[mod['model_name']] = {\n",
    "            'auroc_mean':[],\n",
    "            'auroc_sd':[],\n",
    "            'ap_mean':[],\n",
    "            'ap_sd':[],\n",
    "            'ap_max':[],\n",
    "            'auroc_max':[]\n",
    "        }\n",
    "        \n",
    "        auc_pr_cruve_dict[mod['model_name']] = {\n",
    "            'fprdict_auc':[],\n",
    "            'tprdict_auc':[],\n",
    "            'dict_precision':[],\n",
    "            'dict_recall':[],\n",
    "        }\n",
    "        \n",
    "        preds_dict[mod['model_name']] = {\n",
    "            'preds': [],\n",
    "            'actuals': [],\n",
    "        }\n",
    "            \n",
    "    # Iterate through models:\n",
    "    for mod in run_models:\n",
    "        \n",
    "        print('###############', mod['model_name'])\n",
    "\n",
    "        # First split data into k folds:\n",
    "        print('Creating folds')\n",
    "        kfolds = fun_analysis.cross_validation_split(df_analysis,n_rep=n_reps,set_seed=1308)\n",
    "\n",
    "        # Iterate through folds\n",
    "        fpr_dict_auc,tpr_dict_auc,prec_dict,recall_dict = {},{},{},{}\n",
    "        modelobj = {}\n",
    "        shapvals = {}\n",
    "        shapinters = {}\n",
    "        train_out_df = {}\n",
    "        permimps = {}\n",
    "        ls_ap_scores = []\n",
    "        ls_auroc_scores = []\n",
    "        ls_preds = []\n",
    "        ls_actuals = []\n",
    "        \n",
    "        \n",
    "        for k in range(0,n_reps):\n",
    "            \n",
    "            print('Fold',k+1)\n",
    "                \n",
    "            if do_perm_imp == True:\n",
    "                predicted_y, true_y, auc, ap, fpr_roc, tpr_roc, thresh_roc, precision, \n",
    "                recall, thresh_pr, fitted_model, perm_imp = fun_analysis.split_train_test_eval(\n",
    "                    test_indx = kfolds[k],\n",
    "                    df=df_analysis_imp,\n",
    "                    cols_X = mod['cols_features'],\n",
    "                    col_Y = mod['col_outcome'],\n",
    "                    modelname = mod['model_name'],\n",
    "                    esttype=esttype,\n",
    "                    fold=k,\n",
    "                    imputation=['numerical','categorical','ordinal'],\n",
    "                    imp_numerical_missing =imp_numerical_missing,\n",
    "                    imp_numerical_predictors =imp_numerical_predictors,\n",
    "                    imp_ordinal_missing =imp_ord_missing,\n",
    "                    imp_ordinal_predictors =imp_ord_predictors,\n",
    "                    imp_categorical_missing =imp_cat_missing,\n",
    "                    imp_categorical_predictors =imp_cat_predictors,\n",
    "                    model_to_impute = model_to_impute,\n",
    "                    imp_iter=imp_iter,\n",
    "                    sampling=sampling,\n",
    "                    osampling_strategy=osampling_strategy,\n",
    "                    share_neg=share_neg,\n",
    "                    estimator= mod['estimator'],\n",
    "                    calibration=False,\n",
    "                    method_calib='isotonic',\n",
    "                    shap_val = False, \n",
    "                    shap_val_inter = False,\n",
    "                    perm_importance = True,\n",
    "                )\n",
    "\n",
    "                ls_ap_scores.append(ap)\n",
    "                ls_auroc_scores.append(auc)\n",
    "                print(ap)\n",
    "                print(auc)\n",
    "                ls_preds.append(predicted_y)\n",
    "                ls_actuals.append(true_y)\n",
    "\n",
    "                fpr_dict_auc[k],tpr_dict_auc[k],prec_dict[k],recall_dict[k] = fpr_roc, tpr_roc, precision, recall\n",
    "\n",
    "                modelobj[k] = fitted_model\n",
    "                permimps[k] = perm_imp\n",
    "                \n",
    "            elif do_shapvals == True:\n",
    "                \n",
    "                predicted_y, true_y, auc, ap, fpr_roc, tpr_roc, thresh_roc, precision, recall, thresh_pr, fitted_model, shap_values, train_x_out = fun_analysis.split_train_test_eval(\n",
    "                    test_indx = kfolds[k],\n",
    "                    df=df_analysis,\n",
    "                    cols_X = mod['cols_features'],\n",
    "                    col_Y = mod['col_outcome'],\n",
    "                    modelname = mod['model_name'],\n",
    "                    esttype=esttype,\n",
    "                    fold=k,\n",
    "                    model_to_impute = model_to_impute,\n",
    "                    imp_iter=imp_iter,\n",
    "                    sampling=sampling,\n",
    "                    osampling_strategy=osampling_strategy,\n",
    "                    imputation=['numerical','categorical','ordinal'],\n",
    "                    imp_numerical_missing =imp_numerical_missing,\n",
    "                    imp_numerical_predictors =imp_numerical_predictors,\n",
    "                    imp_ordinal_missing =imp_ord_missing,\n",
    "                    imp_ordinal_predictors =imp_ord_predictors,\n",
    "                    imp_categorical_missing =imp_cat_missing,\n",
    "                    imp_categorical_predictors =imp_cat_predictors,\n",
    "                    share_neg=share_neg,\n",
    "                    estimator= mod['estimator'],\n",
    "                    calibration=False,\n",
    "                    method_calib='isotonic',\n",
    "                    shap_val = True, \n",
    "                    shap_val_inter = False,\n",
    "                    perm_importance = False,\n",
    "                )\n",
    "\n",
    "                ls_ap_scores.append(ap)\n",
    "                ls_auroc_scores.append(auc)\n",
    "                ls_preds.append(predicted_y)\n",
    "                ls_actuals.append(true_y)\n",
    "\n",
    "                fpr_dict_auc[k],tpr_dict_auc[k],prec_dict[k],recall_dict[k] = fpr_roc, tpr_roc, precision, recall\n",
    "\n",
    "                modelobj[k] = fitted_model\n",
    "                shapvals[k] = shap_values\n",
    "                train_out_df[k] = train_x_out\n",
    "                \n",
    "            elif do_shapinter == True:\n",
    "\n",
    "                predicted_y, true_y, auc, ap, fpr_roc, tpr_roc, thresh_roc, precision, recall, thresh_pr, fitted_model, shap_values, shap_inters, train_x_out = fun_analysis.split_train_test_eval(\n",
    "                    test_indx = kfolds[k],\n",
    "                    df=df_analysis,\n",
    "                    cols_X = mod['cols_features'],\n",
    "                    col_Y = mod['col_outcome'],\n",
    "                    modelname = mod['model_name'],\n",
    "                    esttype=esttype,\n",
    "                    fold=k,\n",
    "                    model_to_impute = model_to_impute,\n",
    "                    imp_iter=imp_iter,\n",
    "                    sampling=sampling,\n",
    "                    osampling_strategy=osampling_strategy,\n",
    "                    imputation=['numerical','categorical','ordinal'],\n",
    "                    imp_numerical_missing =imp_numerical_missing,\n",
    "                    imp_numerical_predictors =imp_numerical_predictors,\n",
    "                    imp_ordinal_missing =imp_ord_missing,\n",
    "                    imp_ordinal_predictors =imp_ord_predictors,\n",
    "                    imp_categorical_missing =imp_cat_missing,\n",
    "                    imp_categorical_predictors =imp_cat_predictors,\n",
    "                    share_neg=share_neg,\n",
    "                    estimator= mod['estimator'],\n",
    "                    calibration=False,\n",
    "                    method_calib='isotonic',\n",
    "                    shap_val = False, \n",
    "                    shap_val_inter = True,\n",
    "                    perm_importance = False,\n",
    "                )\n",
    "\n",
    "                ls_ap_scores.append(ap)\n",
    "                ls_auroc_scores.append(auc)\n",
    "                ls_preds.append(predicted_y)\n",
    "                ls_actuals.append(true_y)\n",
    "\n",
    "                fpr_dict_auc[k],tpr_dict_auc[k],prec_dict[k],recall_dict[k] = fpr_roc, tpr_roc, precision, recall\n",
    "\n",
    "                modelobj[k] = fitted_model\n",
    "                shapvals[k] = shap_values\n",
    "                shapinters[k] = shap_inters\n",
    "                train_out_df[k] = train_x_out\n",
    "                \n",
    "            else:\n",
    "                print('Run models without shaps.')\n",
    "                predicted_y, true_y, auc, ap, fpr_roc, tpr_roc, thresh_roc, precision, recall, thresh_pr, fitted_model = fun_analysis.split_train_test_eval(\n",
    "                        test_indx = kfolds[k],\n",
    "                        df=df_analysis,\n",
    "                        cols_X = mod['cols_features'],\n",
    "                        col_Y = mod['col_outcome'],\n",
    "                        modelname = mod['model_name'],\n",
    "                        esttype=esttype,\n",
    "                        fold=k,\n",
    "                        model_to_impute = model_to_impute,\n",
    "                        imp_iter=imp_iter,\n",
    "                        sampling=sampling,\n",
    "                        osampling_strategy=osampling_strategy,\n",
    "                        imputation=['numerical','categorical','ordinal'],\n",
    "                        imp_numerical_missing =imp_numerical_missing,\n",
    "                        imp_numerical_predictors =imp_numerical_predictors,\n",
    "                        imp_ordinal_missing =imp_ord_missing,\n",
    "                        imp_ordinal_predictors =imp_ord_predictors,\n",
    "                        imp_categorical_missing =imp_cat_missing,\n",
    "                        imp_categorical_predictors =imp_cat_predictors,\n",
    "                        share_neg=share_neg,\n",
    "                        estimator= mod['estimator'],\n",
    "                        calibration=False,\n",
    "                        method_calib='isotonic',\n",
    "                        shap_val = False, \n",
    "                        shap_val_inter = False,\n",
    "                        perm_importance = False,\n",
    "                    )\n",
    "    \n",
    "                ls_ap_scores.append(ap)\n",
    "                ls_auroc_scores.append(auc)\n",
    "                ls_preds.append(predicted_y)\n",
    "                ls_actuals.append(true_y)\n",
    "\n",
    "                fpr_dict_auc[k],tpr_dict_auc[k],prec_dict[k],recall_dict[k] = fpr_roc, tpr_roc, precision, recall\n",
    "\n",
    "                modelobj[k] = fitted_model\n",
    "            \n",
    "        # Summarize model performance\n",
    "        ap_mean_s, ap_std_s, ap_max_s = np.mean(ls_ap_scores), np.std(ls_ap_scores), np.max(ls_ap_scores)\n",
    "        auroc_mean_s, auroc_std_s, auroc_max_s = np.mean(ls_auroc_scores), np.std(ls_auroc_scores), np.max(ls_auroc_scores)\n",
    "\n",
    "        # Store scores\n",
    "        scores_dict[mod['model_name']]['auroc_mean'].append(auroc_mean_s)\n",
    "        scores_dict[mod['model_name']]['auroc_sd'].append(auroc_std_s)\n",
    "        scores_dict[mod['model_name']]['auroc_max'].append(auroc_max_s)\n",
    "        scores_dict[mod['model_name']]['ap_mean'].append(ap_mean_s)\n",
    "        scores_dict[mod['model_name']]['ap_sd'].append(ap_std_s)\n",
    "        scores_dict[mod['model_name']]['ap_max'].append(ap_max_s)\n",
    "\n",
    "        auc_pr_cruve_dict[mod['model_name']]['fprdict_auc']=fpr_dict_auc\n",
    "        auc_pr_cruve_dict[mod['model_name']]['tprdict_auc']=tpr_dict_auc\n",
    "        auc_pr_cruve_dict[mod['model_name']]['dict_precision']=prec_dict\n",
    "        auc_pr_cruve_dict[mod['model_name']]['dict_recall']=recall_dict\n",
    "\n",
    "        preds_dict[mod['model_name']]['preds']=ls_preds\n",
    "        preds_dict[mod['model_name']]['actuals']=ls_actuals        \n",
    "\n",
    "        modelobjdict[mod['model_name']]= modelobj\n",
    "        \n",
    "        if do_shapvals == True:\n",
    "            shapvalsdict[mod['model_name']]=shapvals\n",
    "            train_outdict[mod['model_name']]=train_out_df\n",
    "            \n",
    "        if do_shapinter == True:\n",
    "            shapvalsdict[mod['model_name']]=shapvals\n",
    "            shapinterdict[mod['model_name']]=shapinters\n",
    "            train_outdict[mod['model_name']]=train_out_df\n",
    "        \n",
    "        if do_perm_imp == True:\n",
    "            permimpsdict[mod['model_name']]=permimps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3de0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Store and load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name to store the results\n",
    "run_preds = 'rr_nee_allmodels'\n",
    "run_shap = 'rr_nee_shap_fullmodel'\n",
    "run_shap_inter = 'rr_nee_shap_inter_fullmodel'\n",
    "run_robustness = 'rr_nee_robtest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether to store predictions or to load perviously made predictions\n",
    "storing_trained = False\n",
    "load_scores = False\n",
    "\n",
    "# Define whether to store shap and shap interaction values\n",
    "storing_shapvals = False\n",
    "storing_shapinter = False\n",
    "\n",
    "# Define whether to do shap analysis\n",
    "load_shap = False\n",
    "\n",
    "# Define whether to store the robustness run\n",
    "storing_robustness = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if storing_trained:\n",
    "    # Predictions and evaluation scores.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_preds}.p\"), 'wb') as fp:\n",
    "        pickle.dump(preds_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Saved predictions')\n",
    "\n",
    "        preds_dict_eval = preds_dict.copy()\n",
    "        \n",
    "    with open(os.path.join(output_paths['evalscores'], f\"evals_dict_{run_preds}.p\"), 'wb') as fp:\n",
    "        pickle.dump(scores_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Saved evalscores')\n",
    "\n",
    "        scores_dict_eval = scores_dict.copy()\n",
    "        \n",
    "elif load_scores:\n",
    "    preds_dict_eval = pd.read_pickle(os.path.join(output_paths['predictions'], f\"preds_dict_{run_preds}.p\"))\n",
    "    scores_dict_eval = pd.read_pickle(os.path.join(output_paths['evalscores'], f\"evals_dict_{run_preds}.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if storing_shapvals:\n",
    "    with open(os.path.join(output_paths['shapvals'], f\"shap_dict_{run_shap}.p\"), 'wb') as fp:\n",
    "        pickle.dump(shapvalsdict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(output_paths['shapvals'], f\"df_train_dict_{run_shap}.p\"), 'wb') as fm:\n",
    "        pickle.dump(train_outdict, fm, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(output_paths['shapvals'], f\"preds_dict_{run_shap}.p\"), 'wb') as fm:\n",
    "        pickle.dump(preds_dict, fm, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Rename\n",
    "    preds_dict_shap = preds_dict.copy()\n",
    "\n",
    "if storing_shapinter:\n",
    "    with open(os.path.join(output_paths['shapvals'], f\"shap_dict_{run_shap_inter}.p\"), 'wb') as fp:\n",
    "        pickle.dump(shapinterdict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ed6e9-539c-46b7-ac02-9f2de829d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_shap:\n",
    "    shapvalsdict = pd.read_pickle(os.path.join(output_paths['shapvals'], f\"shap_dict_{run_shap}.p\"))\n",
    "    shapinterdict = pd.read_pickle(os.path.join(output_paths['shapvals'], f\"shap_dict_{run_shap_inter}.p\"))\n",
    "    preds_dict_shap = pd.read_pickle(os.path.join(output_paths['shapvals'], f\"preds_dict_{run_shap}.p\"))\n",
    "    train_outdict = pd.read_pickle(os.path.join(output_paths['shapvals'], f\"df_train_dict_{run_shap}.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70c796-7615-4c43-9d57-4f6405e601c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if storing_robustness:\n",
    "    # Predictions and evaluation scores.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_robustness}.p\"), 'wb') as fp:\n",
    "        pickle.dump(preds_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Saved predictions')\n",
    "\n",
    "    with open(os.path.join(output_paths['evalscores'], f\"evals_dict_{run_robustness}.p\"), 'wb') as fp:\n",
    "        pickle.dump(scores_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Saved evalscores')\n",
    "\n",
    "    with open(os.path.join(output_paths['shapvals'], f\"shap_dict_{run_robustness}.p\"), 'wb') as fp:\n",
    "        pickle.dump(shapvalsdict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(os.path.join(output_paths['shapvals'], f\"df_train_dict_{run_robustness}.p\"), 'wb') as fm:\n",
    "        pickle.dump(train_outdict, fm, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        preds_dict_shap = preds_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72b4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad73932",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance - evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = False\n",
    "analysis_models = allmodels\n",
    "n_reps_eval = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f66459",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "\n",
    "    # Save to table\n",
    "    save_evalscores = True\n",
    "\n",
    "    df_evalscore = pd.DataFrame.from_dict(scores_dict_eval,orient='index')\n",
    "    df_evalscore['auroc_mean'] = df_evalscore.auroc_mean.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    df_evalscore['auroc_max'] = df_evalscore.auroc_max.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    df_evalscore['auroc_sd'] = df_evalscore.auroc_sd.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    df_evalscore['ap_mean'] = df_evalscore.ap_mean.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    df_evalscore['ap_max'] = df_evalscore.ap_max.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    df_evalscore['ap_sd'] = df_evalscore.ap_sd.astype('str').str.replace('[', '').str.replace(']', '')\n",
    "    \n",
    "    df_evalscore = df_evalscore.replace(r'np\\.float64\\(|\\)', '', regex=True)\n",
    "    df_evalscore = df_evalscore.astype(float)\n",
    "\n",
    "    if save_evalscores: \n",
    "        # Write to tex. file. \n",
    "        tex = df_evalscore.reset_index().to_latex(index=False)\n",
    "        # Get meta infromation\n",
    "        with open(os.path.join(output_paths['evalscores'], f\"main_table1_{run_preds}_{esttype}.tex\"), \"w\") as f:\n",
    "            f.write(tex)\n",
    "        print(df_evalscore)\n",
    "\n",
    "    else:\n",
    "        print(df_evalscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    ##### Compute CI intervals\n",
    "    for model in analysis_models:\n",
    "        mname = model['model_name']\n",
    "        max_auroc = []\n",
    "        max_ap = []\n",
    "        print(mname)\n",
    "        for k in range(n_reps_eval):\n",
    "            max_auroc.append(roc_auc_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k]))\n",
    "            max_ap.append(average_precision_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k]))\n",
    "\n",
    "        # 95% CI\n",
    "        print(np.mean(max_ap) - (1.96 * np.std(max_ap) / np.sqrt(200))) \n",
    "        print(np.mean(max_ap) + 1.96 * np.std(max_ap) / np.sqrt(200))\n",
    "        print('f', np.mean(max_ap) - (np.std(max_ap)*2)) \n",
    "        print('f', np.mean(max_ap)+ (np.std(max_ap)*2)) \n",
    "\n",
    "        print(np.mean(max_auroc) - 1.96 * np.std(max_auroc) / np.sqrt(200))\n",
    "        print(np.mean(max_auroc) + 1.96 * np.std(max_auroc) / np.sqrt(200))\n",
    "        print('f', np.mean(max_auroc) - (np.std(max_auroc)*2)) \n",
    "        print('f', np.mean(max_auroc)+ (np.std(max_auroc)*2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a84a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    diff_models = [\n",
    "        district_violence,\n",
    "        district_disaster,\n",
    "        district_violence_disaster,    \n",
    "    ]\n",
    "    mean_ap,  mean_auc = [], []\n",
    "    ap_sd, auc_sd = [], []\n",
    "    for model in diff_models:\n",
    "\n",
    "        mname = model['model_name']\n",
    "        max_auroc = []\n",
    "        max_ap = []\n",
    "        print(mname)\n",
    "        for k in range(n_reps_eval):\n",
    "            max_auroc.append(roc_auc_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k]) - roc_auc_score(\n",
    "                y_true = preds_dict_eval[f'baseline_district']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'baseline_district']['preds'][k]))\n",
    "            max_ap.append(average_precision_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k])-average_precision_score(\n",
    "                y_true = preds_dict_eval[f'baseline_district']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'baseline_district']['preds'][k]))\n",
    "\n",
    "        # 95% CI\n",
    "        print('f', np.mean(max_ap) - (np.std(max_ap)*2)) \n",
    "        print('f', np.mean(max_ap)+ (np.std(max_ap)*2)) \n",
    "\n",
    "        mean_ap.append(np.mean(max_ap))\n",
    "        ap_sd.append(np.std(max_ap)*2)\n",
    "\n",
    "        mean_auc.append(np.mean(max_auroc))\n",
    "        auc_sd.append(np.std(max_auroc)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21e651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    save_fig = True\n",
    "    fig, axs = plt.subplots(1,2,figsize=(6, 4))\n",
    "\n",
    "    # AP Difference to baseline\n",
    "    a = ['Violence','Disasters\\n Hazards','Full\\n Distress']\n",
    "    b = mean_ap\n",
    "    y_err = ap_sd\n",
    "\n",
    "    axs[0].errorbar(a, b ,yerr=y_err,\n",
    "                marker='o', markersize=8,linestyle='none',color='black',elinewidth=0.7)\n",
    "    axs[0].grid(True,linestyle='--',lw=0.7)\n",
    "    axs[0].spines['right'].set_visible(False)\n",
    "    axs[0].spines['left'].set_visible(False)\n",
    "    axs[0].spines['top'].set_visible(False)\n",
    "    axs[0].spines['bottom'].set_visible(False)\n",
    "\n",
    "    axs[0].axhline(y = 0, color = 'darkred', linestyle = '--',lw=0.7)\n",
    "\n",
    "    axs[0].set_ylabel('Difference to $\\it{Baseline}$')\n",
    "    axs[0].set_ylim(-0.01,0.06)\n",
    "    axs[0].tick_params(axis='x', labelrotation = 45)\n",
    "\n",
    "\n",
    "    axs[0].set_title('AP')\n",
    "\n",
    "    # AP Difference to baseline\n",
    "    a = ['Violence','Disasters\\n Hazards','Full\\n Distress']\n",
    "    b = mean_auc\n",
    "    y_err = auc_sd\n",
    "\n",
    "    axs[1].errorbar(a, b ,yerr=y_err,\n",
    "                marker='o', markersize=8,linestyle='none',color='black',elinewidth=0.7)\n",
    "    axs[1].grid(True,linestyle='--',lw=0.7)\n",
    "    axs[1].spines['right'].set_visible(False)\n",
    "    axs[1].spines['left'].set_visible(False)\n",
    "    axs[1].spines['top'].set_visible(False)\n",
    "    axs[1].spines['bottom'].set_visible(False)\n",
    "\n",
    "    #axs[1].set_ylabel('Difference in AP')\n",
    "    axs[1].set_ylim(-0.01,0.06)\n",
    "    axs[1].yaxis.set_ticklabels([])\n",
    "    axs[1].tick_params(axis='x', labelrotation = 45)\n",
    "\n",
    "    axs[1].axhline(y = 0, color = 'darkred', linestyle = '--',lw=0.7)\n",
    "\n",
    "    axs[1].set_title('AUROC')\n",
    "    \n",
    "    if save_fig:\n",
    "        plt.savefig(os.path.join(output_paths['evalscores'],f'main_fig2a_ap_auc_diff_baseline_{run_preds}_errorplot.png'),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    diff_models = [\n",
    "        district_violence,\n",
    "        district_disaster,  \n",
    "    ]\n",
    "\n",
    "    mean_ap,  mean_auc = [], []\n",
    "    ap_sd, auc_sd = [], []\n",
    "\n",
    "    for model in diff_models:\n",
    "        mname = model['model_name']\n",
    "        max_auroc = []\n",
    "        max_ap = []\n",
    "        print(mname)\n",
    "        for k in range(n_reps_eval):\n",
    "            max_auroc.append(roc_auc_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k]) -\n",
    "                roc_auc_score(y_true = preds_dict_eval[f'district_violence_disaster']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'district_violence_disaster']['preds'][k]))\n",
    "            max_ap.append(average_precision_score(\n",
    "                y_true = preds_dict_eval[f'{mname}']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'{mname}']['preds'][k])-\n",
    "                average_precision_score(y_true = preds_dict_eval[f'district_violence_disaster']['actuals'][k], \n",
    "                y_score = preds_dict_eval[f'district_violence_disaster']['preds'][k]))\n",
    "\n",
    "        # 95% CI\n",
    "        print('f', np.mean(max_ap) - (np.std(max_ap)*2)) \n",
    "        print('f', np.mean(max_ap)+ (np.std(max_ap)*2)) \n",
    "\n",
    "        mean_ap.append(np.mean(max_ap))\n",
    "        ap_sd.append(np.std(max_ap)*2)\n",
    "\n",
    "        mean_auc.append(np.mean(max_auroc))\n",
    "        auc_sd.append(np.std(max_auroc)*2)\n",
    "\n",
    "        print('f', np.mean(max_auroc) - (np.std(max_auroc)*2)) \n",
    "        print('f', np.mean(max_auroc)+ (np.std(max_auroc)*2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    save_fig = True\n",
    "    fig, axs = plt.subplots(1,2,figsize=(6, 4))\n",
    "\n",
    "    # AP Difference to baseline\n",
    "    a = ['Violence','Disasters\\n Hazards']\n",
    "    b = mean_ap\n",
    "    y_err = ap_sd\n",
    "\n",
    "    axs[0].errorbar(a, b ,yerr=y_err,\n",
    "                marker='o', markersize=8,linestyle='none',color='black',elinewidth=0.7)\n",
    "    axs[0].grid(True,linestyle='--',lw=0.7)\n",
    "    axs[0].spines['right'].set_visible(False)\n",
    "    axs[0].spines['left'].set_visible(False)\n",
    "    axs[0].spines['top'].set_visible(False)\n",
    "    axs[0].spines['bottom'].set_visible(False)\n",
    "\n",
    "    axs[0].set_ylabel('Difference to $\\it{Full}$  $\\it{Distress}$')\n",
    "    axs[0].set_ylim(-0.06,0.01)\n",
    "    axs[0].tick_params(axis='x', labelrotation = 45)\n",
    "    axs[0].axhline(y = 0, color = 'darkred', linestyle = '--',lw=0.7)\n",
    "\n",
    "\n",
    "    axs[0].set_title('AP')\n",
    "\n",
    "    # AP Difference to baseline\n",
    "    a = ['Violence','Disasters\\n Hazards']\n",
    "    b = mean_auc\n",
    "    y_err = auc_sd\n",
    "\n",
    "    axs[1].errorbar(a, b ,yerr=y_err,\n",
    "                marker='o', markersize=8,linestyle='none',color='black',elinewidth=0.7)\n",
    "    axs[1].grid(True,linestyle='--',lw=0.7)\n",
    "    axs[1].spines['right'].set_visible(False)\n",
    "    axs[1].spines['left'].set_visible(False)\n",
    "    axs[1].spines['top'].set_visible(False)\n",
    "    axs[1].spines['bottom'].set_visible(False)\n",
    "\n",
    "    #axs[1].set_ylabel('Difference in AP')\n",
    "    axs[1].set_ylim(-0.06,0.01)\n",
    "    axs[1].yaxis.set_ticklabels([])\n",
    "    axs[1].tick_params(axis='x', labelrotation = 45)\n",
    "\n",
    "    axs[1].axhline(y = 0, color = 'darkred', linestyle = '--',lw=0.7)\n",
    "\n",
    "    axs[1].set_title('AUROC')\n",
    "    if save_fig: \n",
    "        plt.savefig(os.path.join(output_paths['evalscores'],f'main_fig2b_ap_auc_diff_fulldistress_{run_preds}_errorplot.png'),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967373f-ad78-4f78-87a2-8af001bf512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    f1_models = [\n",
    "        baseline_district,\n",
    "        district_violence,\n",
    "        district_disaster,\n",
    "        district_violence_disaster,    \n",
    "    ]\n",
    "    mean_f1 = []\n",
    "    for model in f1_models:\n",
    "        mname = model['model_name']\n",
    "        print(mname)\n",
    "        \n",
    "        f1_scores = []\n",
    "        for k in range(n_reps_eval):\n",
    "            y_true = preds_dict_eval[mname]['actuals'][k]\n",
    "            y_prob = preds_dict_eval[mname]['preds'][k]\n",
    "    \n",
    "            # convert probabilities to binary predictions\n",
    "            y_pred = (np.array(y_prob) >= 0.1).astype(int)\n",
    "    \n",
    "            # compute F1\n",
    "            f1_scores.append(f1_score(y_true, y_pred,average='macro'))\n",
    "    \n",
    "        mean_f1.append(np.mean(f1_scores))\n",
    "        print(f\"{mname}: mean F1 = {np.mean(f1_scores):.3f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229032b1-0881-4c6c-89ad-0f08092fca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    fold_prevalences = [np.mean(preds_dict_eval['baseline_district']['actuals'][k]) \n",
    "                        for k in range(len(preds_dict_eval['baseline_district']['actuals']))]\n",
    "    \n",
    "    print(\"Mean prevalence across folds:\", np.mean(fold_prevalences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe7014-cb24-40ad-b5ce-404370698dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    f1_models = [\n",
    "        baseline_district,\n",
    "        district_violence,\n",
    "        district_disaster,\n",
    "        district_violence_disaster,    \n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for model in f1_models:\n",
    "        mname = model['model_name']\n",
    "\n",
    "        macro_f1_01, macro_f1_05, weighted_f1_05 = [], [], []\n",
    "\n",
    "        for k in range(n_reps_eval):\n",
    "            y_true = preds_dict_eval[mname]['actuals'][k]\n",
    "            y_prob = preds_dict_eval[mname]['preds'][k]\n",
    "\n",
    "            # threshold 0.1\n",
    "            y_pred_01 = (np.array(y_prob) >= 0.1).astype(int)\n",
    "            macro_f1_01.append(f1_score(y_true, y_pred_01, average=\"macro\"))\n",
    "\n",
    "            # threshold 0.5\n",
    "            y_pred_05 = (np.array(y_prob) >= 0.5).astype(int)\n",
    "            macro_f1_05.append(f1_score(y_true, y_pred_05, average=\"macro\"))\n",
    "            weighted_f1_05.append(f1_score(y_true, y_pred_05, average=\"weighted\"))\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": mname,\n",
    "            \"Macro F1 (0.1)\": np.mean(macro_f1_01),\n",
    "            \"Macro F1 (0.5)\": np.mean(macro_f1_05),\n",
    "            \"Weighted F1 (0.5)\": np.mean(weighted_f1_05)\n",
    "        })\n",
    "\n",
    "    df_evalscore = pd.DataFrame(results).round(3)\n",
    "\n",
    "    if save_evalscores: \n",
    "        tex = df_evalscore.reset_index(drop=True).to_latex(index=False, float_format=\"%.3f\")\n",
    "        with open(os.path.join(output_paths['evalscores'], f\"SI_tabs12_f1scores_{run_preds}_{esttype}.tex\"), \"w\") as f:\n",
    "            f.write(tex)\n",
    "        print(df_evalscore)\n",
    "    else:\n",
    "        print(df_evalscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a470ce2-a5e3-4e12-974e-f09755ffabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    admin1_shp = gpd.read_file(os.path.join(input_data_path, 'gadm36_BGD_1.shp'))\n",
    "    \n",
    "    ap_data = []\n",
    "    for model in analysis_models:\n",
    "        mname = model['model_name']\n",
    "        print(mname)\n",
    "\n",
    "        # collect per-fold AP scores by division\n",
    "        fold_results = []\n",
    "\n",
    "        for k in range(n_reps_eval):\n",
    "            # combine preds + actuals + division labels\n",
    "            df_concat = pd.concat(\n",
    "                [\n",
    "                    pd.DataFrame(preds_dict_eval[f'{mname}']['preds'][k]).rename(columns={0: 'preds'}),\n",
    "                    pd.DataFrame(preds_dict_eval[f'{mname}']['actuals'][k]).reset_index()\n",
    "                ],\n",
    "                axis=1,\n",
    "                ignore_index=False\n",
    "            )\n",
    "            df_concat = df_concat.set_index(['hh_n','year']).join(df_agg[['Division_Name']], how='left')\n",
    "\n",
    "            # compute AP per division (one value per division in this fold)\n",
    "            df_div_ap = df_concat.groupby('Division_Name').apply(\n",
    "                lambda g: average_precision_score(g['both_out'], g['preds']) \n",
    "                          if g['both_out'].nunique() > 1 else np.nan,\n",
    "                 include_groups=False\n",
    "            )\n",
    "            fold_results.append(df_div_ap)\n",
    "\n",
    "        # stack results: rows = folds, columns = divisions\n",
    "        df_divcon = pd.DataFrame(fold_results)\n",
    "\n",
    "        # average AP per division across folds\n",
    "        df_div_mean = df_divcon.mean(axis=0)\n",
    "\n",
    "        # join with shapefile geometries\n",
    "        df_div_plot = (\n",
    "            pd.DataFrame({score_name: df_div_mean})\n",
    "            .join(admin1_shp.set_index('NAME_1').geometry, how='left')\n",
    "        )\n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        df_div_plot.set_geometry('geometry').plot(\n",
    "            column=score_name, legend=True, ax=ax, cmap='BuPu',\n",
    "            #vmin=v_min, vmax=v_max\n",
    "        )\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.savefig(os.path.join(\n",
    "            output_paths['evalscores'],\n",
    "            f'SI_figs_division_{mname}_means_ap.png'\n",
    "        ))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        ap_data.append(df_div_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce1414-183e-4b88-9355-ddc04ae404e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_scores:\n",
    "    baseline = ap_data[0]  # baseline GeoDataFrame\n",
    "    \n",
    "    for n in range(len(ap_data)):\n",
    "        if n == 0:\n",
    "            continue  # skip baseline vs. itself\n",
    "        \n",
    "        model_gdf = ap_data[n]  # GeoDataFrame for this model\n",
    "\n",
    "        # compute difference in AP per division\n",
    "        diff = model_gdf['ap'] - baseline['ap']\n",
    "\n",
    "        # merge with geometry\n",
    "        diff_gdf = pd.DataFrame(diff).join(baseline[['geometry']])\n",
    "        diff_gdf = gpd.GeoDataFrame(diff_gdf, geometry='geometry')\n",
    "\n",
    "        # check range\n",
    "        print(f\"Model {n} vs baseline: min={diff.min():.3f}, max={diff.max():.3f}\")\n",
    "\n",
    "        # plot differences\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        diff_gdf.plot(\n",
    "            column='ap', legend=True, ax=ax, cmap='inferno',\n",
    "            vmin=0, vmax=0.05 \n",
    "        )\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.savefig(os.path.join(\n",
    "            output_paths['evalscores'],\n",
    "            f'SI_figs_ap_division_diff_bl_m{n}_means.png'\n",
    "        ))\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3eb20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ML interpretability tools - SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_eval = True\n",
    "n_reps_shap = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load human variable names\n",
    "if shap_eval:\n",
    "    with open(os.path.join(os.getcwd(),'variable_dict.json'), 'r') as file:\n",
    "        name_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005c35e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Influential predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d41814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sum of shap values for all repetitions\n",
    "if shap_eval:\n",
    "    analysis_models_shap = [district_violence_disaster]\n",
    "    figlabels = ['a)']\n",
    "    fig_nr = 'fig3a'\n",
    "\n",
    "    feat_group1 = ['nlight_mean', 'PG_GCP_PPP_LI', 'head_age', 'head_sex', 'prim_educ', 'sec_educ', 'ter_educ', 'no_educ', 'religion', 'ethnicity', 'head_ocup_agg', 'mig_history', 'ln_mob_asset_value_own', 'ln_savings', 'ln_loans', 'ln_land_size', 'ln_pc_expm', 'ag_subsidy', 'ln_livestock_value', 'sl_benefited', 'ln_income', 'ln_remittances_in_value', 'active_member', 'active_leader', 'satisfied_leave', 'satisfied_life', 'mobile', 'radio', 'tv']\n",
    "\n",
    "    feat_group2 =  ['ged_best_ns_tlag12', 'ged_best_sb_tlag12', 'ged_best_osv_tlag12', 'ged_best_ns_splag1_tlag12', 'ged_best_sb_splag1_tlag12', 'ged_best_osv_splag1_tlag12', 'acled_prrio_count_tlag12', 'acled_prrio_count_splag1_tlag12', 'acled_prex_count_tlag12', 'acled_prex_count_splag1_tlag12', 'ged_best_ns_ts_decay_6_tlag12', 'ged_best_osv_ts_decay_6_tlag12', 'ged_best_sb_ts_decay_6_tlag12', 'acled_prrio_count_ts_decay_6_tlag12', 'acled_prrio_count_splag1_ts_decay_6_tlag12', 'acled_prex_count_ts_decay_6_tlag12', 'acled_prex_count_splag1_ts_decay_6_tlag12', 'deco_best_ns_tlag12', 'deco_best_sb_tlag12', 'deco_best_osv_tlag12', 'deco_best_ns_splag1_tlag12', 'deco_best_sb_splag1_tlag12', 'deco_best_osv_splag1_tlag12', 'deco_best_ns_ts_decay_6_tlag12', 'deco_best_osv_ts_decay_6_tlag12', 'deco_best_sb_ts_decay_6_tlag12']\n",
    "\n",
    "    feat_group3 =  ['gdis_n_disasters_tlag12', 'flood_dummy_ts_decay_6_tlag12', 'gdis_disastertype_flood_ts_decay_6_tlag12', 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'gdis_disastertype_storm_ts_decay_6_tlag12', 'spei_3_severe_ts_decay_6_tlag12']\n",
    "\n",
    "    for mod,figlabel in zip(analysis_models_shap,figlabels):\n",
    "        print(mod['model_name'])\n",
    "        mname= mod['model_name']\n",
    "\n",
    "        pathsave = os.path.join(\n",
    "            output_paths['shapvals'],\n",
    "            f'main_{fig_nr}_shap_mean_barplots_aggfeats_{esttype}_test_{run_shap}.png')\n",
    "\n",
    "        fun_plots.plot_barshap_sumfeatures(\n",
    "            dict_shaps=shapvalsdict, \n",
    "            dict_train_cols=train_outdict, \n",
    "            modelname=mname,\n",
    "            do_mean=True,\n",
    "            feature_sets=[feat_group1,feat_group2,feat_group3],\n",
    "            feature_set_names=['Baseline','Violence','Hazards/Disasters'],\n",
    "            n_range=n_reps_shap, \n",
    "            #figure_label=figlabel,\n",
    "            save_fig_path=pathsave,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual features for mean folds\n",
    "if shap_eval:\n",
    "    analysis_models_shap = [district_violence_disaster]\n",
    "    figlabels = ['b)']\n",
    "    fig_nr = 'fig3b'\n",
    "\n",
    "    for mod,figlabel in zip(analysis_models_shap,figlabels):\n",
    "        print(mod['model_name'])\n",
    "        mname= mod['model_name']\n",
    "\n",
    "        pathsave = os.path.join(\n",
    "            output_paths['shapvals'],\n",
    "            f'main_{fig_nr}_shap_mean_barplots_{mname}_{esttype}_test_{run_shap}.png')\n",
    "\n",
    "        fun_plots.plot_barshap(\n",
    "            dict_shaps=shapvalsdict, \n",
    "            dict_train_cols=train_outdict, \n",
    "            name_dict=name_dict,\n",
    "            modelname=mname,\n",
    "            do_mean=True,\n",
    "            do_best=False,\n",
    "            #bestfold=bf,\n",
    "            n_range=n_reps_shap,\n",
    "            #threshold=0.01,\n",
    "            n_max_values=10,\n",
    "            #figure_label=figlabel,\n",
    "            save_fig_path=pathsave,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb919255-078e-496a-9bfd-63773caf2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual features for mean folds\n",
    "if shap_eval:\n",
    "    analysis_models_shap = allmodels\n",
    "    fig_nr = 'SI_figS7'\n",
    "\n",
    "    for mod in analysis_models_shap:\n",
    "        print(mod['model_name'])\n",
    "        mname= mod['model_name']\n",
    "\n",
    "        pathsave = os.path.join(\n",
    "            output_paths['shapvals'],\n",
    "            f'{fig_nr}_shap_mean_barplots_{mname}_{esttype}_test_{run_shap}.png')\n",
    "\n",
    "        fun_plots.plot_barshap(\n",
    "            dict_shaps=shapvalsdict, \n",
    "            dict_train_cols=train_outdict, \n",
    "            name_dict=name_dict,\n",
    "            modelname=mname,\n",
    "            do_mean=True,\n",
    "            do_best=False,\n",
    "            #bestfold=bf,\n",
    "            n_range=100, # Number of repetitions\n",
    "            #threshold=0.01,\n",
    "            n_max_values=10,\n",
    "            save_fig_path=pathsave,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853760ac-6ee0-45ba-b730-e5f1f55d693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    analysis_models_shap = allmodels\n",
    "    selected_folds_dict = {}  \n",
    "\n",
    "    for mod in analysis_models_shap:\n",
    "        print(mod['model_name'])\n",
    "        mname = mod['model_name']\n",
    "\n",
    "        # collect AP for each fold\n",
    "        ap_scores = []\n",
    "        for k in range(len(preds_dict_shap[mname]['preds'])):  \n",
    "            ap_val = average_precision_score(\n",
    "                y_true = preds_dict_shap[mname]['actuals'][k],\n",
    "                y_score = preds_dict_shap[mname]['preds'][k]\n",
    "            )\n",
    "            ap_scores.append((k, ap_val))\n",
    "\n",
    "        # compute median AP\n",
    "        ap_vals = [val for _, val in ap_scores]\n",
    "        median_ap = np.median(ap_vals)\n",
    "\n",
    "        # select folds above (or equal to) median\n",
    "        selected_folds = [fold for fold, val in ap_scores if val >= median_ap]\n",
    "        selected_folds_dict[mname] = selected_folds  # save to dict\n",
    "\n",
    "        print(f\"Selected {len(selected_folds)} folds for model {mname} ( median AP = {median_ap:.3f})\")\n",
    "    \n",
    "    print(selected_folds_dict['district_violence_disaster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a323a8-0ca3-4771-97f8-ba78db1dbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    if do_robustness:\n",
    "        analysis_models_shap = rob_models\n",
    "        selected_folds_dict = {}  \n",
    "    \n",
    "        for mod in analysis_models_shap:\n",
    "            print(mod['model_name'])\n",
    "            mname = mod['model_name']\n",
    "    \n",
    "            # collect AP for each fold\n",
    "            ap_scores = []\n",
    "            for k in range(len(preds_dict_shap[mname]['preds'])):  \n",
    "                ap_val = average_precision_score(\n",
    "                    y_true = preds_dict_shap[mname]['actuals'][k],\n",
    "                    y_score = preds_dict_shap[mname]['preds'][k]\n",
    "                )\n",
    "                ap_scores.append((k, ap_val))\n",
    "    \n",
    "            # compute median AP\n",
    "            ap_vals = [val for _, val in ap_scores]\n",
    "            median_ap = np.median(ap_vals)\n",
    "    \n",
    "            # select folds above (or equal to) median\n",
    "            selected_folds = [fold for fold, val in ap_scores if val >= median_ap]\n",
    "            selected_folds_dict[mname] = selected_folds  # save to dict\n",
    "    \n",
    "            print(f\"Selected {len(selected_folds)} folds for model {mname} ( median AP = {median_ap:.3f})\")\n",
    "        \n",
    "        print(selected_folds_dict['district_violence_disaster_rob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d95b5-b26b-4915-ae12-ff8ee62c4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 50-best peforming folds\n",
    "shapvalsdict_filtered = {\n",
    "    model: {fold: shapvalsdict[model][fold] \n",
    "            for fold in selected_folds_dict[model] if fold in shapvalsdict[model]}\n",
    "    for model in selected_folds_dict.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd0d9e-0bdf-47af-b911-a98dc24ed053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual features for mean folds for best 50 folds \n",
    "if shap_eval:\n",
    "    analysis_models_shap = allmodels\n",
    "    fig_nr = 'SI_figS8'\n",
    "\n",
    "    for mod in analysis_models_shap:\n",
    "        print(mod['model_name'])\n",
    "        mname= mod['model_name']\n",
    "\n",
    "        pathsave = os.path.join(\n",
    "            output_paths['shapvals'],\n",
    "            f'{fig_nr}_shap_mean_best50_barplots_{mname}_{esttype}_test_{run_shap}.png')\n",
    "\n",
    "        fun_plots.plot_barshap(\n",
    "            dict_shaps=shapvalsdict_filtered, \n",
    "            dict_train_cols=train_outdict, \n",
    "            name_dict=name_dict,\n",
    "            modelname=mname,\n",
    "            do_mean=False,\n",
    "            do_best=True,\n",
    "            #bestfold=bf,\n",
    "            #n_range=100, # Number of repetitions\n",
    "            selected_folds = selected_folds_dict[mname],\n",
    "            #threshold=0.01,\n",
    "            n_max_values=10,\n",
    "            save_fig_path=pathsave,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113485b-edfe-487d-9594-baa11e936586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample from the pooled \n",
    "pooled_shap_full_model = fun_plots.pool_shap_selected(\n",
    "    shapdict_model=shapvalsdict['district_violence_disaster'],\n",
    "    selected_folds=selected_folds_dict['district_violence_disaster'], \n",
    "    sample_size=10000, \n",
    "    random_state=1308\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af287c0-4455-4a5e-a260-494d2e42d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the pooled for all folds as robustness test\n",
    "pooled_shap_full_model_all = fun_plots.pool_shap_selected(\n",
    "    shapdict_model=shapvalsdict['district_violence_disaster'],\n",
    "    selected_folds=list(range(100)), \n",
    "    sample_size=10000, \n",
    "    random_state=1308\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe8add-4d5c-49a8-873a-445536823fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    # variables you want to plot\n",
    "    x_vars = [\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "    ]\n",
    "    labels = [\"a)\", \"b)\"]\n",
    "    fig_nr = \"fig4\"\n",
    "    txt_save = \"actuals\"\n",
    "\n",
    "    for x_var, figlab in zip(x_vars, labels):\n",
    "        fun_plots.beeswarmplot_continuous(\n",
    "            shapvals=pooled_shap_full_model_all, # pooled model\n",
    "            model=district_violence_disaster, \n",
    "            varname=x_var, \n",
    "            name_dict=name_dict,\n",
    "            #figure_label=figlab,\n",
    "            path_figure=os.path.join(\n",
    "                output_paths['shapvals'],\n",
    "                f\"main_{fig_nr}{figlab}_beeswarm_{x_var}_{txt_save}_{esttype}_pooled_{run_shap}.png\"\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eefa2a-34db-4045-96f6-eb1ef683b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    # variables you want to plot\n",
    "    x_vars = [\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "    ]\n",
    "    labels = [\"a)\", \"b)\"]\n",
    "    fig_nr = \"SI_figS9\"\n",
    "    txt_save = \"actuals\"\n",
    "\n",
    "    for x_var, figlab in zip(x_vars, labels):\n",
    "        fun_plots.beeswarmplot_continuous(\n",
    "            shapvals=pooled_shap_full_model, # pooled model only beased on best folds\n",
    "            model=district_violence_disaster, \n",
    "            varname=x_var, \n",
    "            name_dict=name_dict,\n",
    "            #figure_label=figlab,\n",
    "            path_figure=os.path.join(\n",
    "                output_paths['shapvals'],\n",
    "                f\"{fig_nr}{figlab}_beeswarm_{x_var}_{txt_save}_{esttype}_pooled_robustness_{run_shap}.png\"\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09839933-efff-42f5-ae1f-690c802f3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    analysis_models_shap = allmodels\n",
    "    best_folds_dict = {}  \n",
    "\n",
    "    for mod in analysis_models_shap:\n",
    "        print(mod['model_name'])\n",
    "        mname = mod['model_name']\n",
    "\n",
    "        # collect AP for each fold\n",
    "        ap_scores = []\n",
    "        for k in range(len(preds_dict_shap[mname]['preds'])):  \n",
    "            ap_val = average_precision_score(\n",
    "                y_true = preds_dict_shap[mname]['actuals'][k],\n",
    "                y_score = preds_dict_shap[mname]['preds'][k]\n",
    "            )\n",
    "            ap_scores.append((k, ap_val))\n",
    "\n",
    "        # compute median AP\n",
    "        ap_vals = [val for _, val in ap_scores]\n",
    "        median_ap = np.max(ap_vals)\n",
    "\n",
    "        # select folds above (or equal to) median\n",
    "        selected_folds = [fold for fold, val in ap_scores if val == median_ap]\n",
    "        best_folds_dict[mname] = selected_folds  # save to dict\n",
    "\n",
    "        print(f\"Selected {len(selected_folds)} folds for model {mname} ( median AP = {median_ap:.3f})\")\n",
    "    \n",
    "    print(best_folds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78111d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look at initial relationships between indicators of interest.\n",
    "if shap_eval:\n",
    "    fig_nr = 'SI_figs16'\n",
    "    analysis_models_shap = allmodels\n",
    "\n",
    "    bestfolds = [37,38,38,38]\n",
    "\n",
    "    for mod,bf in zip(analysis_models_shap,bestfolds):\n",
    "        mname = mod['model_name']\n",
    "        fig, ax = plt.subplots()\n",
    "        shap.plots.beeswarm(\n",
    "            shapvalsdict[mname][bf],\n",
    "            max_display=20,show=False)\n",
    "\n",
    "        # Modifying main plot parameters\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=14)\n",
    "        plt.tick_params(axis='x', labelcolor='black')\n",
    "\n",
    "        # Get colorbar\n",
    "        cb_ax = fig.axes[1] \n",
    "\n",
    "        # Modifying color bar parameters\n",
    "        cb_ax.tick_params(labelsize=14)\n",
    "        cb_ax.set_ylabel(\"Feature value\", fontsize=14)\n",
    "\n",
    "        # Re-label\n",
    "        y_labels = [label.get_text() for label in plt.gca().get_yticklabels()]\n",
    "        relabeled_strings = [name_dict.get(label, label) for label in y_labels]\n",
    "\n",
    "        ax.set_yticklabels(relabeled_strings,size=14,color='black')\n",
    "\n",
    "        if save_fig:\n",
    "\n",
    "            plt.savefig(os.path.join(\n",
    "                output_paths['shapvals'],\n",
    "                f'{fig_nr}_shap_summaryplots_bestfold_{mname}_{esttype}_test_{run_shap}.png')\n",
    "                        ,bbox_inches=\"tight\", dpi=400,\n",
    "                        transparent=False)\n",
    "            plt.show(fig)\n",
    "            plt.close()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd59dd9-0652-40c9-8aef-34720153d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    if do_robustness:\n",
    "\n",
    "        analysis_models_shap = [district_violence_disaster_rob]\n",
    "        figlabels = ['b)']\n",
    "        fig_nr = 'fig3b'\n",
    "    \n",
    "        for mod,figlabel in zip(analysis_models_shap,figlabels):\n",
    "            print(mod['model_name'])\n",
    "            mname= mod['model_name']\n",
    "    \n",
    "            pathsave = os.path.join(\n",
    "                output_paths['shapvals'],\n",
    "                f'rob_{fig_nr}_shap_mean_barplots_{mname}_{esttype}_test_{run_robustness}.png')\n",
    "    \n",
    "            fun_plots.plot_barshap(\n",
    "                dict_shaps=shapvalsdict, \n",
    "                dict_train_cols=train_outdict, \n",
    "                name_dict=name_dict,\n",
    "                modelname=mname,\n",
    "                do_mean=True,\n",
    "                do_best=False,\n",
    "                #bestfold=bf,\n",
    "                n_range=n_reps_shap,\n",
    "                #threshold=0.01,\n",
    "                n_max_values=10,\n",
    "                #figure_label=figlabel,\n",
    "                save_fig_path=pathsave,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f4df",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Heterogenous effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    fold_n = 38\n",
    "    esttype = 'xgb'\n",
    "    path_figure = os.path.join(\n",
    "                    output_paths['shapvals'],\n",
    "                    f'main_fig5_{esttype}_f{fold_n}_train_{run_shap}.png')\n",
    "\n",
    "    # Define multiple variable sets and their corresponding names for each plot\n",
    "    plot_configs = [\n",
    "        {'select_var2': 'deco_best_osv_ts_decay_6_tlag12', 'pretty_name': 'OSV Electoral Violence', 'cutoffs': [1]},  # Plot 1: One-sided violence\n",
    "        {'select_var2': 'gdis_disastertype_flood_ts_decay_6_tlag12', 'pretty_name': 'Floods', 'cutoffs': [1]},  # Plot 2: Floods\n",
    "        #{'select_var2': 'gdis_disastertype_earthquake_ts_decay_6_tlag12', 'pretty_name': 'Earthquakes', 'cutoffs': [1]},  # Plot 3: Earthquakes\n",
    "        {'select_var2': 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'pretty_name': 'Landslides', 'cutoffs': [1]}  # Plot 4: Landslides\n",
    "    ]\n",
    "\n",
    "    # Variables to be used across all the plots\n",
    "    select_vars = ['mig_history','ln_loans','ln_remittances_in_value',]\n",
    "\n",
    "    # Mapping select_vars to more readable labels\n",
    "    var_labels = {\n",
    "        'ln_remittances_in_value': 'Remittances (ln)',\n",
    "        'ln_loans': 'Loans (ln)',\n",
    "        'mig_history': 'Migration History',\n",
    "    }\n",
    "\n",
    "    # Define the labels for the textboxes\n",
    "    textbox_labels = ['a)', 'b)', 'c)', 'd)']\n",
    "\n",
    "    do_net_effects = True\n",
    "\n",
    "    # Set up subplots: create 1 row and 4 columns, sharing y-axis for the same select_vars\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "    # Plot elements to collect for a single legend\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over each plot configuration to generate the plots\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        select_var2 = config['select_var2']  # Get the variable for this plot\n",
    "        var2_pretty_name = config['pretty_name']  # Get the pretty name for this variable\n",
    "        var2_cutoffs = config['cutoffs']  # Get the cutoffs for this variable\n",
    "\n",
    "        shapdf = pd.DataFrame()  # Initialize a new shapdf for each plot\n",
    "\n",
    "        # Prepare the shapdf with SHAP values and the selected variable \n",
    "        # SHAP values\n",
    "        shapdf = pd.DataFrame(\n",
    "            shapvalsdict[\"district_violence_disaster\"][fold_n].values,\n",
    "            columns=train_outdict[\"district_violence_disaster\"][fold_n].columns\n",
    "        )\n",
    "        \n",
    "        # Add conditioning variable (from .data)\n",
    "        shapdf[var2_pretty_name] = (\n",
    "            pd.DataFrame(\n",
    "                shapvalsdict[\"district_violence_disaster\"][fold_n].data,\n",
    "                columns=train_outdict[\"district_violence_disaster\"][fold_n].columns\n",
    "            )[select_var2].values\n",
    "        )\n",
    "\n",
    "        # Create an empty DataFrame to store results for each cutoff\n",
    "        shap_means_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each cutoff and calculate SHAP values\n",
    "        for cutoff in var2_cutoffs:\n",
    "            if do_net_effects:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].mean(0)\n",
    "            else:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].abs().mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].abs().mean(0)\n",
    "\n",
    "            # Store the results in the DataFrame\n",
    "            shap_means_df[f'{var2_pretty_name} >= {cutoff}'] = shap_mean_gte\n",
    "            shap_means_df[f'{var2_pretty_name} < {cutoff}'] = shap_mean_lt\n",
    "\n",
    "        # Offset the \"No\" group slightly to the left for visibility\n",
    "        offset = 0.1 # Adjust the offset to control the separation between the bars\n",
    "\n",
    "        # Plot the current variable cohort in the corresponding subplot\n",
    "        bars1 = axes[i].barh(np.arange(len(select_vars)) - offset, shap_means_df[f'{var2_pretty_name} >= {cutoff}'], \n",
    "                             height=0.2, color='darkred', label='Yes')\n",
    "        bars2 = axes[i].barh(np.arange(len(select_vars)) + offset, shap_means_df[f'{var2_pretty_name} < {cutoff}'], \n",
    "                             height=0.2, color='darkblue', label='No')\n",
    "\n",
    "        # Collect bars for the legend\n",
    "        if i == 0:\n",
    "            bars = [bars1, bars2]\n",
    "\n",
    "        # Add vertical line at zero and horizontal dashed lines for the grid\n",
    "        axes[i].axvline(0, color='black', linewidth=1)  # Add vertical line at zero\n",
    "        axes[i].grid(axis='x', linestyle='--', linewidth=0.5)  # Dashed horizontal lines\n",
    "        axes[i].set_title(f'{var2_pretty_name}', fontsize=22)\n",
    "        axes[i].set_xlabel('Mean SHAP Value',fontsize=14)\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_visible(False)\n",
    "        axes[i].spines['bottom'].set_visible(False)\n",
    "\n",
    "        # Rename the y-tick labels using var_labels\n",
    "        axes[i].set_yticks(np.arange(len(select_vars)))  # Set y-ticks to be in the middle\n",
    "        axes[i].set_yticklabels([var_labels[var] for var in select_vars], fontsize=14)\n",
    "\n",
    "        # Adjust x-ticks: fewer and larger\n",
    "        axes[i].tick_params(axis='x', labelsize=14)\n",
    "        axes[i].set_xlim([-0.01, 0.01])\n",
    "\n",
    "        # Add text box in the lower right corner of each plot\n",
    "        axes[i].text(0.95, 0.05, textbox_labels[i], fontsize=18, transform=axes[i].transAxes, \n",
    "                     verticalalignment='bottom', horizontalalignment='right', \n",
    "                     bbox=dict(facecolor='white',edgecolor='white' ,boxstyle='round,pad=0.5'))\n",
    "\n",
    "    # Add a common x-label, y-label, and title\n",
    "    fig.supylabel('')\n",
    "\n",
    "    # Add a single legend below the figure\n",
    "    fig.legend(bars, handles=[bars1, bars2], loc='lower center', ncol=2, fontsize=16, title_fontsize=16,   \n",
    "               title='Recent exposure to Violence / Disaster', fancybox=False, edgecolor='black',\n",
    "               bbox_to_anchor=(0.5, -0.15))  # Position the legend below the figure\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.subplots_adjust(bottom=0.09)\n",
    "\n",
    "    if path_figure: \n",
    "        plt.savefig(path_figure, bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "        plt.show(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc6e8c-2f28-4332-8a3e-802f9953c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    esttype = 'xgb'\n",
    "    path_figure = os.path.join(\n",
    "        output_paths['shapvals'],\n",
    "        f'main_fig5_{esttype}_allflods_train_{run_shap}.png'\n",
    "    )\n",
    "\n",
    "    # Define multiple variable sets and their corresponding names for each plot\n",
    "    plot_configs = [\n",
    "        {'select_var2': 'deco_best_osv_ts_decay_6_tlag12', 'pretty_name': 'OSV Electoral Violence', 'cutoffs': [1]},\n",
    "        {'select_var2': 'gdis_disastertype_flood_ts_decay_6_tlag12', 'pretty_name': 'Floods', 'cutoffs': [1]},\n",
    "        #{'select_var2': 'gdis_disastertype_earthquake_ts_decay_6_tlag12', 'pretty_name': 'Earthquakes', 'cutoffs': [1]},\n",
    "        {'select_var2': 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'pretty_name': 'Landslides', 'cutoffs': [1]}\n",
    "    ]\n",
    "\n",
    "    # Variables to be used across all the plots\n",
    "    select_vars = ['mig_history', 'ln_loans', 'ln_remittances_in_value']\n",
    "\n",
    "    # Mapping select_vars to more readable labels\n",
    "    var_labels = {\n",
    "        'ln_remittances_in_value': 'Remittances (ln)',\n",
    "        'ln_loans': 'Loans (ln)',\n",
    "        'mig_history': 'Migration History',\n",
    "    }\n",
    "\n",
    "    # Define the labels for the textboxes\n",
    "    textbox_labels = ['a)', 'b)', 'c)']\n",
    "\n",
    "    do_net_effects = True\n",
    "\n",
    "    # Set up subplots: create 1 row and 4 columns, sharing y-axis for the same select_vars\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "    # Iterate over each plot configuration to generate the plots\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        select_var2 = config['select_var2']\n",
    "        var2_pretty_name = config['pretty_name']\n",
    "        var2_cutoffs = config['cutoffs']\n",
    "\n",
    "        # --- collect shapdf across all folds ---\n",
    "        all_shapdfs = []\n",
    "        for f in list(range(n_reps_shap)):  \n",
    "            shapdf_f = pd.DataFrame(\n",
    "                shapvalsdict[\"district_violence_disaster\"][f].values,\n",
    "                columns=train_outdict[\"district_violence_disaster\"][f].columns\n",
    "            )\n",
    "            shapdf_f[var2_pretty_name] = (\n",
    "                pd.DataFrame(\n",
    "                    shapvalsdict[\"district_violence_disaster\"][f].data,\n",
    "                    columns=train_outdict[\"district_violence_disaster\"][f].columns\n",
    "                )[select_var2].values\n",
    "            )\n",
    "            all_shapdfs.append(shapdf_f)\n",
    "\n",
    "        shapdf = pd.concat(all_shapdfs, ignore_index=True)\n",
    "\n",
    "        # Create an empty DataFrame to store results for each cutoff\n",
    "        shap_means_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each cutoff and calculate SHAP values\n",
    "        for cutoff in var2_cutoffs:\n",
    "            if do_net_effects:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].mean(0)\n",
    "            else:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].abs().mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].abs().mean(0)\n",
    "\n",
    "            # Store the results in the DataFrame\n",
    "            shap_means_df[f'{var2_pretty_name} >= {cutoff}'] = shap_mean_gte\n",
    "            shap_means_df[f'{var2_pretty_name} < {cutoff}'] = shap_mean_lt\n",
    "\n",
    "        # Offset the \"No\" group slightly to the left for visibility\n",
    "        offset = 0.1\n",
    "\n",
    "        # Plot the current variable cohort in the corresponding subplot\n",
    "        bars1 = axes[i].barh(np.arange(len(select_vars)) - offset,\n",
    "                             shap_means_df[f'{var2_pretty_name} >= {cutoff}'],\n",
    "                             height=0.2, color='darkred', label='Yes')\n",
    "        bars2 = axes[i].barh(np.arange(len(select_vars)) + offset,\n",
    "                             shap_means_df[f'{var2_pretty_name} < {cutoff}'],\n",
    "                             height=0.2, color='darkblue', label='No')\n",
    "\n",
    "        # Add vertical line at zero and horizontal dashed lines for the grid\n",
    "        axes[i].axvline(0, color='black', linewidth=1)\n",
    "        axes[i].grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "        axes[i].set_title(f'{var2_pretty_name}', fontsize=22)\n",
    "        axes[i].set_xlabel('Mean SHAP Value', fontsize=14)\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_visible(False)\n",
    "        axes[i].spines['bottom'].set_visible(False)\n",
    "\n",
    "        # Rename the y-tick labels using var_labels\n",
    "        axes[i].set_yticks(np.arange(len(select_vars)))\n",
    "        axes[i].set_yticklabels([var_labels[var] for var in select_vars], fontsize=14)\n",
    "\n",
    "        # Adjust x-ticks: fewer and larger\n",
    "        axes[i].tick_params(axis='x', labelsize=14)\n",
    "        axes[i].set_xlim([-0.01, 0.01])\n",
    "\n",
    "        # Add text box in the lower right corner of each plot\n",
    "        axes[i].text(0.95, 0.05, textbox_labels[i], fontsize=18, transform=axes[i].transAxes,\n",
    "                     verticalalignment='bottom', horizontalalignment='right',\n",
    "                     bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.5'))\n",
    "\n",
    "    # Add a single legend below the figure\n",
    "    fig.legend([bars1, bars2], ['Yes', 'No'], loc='lower center', ncol=2,\n",
    "               fontsize=16, title_fontsize=16,\n",
    "               title='Recent exposure to Violence / Disaster', fancybox=False,\n",
    "               edgecolor='black', bbox_to_anchor=(0.5, -0.15))\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.subplots_adjust(bottom=0.09)\n",
    "\n",
    "    if path_figure: \n",
    "        plt.savefig(path_figure, bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae085719-4253-4d35-b0a8-1911a960737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    esttype = 'xgb'\n",
    "    path_figure = os.path.join(\n",
    "        output_paths['shapvals'],\n",
    "        f'SI_figS10_{esttype}_bestfolds_train_{run_shap}.png'\n",
    "    )\n",
    "\n",
    "    # Define multiple variable sets and their corresponding names for each plot\n",
    "    plot_configs = [\n",
    "        {'select_var2': 'deco_best_osv_ts_decay_6_tlag12', 'pretty_name': 'OSV Electoral Violence', 'cutoffs': [1]},\n",
    "        {'select_var2': 'gdis_disastertype_flood_ts_decay_6_tlag12', 'pretty_name': 'Floods', 'cutoffs': [1]},\n",
    "        #{'select_var2': 'gdis_disastertype_earthquake_ts_decay_6_tlag12', 'pretty_name': 'Earthquakes', 'cutoffs': [1]},\n",
    "        {'select_var2': 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'pretty_name': 'Landslides', 'cutoffs': [1]}\n",
    "    ]\n",
    "\n",
    "    # Variables to be used across all the plots\n",
    "    select_vars = ['mig_history', 'ln_loans', 'ln_remittances_in_value']\n",
    "\n",
    "    # Mapping select_vars to more readable labels\n",
    "    var_labels = {\n",
    "        'ln_remittances_in_value': 'Remittances (ln)',\n",
    "        'ln_loans': 'Loans (ln)',\n",
    "        'mig_history': 'Migration History',\n",
    "    }\n",
    "\n",
    "    # Define the labels for the textboxes\n",
    "    textbox_labels = ['a)', 'b)', 'c)']\n",
    "\n",
    "    do_net_effects = True\n",
    "\n",
    "    # Set up subplots: create 1 row and 4 columns, sharing y-axis for the same select_vars\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "    # Iterate over each plot configuration to generate the plots\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        select_var2 = config['select_var2']\n",
    "        var2_pretty_name = config['pretty_name']\n",
    "        var2_cutoffs = config['cutoffs']\n",
    "\n",
    "        # --- collect shapdf across all folds ---\n",
    "        all_shapdfs = []\n",
    "        for f in selected_folds_dict['district_violence_disaster']:  \n",
    "            shapdf_f = pd.DataFrame(\n",
    "                shapvalsdict_filtered[\"district_violence_disaster\"][f].values,\n",
    "                columns=train_outdict[\"district_violence_disaster\"][f].columns\n",
    "            )\n",
    "            shapdf_f[var2_pretty_name] = (\n",
    "                pd.DataFrame(\n",
    "                    shapvalsdict_filtered[\"district_violence_disaster\"][f].data,\n",
    "                    columns=train_outdict[\"district_violence_disaster\"][f].columns\n",
    "                )[select_var2].values\n",
    "            )\n",
    "            all_shapdfs.append(shapdf_f)\n",
    "\n",
    "        shapdf = pd.concat(all_shapdfs, ignore_index=True)\n",
    "\n",
    "        # Create an empty DataFrame to store results for each cutoff\n",
    "        shap_means_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each cutoff and calculate SHAP values\n",
    "        for cutoff in var2_cutoffs:\n",
    "            if do_net_effects:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].mean(0)\n",
    "            else:\n",
    "                shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].abs().mean(0)\n",
    "                shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].abs().mean(0)\n",
    "\n",
    "            # Store the results in the DataFrame\n",
    "            shap_means_df[f'{var2_pretty_name} >= {cutoff}'] = shap_mean_gte\n",
    "            shap_means_df[f'{var2_pretty_name} < {cutoff}'] = shap_mean_lt\n",
    "\n",
    "        # Offset the \"No\" group slightly to the left for visibility\n",
    "        offset = 0.1\n",
    "\n",
    "        # Plot the current variable cohort in the corresponding subplot\n",
    "        bars1 = axes[i].barh(np.arange(len(select_vars)) - offset,\n",
    "                             shap_means_df[f'{var2_pretty_name} >= {cutoff}'],\n",
    "                             height=0.2, color='darkred', label='Yes')\n",
    "        bars2 = axes[i].barh(np.arange(len(select_vars)) + offset,\n",
    "                             shap_means_df[f'{var2_pretty_name} < {cutoff}'],\n",
    "                             height=0.2, color='darkblue', label='No')\n",
    "\n",
    "        # Add vertical line at zero and horizontal dashed lines for the grid\n",
    "        axes[i].axvline(0, color='black', linewidth=1)\n",
    "        axes[i].grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "        axes[i].set_title(f'{var2_pretty_name}', fontsize=22)\n",
    "        axes[i].set_xlabel('Mean SHAP Value', fontsize=14)\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_visible(False)\n",
    "        axes[i].spines['bottom'].set_visible(False)\n",
    "\n",
    "        # Rename the y-tick labels using var_labels\n",
    "        axes[i].set_yticks(np.arange(len(select_vars)))\n",
    "        axes[i].set_yticklabels([var_labels[var] for var in select_vars], fontsize=14)\n",
    "\n",
    "        # Adjust x-ticks: fewer and larger\n",
    "        axes[i].tick_params(axis='x', labelsize=14)\n",
    "        axes[i].set_xlim([-0.01, 0.01])\n",
    "\n",
    "        # Add text box in the lower right corner of each plot\n",
    "        axes[i].text(0.95, 0.05, textbox_labels[i], fontsize=18, transform=axes[i].transAxes,\n",
    "                     verticalalignment='bottom', horizontalalignment='right',\n",
    "                     bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.5'))\n",
    "\n",
    "    # Add a single legend below the figure\n",
    "    fig.legend([bars1, bars2], ['Yes', 'No'], loc='lower center', ncol=2,\n",
    "               fontsize=16, title_fontsize=16,\n",
    "               title='Recent exposure to Violence / Disaster', fancybox=False,\n",
    "               edgecolor='black', bbox_to_anchor=(0.5, -0.15))\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.subplots_adjust(bottom=0.09)\n",
    "\n",
    "    if path_figure: \n",
    "        plt.savefig(path_figure, bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ea031-1078-4a2b-bf09-863cc9bc61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness test excluding migration history\n",
    "\n",
    "if shap_eval:\n",
    "    if do_robustness:\n",
    "        fold_n = 37\n",
    "        esttype = 'xgb'\n",
    "        path_figure = os.path.join(\n",
    "                        output_paths['shapvals'],\n",
    "                        f'rob_fig5_{esttype}_f{fold_n}_train_{run_robustness}.png')\n",
    "    \n",
    "        # Define multiple variable sets and their corresponding names for each plot\n",
    "        plot_configs = [\n",
    "            {'select_var2': 'deco_best_osv_ts_decay_6_tlag12', 'pretty_name': 'OSV Electoral Violence', 'cutoffs': [1]},  # Plot 1: One-sided violence\n",
    "            {'select_var2': 'gdis_disastertype_flood_ts_decay_6_tlag12', 'pretty_name': 'Floods', 'cutoffs': [1]},  # Plot 2: Floods\n",
    "            #{'select_var2': 'gdis_disastertype_earthquake_ts_decay_6_tlag12', 'pretty_name': 'Earthquakes', 'cutoffs': [1]},  # Plot 3: Earthquakes\n",
    "            {'select_var2': 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'pretty_name': 'Landslides', 'cutoffs': [1]}  # Plot 4: Landslides\n",
    "        ]\n",
    "    \n",
    "        # Variables to be used across all the plots\n",
    "        select_vars = ['ln_loans','ln_remittances_in_value',]\n",
    "    \n",
    "        # Mapping select_vars to more readable labels\n",
    "        var_labels = {\n",
    "            'ln_remittances_in_value': 'Remittances (ln)',\n",
    "            'ln_loans': 'Loans (ln)',\n",
    "        }\n",
    "    \n",
    "        # Define the labels for the textboxes\n",
    "        textbox_labels = ['a)', 'b)', 'c)', 'd)']\n",
    "    \n",
    "        do_net_effects = True\n",
    "    \n",
    "        # Set up subplots: create 1 row and 4 columns, sharing y-axis for the same select_vars\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "    \n",
    "        # Plot elements to collect for a single legend\n",
    "        bars = []\n",
    "    \n",
    "        # Iterate over each plot configuration to generate the plots\n",
    "        for i, config in enumerate(plot_configs):\n",
    "            select_var2 = config['select_var2']  # Get the variable for this plot\n",
    "            var2_pretty_name = config['pretty_name']  # Get the pretty name for this variable\n",
    "            var2_cutoffs = config['cutoffs']  # Get the cutoffs for this variable\n",
    "    \n",
    "            shapdf = pd.DataFrame()  # Initialize a new shapdf for each plot\n",
    "    \n",
    "            # Prepare the shapdf with SHAP values and the selected variable \n",
    "            # SHAP values\n",
    "            shapdf = pd.DataFrame(\n",
    "                shapvalsdict[\"district_violence_disaster_rob\"][fold_n].values,\n",
    "                columns=train_outdict[\"district_violence_disaster_rob\"][fold_n].columns\n",
    "            )\n",
    "            \n",
    "            # Add conditioning variable (from .data)\n",
    "            shapdf[var2_pretty_name] = (\n",
    "                pd.DataFrame(\n",
    "                    shapvalsdict[\"district_violence_disaster_rob\"][fold_n].data,\n",
    "                    columns=train_outdict[\"district_violence_disaster_rob\"][fold_n].columns\n",
    "                )[select_var2].values\n",
    "            )\n",
    "    \n",
    "            # Create an empty DataFrame to store results for each cutoff\n",
    "            shap_means_df = pd.DataFrame()\n",
    "    \n",
    "            # Iterate over each cutoff and calculate SHAP values\n",
    "            for cutoff in var2_cutoffs:\n",
    "                if do_net_effects:\n",
    "                    shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].mean(0)\n",
    "                    shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].mean(0)\n",
    "                else:\n",
    "                    shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].abs().mean(0)\n",
    "                    shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].abs().mean(0)\n",
    "    \n",
    "                # Store the results in the DataFrame\n",
    "                shap_means_df[f'{var2_pretty_name} >= {cutoff}'] = shap_mean_gte\n",
    "                shap_means_df[f'{var2_pretty_name} < {cutoff}'] = shap_mean_lt\n",
    "    \n",
    "            # Offset the \"No\" group slightly to the left for visibility\n",
    "            offset = 0.1 # Adjust the offset to control the separation between the bars\n",
    "    \n",
    "            # Plot the current variable cohort in the corresponding subplot\n",
    "            bars1 = axes[i].barh(np.arange(len(select_vars)) - offset, shap_means_df[f'{var2_pretty_name} >= {cutoff}'], \n",
    "                                 height=0.2, color='darkred', label='Yes')\n",
    "            bars2 = axes[i].barh(np.arange(len(select_vars)) + offset, shap_means_df[f'{var2_pretty_name} < {cutoff}'], \n",
    "                                 height=0.2, color='darkblue', label='No')\n",
    "    \n",
    "            # Collect bars for the legend\n",
    "            if i == 0:\n",
    "                bars = [bars1, bars2]\n",
    "    \n",
    "            # Add vertical line at zero and horizontal dashed lines for the grid\n",
    "            axes[i].axvline(0, color='black', linewidth=1)  # Add vertical line at zero\n",
    "            axes[i].grid(axis='x', linestyle='--', linewidth=0.5)  # Dashed horizontal lines\n",
    "            axes[i].set_title(f'{var2_pretty_name}', fontsize=22)\n",
    "            axes[i].set_xlabel('Mean SHAP Value',fontsize=14)\n",
    "            axes[i].spines['top'].set_visible(False)\n",
    "            axes[i].spines['right'].set_visible(False)\n",
    "            axes[i].spines['left'].set_visible(False)\n",
    "            axes[i].spines['bottom'].set_visible(False)\n",
    "    \n",
    "            # Rename the y-tick labels using var_labels\n",
    "            axes[i].set_yticks(np.arange(len(select_vars)))  # Set y-ticks to be in the middle\n",
    "            axes[i].set_yticklabels([var_labels[var] for var in select_vars], fontsize=14)\n",
    "    \n",
    "            # Adjust x-ticks: fewer and larger\n",
    "            axes[i].tick_params(axis='x', labelsize=14)\n",
    "            axes[i].set_xlim([-0.01, 0.01])\n",
    "    \n",
    "            # Add text box in the lower right corner of each plot\n",
    "            axes[i].text(0.95, 0.05, textbox_labels[i], fontsize=18, transform=axes[i].transAxes, \n",
    "                         verticalalignment='bottom', horizontalalignment='right', \n",
    "                         bbox=dict(facecolor='white',edgecolor='white' ,boxstyle='round,pad=0.5'))\n",
    "    \n",
    "        # Add a common x-label, y-label, and title\n",
    "        fig.supylabel('')\n",
    "    \n",
    "        # Add a single legend below the figure\n",
    "        fig.legend(bars, handles=[bars1, bars2], loc='lower center', ncol=2, fontsize=16, title_fontsize=16,   \n",
    "                   title='Recent exposure to Violence / Disaster', fancybox=False, edgecolor='black',\n",
    "                   bbox_to_anchor=(0.5, -0.15))  # Position the legend below the figure\n",
    "    \n",
    "        # Adjust layout to make space for the legend\n",
    "        plt.subplots_adjust(bottom=0.09)\n",
    "    \n",
    "        if path_figure: \n",
    "            plt.savefig(path_figure, bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "            plt.show(fig)\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4cf14-5c67-45de-afb8-ab6dea8c6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    if do_robustness:\n",
    "        esttype = 'xgb'\n",
    "        path_figure = os.path.join(\n",
    "            output_paths['shapvals'],\n",
    "            f'SI_rob_figS10_{esttype}_bestfolds_train_{run_shap}.png'\n",
    "        )\n",
    "    \n",
    "        # Define multiple variable sets and their corresponding names for each plot\n",
    "        plot_configs = [\n",
    "            {'select_var2': 'deco_best_osv_ts_decay_6_tlag12', 'pretty_name': 'OSV Electoral Violence', 'cutoffs': [1]},\n",
    "            {'select_var2': 'gdis_disastertype_flood_ts_decay_6_tlag12', 'pretty_name': 'Floods', 'cutoffs': [1]},\n",
    "            #{'select_var2': 'gdis_disastertype_earthquake_ts_decay_6_tlag12', 'pretty_name': 'Earthquakes', 'cutoffs': [1]},\n",
    "            {'select_var2': 'gdis_disastertype_landslide_ts_decay_6_tlag12', 'pretty_name': 'Landslides', 'cutoffs': [1]}\n",
    "        ]\n",
    "    \n",
    "        # Variables to be used across all the plots\n",
    "        select_vars = ['ln_loans', 'ln_remittances_in_value']\n",
    "    \n",
    "        # Mapping select_vars to more readable labels\n",
    "        var_labels = {\n",
    "            'ln_remittances_in_value': 'Remittances (ln)',\n",
    "            'ln_loans': 'Loans (ln)',\n",
    "        }\n",
    "    \n",
    "        # Define the labels for the textboxes\n",
    "        textbox_labels = ['a)', 'b)', 'c)']\n",
    "    \n",
    "        do_net_effects = True\n",
    "    \n",
    "        # Set up subplots: create 1 row and 4 columns, sharing y-axis for the same select_vars\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "    \n",
    "        # Iterate over each plot configuration to generate the plots\n",
    "        for i, config in enumerate(plot_configs):\n",
    "            select_var2 = config['select_var2']\n",
    "            var2_pretty_name = config['pretty_name']\n",
    "            var2_cutoffs = config['cutoffs']\n",
    "    \n",
    "            # --- collect shapdf across all folds ---\n",
    "            all_shapdfs = []\n",
    "            for f in selected_folds_dict['district_violence_disaster_rob']:  \n",
    "                shapdf_f = pd.DataFrame(\n",
    "                    shapvalsdict_filtered[\"district_violence_disaster_rob\"][f].values,\n",
    "                    columns=train_outdict[\"district_violence_disaster_rob\"][f].columns\n",
    "                )\n",
    "                shapdf_f[var2_pretty_name] = (\n",
    "                    pd.DataFrame(\n",
    "                        shapvalsdict_filtered[\"district_violence_disaster_rob\"][f].data,\n",
    "                        columns=train_outdict[\"district_violence_disaster_rob\"][f].columns\n",
    "                    )[select_var2].values\n",
    "                )\n",
    "                all_shapdfs.append(shapdf_f)\n",
    "    \n",
    "            shapdf = pd.concat(all_shapdfs, ignore_index=True)\n",
    "    \n",
    "            # Create an empty DataFrame to store results for each cutoff\n",
    "            shap_means_df = pd.DataFrame()\n",
    "    \n",
    "            # Iterate over each cutoff and calculate SHAP values\n",
    "            for cutoff in var2_cutoffs:\n",
    "                if do_net_effects:\n",
    "                    shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].mean(0)\n",
    "                    shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].mean(0)\n",
    "                else:\n",
    "                    shap_mean_gte = shapdf[shapdf[var2_pretty_name] >= cutoff][select_vars].abs().mean(0)\n",
    "                    shap_mean_lt = shapdf[shapdf[var2_pretty_name] < cutoff][select_vars].abs().mean(0)\n",
    "    \n",
    "                # Store the results in the DataFrame\n",
    "                shap_means_df[f'{var2_pretty_name} >= {cutoff}'] = shap_mean_gte\n",
    "                shap_means_df[f'{var2_pretty_name} < {cutoff}'] = shap_mean_lt\n",
    "    \n",
    "            # Offset the \"No\" group slightly to the left for visibility\n",
    "            offset = 0.1\n",
    "    \n",
    "            # Plot the current variable cohort in the corresponding subplot\n",
    "            bars1 = axes[i].barh(np.arange(len(select_vars)) - offset,\n",
    "                                 shap_means_df[f'{var2_pretty_name} >= {cutoff}'],\n",
    "                                 height=0.2, color='darkred', label='Yes')\n",
    "            bars2 = axes[i].barh(np.arange(len(select_vars)) + offset,\n",
    "                                 shap_means_df[f'{var2_pretty_name} < {cutoff}'],\n",
    "                                 height=0.2, color='darkblue', label='No')\n",
    "    \n",
    "            # Add vertical line at zero and horizontal dashed lines for the grid\n",
    "            axes[i].axvline(0, color='black', linewidth=1)\n",
    "            axes[i].grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "            axes[i].set_title(f'{var2_pretty_name}', fontsize=22)\n",
    "            axes[i].set_xlabel('Mean SHAP Value', fontsize=14)\n",
    "            axes[i].spines['top'].set_visible(False)\n",
    "            axes[i].spines['right'].set_visible(False)\n",
    "            axes[i].spines['left'].set_visible(False)\n",
    "            axes[i].spines['bottom'].set_visible(False)\n",
    "    \n",
    "            # Rename the y-tick labels using var_labels\n",
    "            axes[i].set_yticks(np.arange(len(select_vars)))\n",
    "            axes[i].set_yticklabels([var_labels[var] for var in select_vars], fontsize=14)\n",
    "    \n",
    "            # Adjust x-ticks: fewer and larger\n",
    "            axes[i].tick_params(axis='x', labelsize=14)\n",
    "            axes[i].set_xlim([-0.01, 0.01])\n",
    "    \n",
    "            # Add text box in the lower right corner of each plot\n",
    "            axes[i].text(0.95, 0.05, textbox_labels[i], fontsize=18, transform=axes[i].transAxes,\n",
    "                         verticalalignment='bottom', horizontalalignment='right',\n",
    "                         bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.5'))\n",
    "    \n",
    "        # Add a single legend below the figure\n",
    "        fig.legend([bars1, bars2], ['Yes', 'No'], loc='lower center', ncol=2,\n",
    "                   fontsize=16, title_fontsize=16,\n",
    "                   title='Recent exposure to Violence / Disaster', fancybox=False,\n",
    "                   edgecolor='black', bbox_to_anchor=(0.5, -0.15))\n",
    "    \n",
    "        # Adjust layout to make space for the legend\n",
    "        plt.subplots_adjust(bottom=0.09)\n",
    "    \n",
    "        if path_figure: \n",
    "            plt.savefig(path_figure, bbox_inches=\"tight\", dpi=400, transparent=False)\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fceb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555e3bd-3a72-4af7-92ff-133cd813c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    model_inter = district_violence_disaster\n",
    "    list_inter = [\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "        'spei_3_severe_ts_decay_6_tlag12',\n",
    "        'deco_best_sb_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "        'deco_best_ns_ts_decay_6_tlag12',\n",
    "                 ]\n",
    "    ########\n",
    "    saveintername = 'viodis'\n",
    "    path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     f'SI_figs11_shapinteraction_heatmap_absmean_{saveintername}.png')\n",
    "    interactdf,interactdf_raw = df_interactions_abs(\n",
    "        shap_interactions_dict = shapinterdict,\n",
    "        train_dict = train_outdict,\n",
    "        model = model_inter,\n",
    "        name_dict = name_dict,\n",
    "        inter_vars = list_inter,\n",
    "        selected_folds= list(range(n_reps_shap)),\n",
    "        remove_zeros = False,\n",
    "        agg_effect=False,\n",
    "        save_fig=True, \n",
    "        path_out=path_out_save,\n",
    "        colorpalette='magma_r'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78c618-583a-49e3-86b7-4da2c94719eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    model_inter = district_violence_disaster\n",
    "    list_inter = [\n",
    "        #'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        #'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "        #'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "        #'spei_3_severe_ts_decay_6_tlag12',\n",
    "        'deco_best_sb_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "        'deco_best_ns_ts_decay_6_tlag12',\n",
    "        'ln_remittances_in_value',\n",
    "        'ln_loans',\n",
    "        'mig_history',\n",
    "        'head_age',\n",
    "    ]\n",
    "\n",
    "    ########\n",
    "    saveintername = 'viohh'\n",
    "    path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     f'SI_figs11_shapinteraction_heatmap_absmean_{saveintername}.png')\n",
    "    interactdf_viohh,interactdf_raw_viohh = df_interactions_abs(\n",
    "        shap_interactions_dict = shapinterdict,\n",
    "        train_dict = train_outdict,\n",
    "        model = model_inter,\n",
    "        name_dict = name_dict,\n",
    "        inter_vars = list_inter,\n",
    "        selected_folds= list(range(n_reps_shap)),\n",
    "        remove_zeros = False,\n",
    "        agg_effect=False,\n",
    "        save_fig=True, \n",
    "        path_out=path_out_save,\n",
    "        colorpalette='magma_r'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceeddbf-e5e7-452d-bc12-a40d43c98f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    model_inter = district_violence_disaster\n",
    "    list_inter = [\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "        'spei_3_severe_ts_decay_6_tlag12',\n",
    "        #'deco_best_sb_ts_decay_6_tlag12',\n",
    "        #'deco_best_osv_ts_decay_6_tlag12',\n",
    "        #'deco_best_ns_ts_decay_6_tlag12',\n",
    "        'ln_remittances_in_value',\n",
    "        'ln_loans',\n",
    "        'mig_history',\n",
    "        'head_age',\n",
    "    ]\n",
    "\n",
    "    ########\n",
    "    saveintername = 'dishh'\n",
    "    path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     f'SI_figs11_shapinteraction_heatmap_absmean_{saveintername}.png')\n",
    "    interactdf_dishh,interactdf_raw_dishh = df_interactions_abs(\n",
    "        shap_interactions_dict = shapinterdict,\n",
    "        train_dict = train_outdict,\n",
    "        model = model_inter,\n",
    "        name_dict = name_dict,\n",
    "        inter_vars = list_inter,\n",
    "        selected_folds= list(range(n_reps_shap)),\n",
    "        remove_zeros = False,\n",
    "        agg_effect=False,\n",
    "        save_fig=True, \n",
    "        path_out=path_out_save,\n",
    "        colorpalette='magma_r'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dead13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    save_fig = True\n",
    "    path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     f'main_fig6_shapinteraction_barplot_f{fold_inter}.png')\n",
    "\n",
    "\n",
    "    # Replace diagonals with 0\n",
    "    np.fill_diagonal(interactdf_raw.values, 0)\n",
    "\n",
    "    # Compute the sums for the specified groups\n",
    "    interaction_sums = {\n",
    "        'Violence*Disasters': interactdf_raw.loc[feat_elect_vio_agg+feat_violence_agg, feat_disaster_agg].sum().sum(),\n",
    "        'Violence*Baseline': interactdf_raw.loc[feat_elect_vio_agg+feat_violence_agg, feat_baseline_district].sum().sum(),\n",
    "        'Disasters*Baseline': interactdf_raw.loc[feat_disaster_agg, feat_baseline_district].sum().sum(),\n",
    "        'Violence*Violence': interactdf_raw.loc[feat_elect_vio_agg+feat_violence_agg, feat_elect_vio_agg+feat_violence_agg].sum().sum(),\n",
    "        'Disasters*Disasters': interactdf_raw.loc[feat_disaster_agg, feat_disaster_agg].sum().sum()\n",
    "    }\n",
    "\n",
    "    # Convert the interaction sums to a pandas DataFrame for easy plotting\n",
    "    interaction_df = pd.DataFrame(list(interaction_sums.items()), columns=['Group', 'Sum'])\n",
    "\n",
    "    # Sort the values by the 'Sum' column\n",
    "    interaction_df = interaction_df.sort_values(by='Sum', ascending=False)\n",
    "\n",
    "    # Create a custom color palette from dark blue to lighter blue\n",
    "    custom_palette = sns.light_palette(\"darkblue\", reverse=True, n_colors=len(interaction_df))\n",
    "\n",
    "    # Create a barplot with sorted values\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))  \n",
    "    sns.barplot(x='Group', y='Sum', data=interaction_df, palette=custom_palette, ax=ax)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Sum of mean(|SHAP interaction value|)', fontsize=14)\n",
    "    plt.xlabel('', fontsize=14)\n",
    "    plt.xticks(rotation=0, ha='center', fontsize=12)\n",
    "\n",
    "    # Customize the axes and spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Display the plot\n",
    "    if save_fig:\n",
    "\n",
    "        plt.savefig(path_out_save,bbox_inches=\"tight\", dpi=400,\n",
    "                        transparent=False)\n",
    "\n",
    "        plt.show(fig)\n",
    "        plt.close()\n",
    "    else: \n",
    "        plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3745d-5e3a-428d-80a7-5a6e6bd409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For robustness, look only at best-performing folds\n",
    "if shap_eval:\n",
    "    model_inter = district_violence_disaster\n",
    "    list_inter = [\n",
    "        'gdis_disastertype_flood_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_landslide_ts_decay_6_tlag12',\n",
    "        'gdis_disastertype_storm_ts_decay_6_tlag12',\n",
    "        'spei_3_severe_ts_decay_6_tlag12',\n",
    "        'deco_best_sb_ts_decay_6_tlag12',\n",
    "        'deco_best_osv_ts_decay_6_tlag12',\n",
    "        'deco_best_ns_ts_decay_6_tlag12',\n",
    "                 ]\n",
    "    ########\n",
    "    saveintername = 'viodis'\n",
    "    #path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     #f'SI_figs11_shapinteraction_heatmap_absmean_{saveintername}.png')\n",
    "    interactdf_rob,interactdf_raw_rob = df_interactions_abs(\n",
    "        shap_interactions_dict = shapinterdict,\n",
    "        train_dict = train_outdict,\n",
    "        model = model_inter,\n",
    "        name_dict = name_dict,\n",
    "        inter_vars = list_inter,\n",
    "        selected_folds= selected_folds_dict['district_violence_disaster'],\n",
    "        remove_zeros = False,\n",
    "        agg_effect=False,\n",
    "        save_fig=True, \n",
    "        #path_out=path_out_save,\n",
    "        colorpalette='magma_r'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f2e13-bcf2-4715-b8a6-f53ed86f59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_eval:\n",
    "    save_fig = True\n",
    "    path_out_save = os.path.join(output_paths['shapvals'],\n",
    "                                     f'SI_figs13_shapinteraction_barplot_f{fold_inter}.png')\n",
    "\n",
    "\n",
    "    # Replace diagonals with 0\n",
    "    np.fill_diagonal(interactdf_raw_rob.values, 0)\n",
    "\n",
    "    # Compute the sums for the specified groups\n",
    "    interaction_sums = {\n",
    "        'Violence*Disasters': interactdf_raw_rob.loc[feat_elect_vio_agg+feat_violence_agg, feat_disaster_agg].sum().sum(),\n",
    "        'Violence*Baseline': interactdf_raw_rob.loc[feat_elect_vio_agg+feat_violence_agg, feat_baseline_district].sum().sum(),\n",
    "        'Disasters*Baseline': interactdf_raw_rob.loc[feat_disaster_agg, feat_baseline_district].sum().sum(),\n",
    "        'Violence*Violence': interactdf_raw_rob.loc[feat_elect_vio_agg+feat_violence_agg, feat_elect_vio_agg+feat_violence_agg].sum().sum(),\n",
    "        'Disasters*Disasters': interactdf_raw_rob.loc[feat_disaster_agg, feat_disaster_agg].sum().sum()\n",
    "    }\n",
    "\n",
    "    # Convert the interaction sums to a pandas DataFrame for easy plotting\n",
    "    interaction_df = pd.DataFrame(list(interaction_sums.items()), columns=['Group', 'Sum'])\n",
    "\n",
    "    # Sort the values by the 'Sum' column\n",
    "    interaction_df = interaction_df.sort_values(by='Sum', ascending=False)\n",
    "\n",
    "    # Create a custom color palette from dark blue to lighter blue\n",
    "    custom_palette = sns.light_palette(\"darkblue\", reverse=True, n_colors=len(interaction_df))\n",
    "\n",
    "    # Create a barplot with sorted values\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))  \n",
    "    sns.barplot(x='Group', y='Sum', data=interaction_df, palette=custom_palette, ax=ax)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Sum of mean(|SHAP interaction value|)', fontsize=14)\n",
    "    plt.xlabel('', fontsize=14)\n",
    "    plt.xticks(rotation=0, ha='center', fontsize=12)\n",
    "\n",
    "    # Customize the axes and spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Display the plot\n",
    "    if save_fig:\n",
    "\n",
    "        plt.savefig(path_out_save,bbox_inches=\"tight\", dpi=400,\n",
    "                        transparent=False)\n",
    "\n",
    "        plt.show(fig)\n",
    "        plt.close()\n",
    "    else: \n",
    "        plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2492e46-a083-48ae-a5bf-b928c6be8854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
